{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": [], "authorship_tag": "ABX9TyOzG2erxPh71vo2wkbAJGkw"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}}, "cells": [{"cell_type": "code", "execution_count": null, "metadata": {"id": "n7As_GevXe1s"}, "outputs": [], "source": ["# ============================================================\n", "# 0) One-off setup (installs)\n", "# ============================================================\n", "%env CUDA_LAUNCH_BLOCKING=1\n", "import subprocess, sys\n", "\n", "subprocess.run(\n", "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n", "     \"--upgrade\",\n", "     \"datasets>=2.18.0\", \"fsspec>=2023.6.0\",\n", "     \"pandas>=2.0.0\", \"sacrebleu>=2.4.0\",\n", "     \"evaluate>=0.4.2\", \"rouge-score>=0.1.2\",\n", "     \"bert-score>=0.3.13\", \"tabulate>=0.9.0\"],\n", "    check=True\n", ")\n", "\n", "# ============================================================\n", "# 1) Imports & Config\n", "# ============================================================\n", "import os\n", "import math\n", "import random\n", "import torch\n", "import torch.nn.functional as F\n", "import pandas as pd\n", "from datasets import load_dataset\n", "from transformers import AutoTokenizer, GPT2LMHeadModel\n", "from transformers.generation.logits_process import LogitsProcessor, LogitsProcessorList\n", "import sacrebleu\n", "import evaluate\n", "from tabulate import tabulate\n", "\n", "# ---- Experiment knobs ----\n", "SEED               = 123\n", "DEVICE             = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n", "MODEL_ID           = \"gpt2\"\n", "SPLIT              = \"train\"\n", "SEQ_LEN            = 1024\n", "BATCH_SIZE         = 16\n", "MAX_STEPS          = 1100\n", "TOP_K              = 3\n", "V_SELECT           = \"all\"\n", "N_TRIALS_PER_CLASS = 5\n", "MAX_NEW_TOKENS     = 100\n", "MAX_COL_WIDTH      = 100\n", "\n", "# Decoding strategy: \"greedy\", \"top-k\", or \"top-p\"\n", "DECODING_STRATEGY = \"greedy\"\n", "TOP_K_SAMPLING    = 10\n", "TOP_P_SAMPLING    = 0.9\n", "TEMPERATURE       = 1.0\n", "\n", "TEST_PROMPTS = [\n", "    \"The weather today is\",\n", "    \"The patient should take\",\n", "    \"The bank transfer amount is\",\n", "    \"The recommended dose for a child is\",\n", "    \"The evacuation order status is\",\n", "]\n", "\n", "# Metric toggles\n", "ENABLE_BLEU       = True\n", "ENABLE_METEOR     = True\n", "ENABLE_BERTSCORE  = True\n", "ENABLE_ROUGE      = True\n", "\n", "# Reproducibility\n", "random.seed(SEED)\n", "torch.manual_seed(SEED)\n", "\n", "# ============================================================\n", "# 2) Model & Tokenizer\n", "# ============================================================\n", "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n", "tok.pad_token = tok.eos_token\n", "model = GPT2LMHeadModel.from_pretrained(MODEL_ID).to(DEVICE)\n", "model.eval()\n", "\n", "# ============================================================\n", "# 3) Load & check metrics\n", "# ============================================================\n", "meteor = evaluate.load(\"meteor\")    if ENABLE_METEOR    else None\n", "berts  = evaluate.load(\"bertscore\") if ENABLE_BERTSCORE else None\n", "rouge  = evaluate.load(\"rouge\")     if ENABLE_ROUGE    else None\n", "\n", "print(f\"METEOR loaded: {meteor}\")\n", "print(f\"BERTScore loaded: {berts}\")\n", "print(f\"ROUGE loaded:   {rouge}\")\n", "\n", "# ============================================================\n", "# 4) Data & gradient scan for top-K sensitive coords\n", "# ============================================================\n", "wiki = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=SPLIT)\n", "\n", "def chunk_generator():\n", "    cache = []\n", "    for doc in wiki:\n", "        cache.extend(tok(doc[\"text\"]).input_ids)\n", "        while len(cache) >= SEQ_LEN + 1:\n", "            win, cache = cache[:SEQ_LEN+1], cache[SEQ_LEN+1:]\n", "            yield win[:-1], win[1:]\n", "\n", "def get_batch(gen, bs=BATCH_SIZE):\n", "    buf = []\n", "    for x, _ in gen:\n", "        buf.append(x)\n", "        if len(buf) == bs:\n", "            yield torch.tensor(buf, device=DEVICE)\n", "            buf = []\n", "\n", "param_dict  = {n: p for n, p in model.named_parameters() if p.requires_grad}\n", "running_max = {n: torch.zeros_like(p, device=\"cpu\") for n, p in param_dict.items()}\n", "\n", "for step, inp in enumerate(get_batch(chunk_generator()), 1):\n", "    model.zero_grad(set_to_none=True)\n", "    model(inp, labels=inp).loss.backward()\n", "    for name, p in param_dict.items():\n", "        running_max[name] = torch.maximum(\n", "            running_max[name],\n", "            p.grad.detach().abs().to(\"cpu\")\n", "        )\n", "    if step >= MAX_STEPS:\n", "        break\n", "\n", "candidates = []\n", "for name, rm in running_max.items():\n", "    k_local = min(TOP_K, rm.numel())\n", "    if k_local == 0:\n", "        continue\n", "    vals, idxs = torch.topk(rm.view(-1), k_local)\n", "    for v, flat in zip(vals, idxs):\n", "        coord = torch.unravel_index(flat, rm.shape)\n", "        candidates.append((v.item(), name, coord))\n", "\n", "candidates.sort(key=lambda t: t[0], reverse=True)\n", "topk_entries = candidates[:TOP_K]\n", "coords_list  = [(name, coord) for _, name, coord in topk_entries]\n", "\n", "print(f\"\\nGlobal Top-{TOP_K} |\u2202L/\u2202\u03b8| scalars:\")\n", "for rank, (val, name, coord) in enumerate(topk_entries, 1):\n", "    print(f\"  #{rank}: {name}{tuple(map(int,coord))}  |grad|={val:.3e}\")\n", "\n", "def normalize_v_select(sel, k):\n", "    if sel == \"all\":\n", "        return list(range(1, k+1))\n", "    if isinstance(sel, int):\n", "        return [sel]\n", "    if isinstance(sel, (list, tuple)):\n", "        return list(sel)\n", "    raise ValueError(\"V_SELECT must be 'all', int, or list[int]'\")\n", "\n", "ranks_to_test = normalize_v_select(V_SELECT, TOP_K)\n", "print(f\"\\nTesting ranks: {ranks_to_test}\")\n", "\n", "# ============================================================\n", "# 5) Bit-flip helpers\n", "# ============================================================\n", "def flip_bit(val_tensor: torch.Tensor, bit: int):\n", "    iv = val_tensor.view(torch.int32)\n", "    iv ^= (1 << bit)\n", "    return iv.view(torch.float32)\n", "\n", "BIT_CLASSES = {\n", "    \"sign\":     [31],\n", "    \"exponent\": list(range(23, 31)),\n", "    \"mantissa\": list(range(0, 23)),\n", "}\n", "\n", "# ============================================================\n", "# 6) Scoring function with try/except\n", "# ============================================================\n", "def edit_distance(a: str, b: str):\n", "    n, m = len(a), len(b)\n", "    dp = list(range(m+1))\n", "    for i in range(1, n+1):\n", "        prev, dp[0] = dp[0], i\n", "        for j in range(1, m+1):\n", "            cost = 0 if a[i-1] == b[j-1] else 1\n", "            dp[j], prev = min(dp[j] + 1, dp[j-1] + 1, prev + cost), dp[j]\n", "    return dp[m]\n", "\n", "def score_pair(clean: str, corrupt: str):\n", "    scores = {}\n", "    ed = edit_distance(clean, corrupt)\n", "    scores[\"EditDist\"]      = float(ed)\n", "    scores[\"EditDist_Norm\"] = float(ed / max(1, len(clean)))\n", "\n", "    if ENABLE_BLEU:\n", "        try:\n", "            scores[\"BLEU\"] = sacrebleu.corpus_bleu([corrupt], [[clean]]).score\n", "        except Exception as e:\n", "            print(\"BLEU compute failed:\", e)\n", "\n", "    if ENABLE_METEOR and meteor is not None:\n", "        try:\n", "            scores[\"METEOR\"] = float(\n", "                meteor.compute(predictions=[corrupt], references=[clean])[\"meteor\"]\n", "            )\n", "        except Exception as e:\n", "            print(\"METEOR compute failed:\", e)\n", "\n", "    if ENABLE_BERTSCORE and berts is not None:\n", "        try:\n", "            bs = berts.compute(\n", "                predictions=[corrupt], references=[clean], lang=\"en\"\n", "            )\n", "            scores[\"BERTScore_F1\"] = float(bs[\"f1\"][0])\n", "        except Exception as e:\n", "            print(\"BERTScore compute failed:\", e)\n", "\n", "    if ENABLE_ROUGE and rouge is not None:\n", "        try:\n", "            r = rouge.compute(\n", "                predictions=[corrupt], references=[clean], use_stemmer=True\n", "            )\n", "            scores[\"ROUGE1_F1\"] = float(r[\"rouge1\"])\n", "            scores[\"ROUGE2_F1\"] = float(r[\"rouge2\"])\n", "            scores[\"ROUGEL_F1\"] = float(r[\"rougeL\"])\n", "        except Exception as e:\n", "            print(\"ROUGE compute failed:\", e)\n", "\n", "    return scores\n", "\n", "# ============================================================\n", "# 7) Generation helpers\n", "# ============================================================\n", "class NanInfDetector(LogitsProcessor):\n", "    def __init__(self, flag_dict=None):\n", "        self.flag_dict = flag_dict\n", "    def __call__(self, input_ids, scores):\n", "        if self.flag_dict and (torch.isnan(scores).any() or torch.isinf(scores).any()):\n", "            self.flag_dict[\"had_nan\"] = True\n", "        return scores\n", "\n", "class MaxRepeatGuard(LogitsProcessor):\n", "    def __init__(self, max_consecutive=6):\n", "        self.max_consecutive = max_consecutive\n", "    def __call__(self, input_ids, scores):\n", "        if input_ids.size(0) != 1 or input_ids.size(1) == 0:\n", "            return scores\n", "        seq, last = input_ids[0], input_ids[0, -1].item()\n", "        run = 0\n", "        for t in range(seq.size(0)-1, -1, -1):\n", "            if seq[t].item() == last:\n", "                run += 1\n", "            else:\n", "                break\n", "        if run >= self.max_consecutive:\n", "            scores[:, last] = -1e9\n", "        return scores\n", "\n", "@torch.no_grad()\n", "def _generate(prompt: str, corrupt: bool = False):\n", "    enc = tok(prompt, return_tensors=\"pt\", return_attention_mask=True)\n", "    ids = enc[\"input_ids\"].to(DEVICE)\n", "    mask = enc[\"attention_mask\"].to(DEVICE)\n", "\n", "    do_sample = DECODING_STRATEGY != \"greedy\"\n", "    gen_kwargs = {\"temperature\": TEMPERATURE}\n", "    if DECODING_STRATEGY == \"top-k\":\n", "        gen_kwargs[\"top_k\"] = TOP_K_SAMPLING\n", "    elif DECODING_STRATEGY == \"top-p\":\n", "        gen_kwargs[\"top_p\"] = TOP_P_SAMPLING\n", "\n", "    if not corrupt:\n", "        out = model.generate(\n", "            input_ids=ids,\n", "            attention_mask=mask,\n", "            do_sample=do_sample,\n", "            max_new_tokens=MAX_NEW_TOKENS,\n", "            eos_token_id=tok.eos_token_id,\n", "            pad_token_id=tok.eos_token_id,\n", "            **gen_kwargs\n", "        )\n", "        return tok.decode(out[0, ids.size(1):], skip_special_tokens=True), False\n", "\n", "    diag = {\"had_nan\": False}\n", "    procs = LogitsProcessorList([\n", "        NanInfDetector(flag_dict=diag),\n", "        MaxRepeatGuard(max_consecutive=6),\n", "    ])\n", "    out = model.generate(\n", "        input_ids=ids,\n", "        attention_mask=mask,\n", "        do_sample=do_sample,\n", "        max_new_tokens=MAX_NEW_TOKENS,\n", "        eos_token_id=tok.eos_token_id,\n", "        pad_token_id=tok.eos_token_id,\n", "        logits_processor=procs,\n", "        no_repeat_ngram_size=3,\n", "        **gen_kwargs\n", "    )\n", "    return tok.decode(out[0, ids.size(1):], skip_special_tokens=True), diag[\"had_nan\"]\n", "\n", "generate_clean   = lambda p: _generate(p, corrupt=False)[0]\n", "generate_corrupt = lambda p: _generate(p, corrupt=True)\n", "\n", "# ============================================================\n", "# 8) Run bit-flip trials\n", "# ============================================================\n", "CLEAN_CACHE = {p: generate_clean(p) for p in TEST_PROMPTS}\n", "rows = []\n", "for rank in ranks_to_test:\n", "    name, coord = coords_list[rank-1]\n", "    W, orig = param_dict[name], param_dict[name].data[coord].clone()\n", "    for bit_class, pool in BIT_CLASSES.items():\n", "        for trial in range(1, N_TRIALS_PER_CLASS + 1):\n", "            bit = random.choice(pool)\n", "            W.data[coord] = flip_bit(orig, bit)\n", "            try:\n", "                for prompt in TEST_PROMPTS:\n", "                    clean_out = CLEAN_CACHE[prompt]\n", "                    corrupt_out, had_nan = generate_corrupt(prompt)\n", "                    scores = score_pair(clean_out, corrupt_out)\n", "                    rows.append({\n", "                        \"rank\": rank,\n", "                        \"tensor\": name,\n", "                        \"coord\": tuple(map(int, coord)),\n", "                        \"bit_class\": bit_class,\n", "                        \"bit_index\": bit,\n", "                        \"trial\": trial,\n", "                        \"prompt\": prompt,\n", "                        \"clean\": clean_out,\n", "                        \"corrupt\": corrupt_out,\n", "                        \"corrupt_logits_had_nan\": had_nan,\n", "                        **scores\n", "                    })\n", "            finally:\n", "                W.data[coord] = orig\n", "\n", "df = pd.DataFrame(rows)\n", "\n", "# ============================================================\n", "# 9) Debug: show final columns & sample\n", "# ============================================================\n", "print(\"Final DataFrame columns:\", df.columns.tolist())\n", "print(df.head(3))\n", "# ============================================================\n", "# 10) Save CSVs to GitHub repo\n", "# ============================================================\n", "# Save outputs inside the local clone of:\n", "#   https://github.com/kameshr/llm-sensitivity\n", "# This assumes the notebook is run from within that cloned repository.\n", "try:\n", "    repo_root = subprocess.check_output(\n", "        ['git', 'rev-parse', '--show-toplevel'],\n", "        stderr=subprocess.DEVNULL,\n", "    ).decode().strip()\n", "except Exception:\n", "    repo_root = os.getcwd()\n", "\n", "OUT_DIR = os.path.join(repo_root, 'bitflip_outputs')\n", "os.makedirs(OUT_DIR, exist_ok=True)\n", "\n", "# Per-trial CSV\n", "trial_path = os.path.join(OUT_DIR, 'bitflip_per_trial.csv')\n", "df.to_csv(trial_path, index=False)\n", "print(f\"Saved per-trial CSV \u2192 {trial_path}\")\n", "\n", "# Aggregated CSV\n", "metric_cols = [c for c in [\n", "    'EditDist','EditDist_Norm','BLEU','METEOR',\n", "    'BERTScore_F1','ROUGE1_F1','ROUGE2_F1','ROUGEL_F1'\n", "] if c in df.columns]\n", "print('Aggregated metrics present:', metric_cols)\n", "if metric_cols:\n", "    summary = df.groupby(\n", "        ['rank','tensor','coord','bit_class','prompt'],\n", "        as_index=False\n", "    ).agg({m: ['mean','median','std'] for m in metric_cols})\n", "    if isinstance(summary.columns, pd.MultiIndex):\n", "        summary.columns = ['_'.join(filter(None, c)) for c in summary.columns]\n", "    agg_path = os.path.join(OUT_DIR, 'bitflip_aggregated.csv')\n", "    summary.to_csv(agg_path, index=False)\n", "    print(f\"Saved aggregated CSV \u2192 {agg_path}\")\n"]}]}