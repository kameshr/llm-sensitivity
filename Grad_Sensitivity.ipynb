{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7As_GevXe1s"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 0) One-off setup (installs)\n",
        "# ============================================================\n",
        "%env CUDA_LAUNCH_BLOCKING=1\n",
        "import subprocess, sys\n",
        "\n",
        "subprocess.run(\n",
        "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "     \"--upgrade\",\n",
        "     \"datasets>=2.18.0\", \"fsspec>=2023.6.0\",\n",
        "     \"pandas>=2.0.0\", \"sacrebleu>=2.4.0\",\n",
        "     \"evaluate>=0.4.2\", \"rouge-score>=0.1.2\",\n",
        "     \"bert-score>=0.3.13\", \"tabulate>=0.9.0\"],\n",
        "    check=True\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 1) Imports & Config\n",
        "# ============================================================\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
        "from transformers.generation.logits_process import LogitsProcessor, LogitsProcessorList\n",
        "import sacrebleu\n",
        "import evaluate\n",
        "from tabulate import tabulate\n",
        "\n",
        "# ---- Experiment knobs ----\n",
        "SEED               = 123\n",
        "DEVICE             = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_ID           = \"gpt2\"\n",
        "SPLIT              = \"train\"\n",
        "SEQ_LEN            = 1024\n",
        "BATCH_SIZE         = 16\n",
        "MAX_STEPS          = 1100\n",
        "TOP_K              = 3\n",
        "V_SELECT           = \"all\"\n",
        "N_TRIALS_PER_CLASS = 5\n",
        "MAX_NEW_TOKENS     = 100\n",
        "MAX_COL_WIDTH      = 100\n",
        "\n",
        "# Decoding strategy: \"greedy\", \"top-k\", or \"top-p\"\n",
        "DECODING_STRATEGY = \"greedy\"\n",
        "TOP_K_SAMPLING    = 10\n",
        "TOP_P_SAMPLING    = 0.9\n",
        "TEMPERATURE       = 1.0\n",
        "\n",
        "TEST_PROMPTS = [\n",
        "    \"The weather today is\",\n",
        "    \"The patient should take\",\n",
        "    \"The bank transfer amount is\",\n",
        "    \"The recommended dose for a child is\",\n",
        "    \"The evacuation order status is\",\n",
        "]\n",
        "\n",
        "# Metric toggles\n",
        "ENABLE_BLEU       = True\n",
        "ENABLE_METEOR     = True\n",
        "ENABLE_BERTSCORE  = True\n",
        "ENABLE_ROUGE      = True\n",
        "\n",
        "# Reproducibility\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# ============================================================\n",
        "# 2) Model & Tokenizer\n",
        "# ============================================================\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "tok.pad_token = tok.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# ============================================================\n",
        "# 3) Load & check metrics\n",
        "# ============================================================\n",
        "meteor = evaluate.load(\"meteor\")    if ENABLE_METEOR    else None\n",
        "berts  = evaluate.load(\"bertscore\") if ENABLE_BERTSCORE else None\n",
        "rouge  = evaluate.load(\"rouge\")     if ENABLE_ROUGE    else None\n",
        "\n",
        "print(f\"METEOR loaded: {meteor}\")\n",
        "print(f\"BERTScore loaded: {berts}\")\n",
        "print(f\"ROUGE loaded:   {rouge}\")\n",
        "\n",
        "# ============================================================\n",
        "# 4) Data & gradient scan for top-K sensitive coords\n",
        "# ============================================================\n",
        "wiki = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=SPLIT)\n",
        "\n",
        "def chunk_generator():\n",
        "    cache = []\n",
        "    for doc in wiki:\n",
        "        cache.extend(tok(doc[\"text\"]).input_ids)\n",
        "        while len(cache) >= SEQ_LEN + 1:\n",
        "            win, cache = cache[:SEQ_LEN+1], cache[SEQ_LEN+1:]\n",
        "            yield win[:-1], win[1:]\n",
        "\n",
        "def get_batch(gen, bs=BATCH_SIZE):\n",
        "    buf = []\n",
        "    for x, _ in gen:\n",
        "        buf.append(x)\n",
        "        if len(buf) == bs:\n",
        "            yield torch.tensor(buf, device=DEVICE)\n",
        "            buf = []\n",
        "\n",
        "param_dict  = {n: p for n, p in model.named_parameters() if p.requires_grad}\n",
        "running_max = {n: torch.zeros_like(p, device=\"cpu\") for n, p in param_dict.items()}\n",
        "\n",
        "for step, inp in enumerate(get_batch(chunk_generator()), 1):\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    model(inp, labels=inp).loss.backward()\n",
        "    for name, p in param_dict.items():\n",
        "        running_max[name] = torch.maximum(\n",
        "            running_max[name],\n",
        "            p.grad.detach().abs().to(\"cpu\")\n",
        "        )\n",
        "    if step >= MAX_STEPS:\n",
        "        break\n",
        "\n",
        "candidates = []\n",
        "for name, rm in running_max.items():\n",
        "    k_local = min(TOP_K, rm.numel())\n",
        "    if k_local == 0:\n",
        "        continue\n",
        "    vals, idxs = torch.topk(rm.view(-1), k_local)\n",
        "    for v, flat in zip(vals, idxs):\n",
        "        coord = torch.unravel_index(flat, rm.shape)\n",
        "        candidates.append((v.item(), name, coord))\n",
        "\n",
        "candidates.sort(key=lambda t: t[0], reverse=True)\n",
        "topk_entries = candidates[:TOP_K]\n",
        "coords_list  = [(name, coord) for _, name, coord in topk_entries]\n",
        "\n",
        "print(f\"\\nGlobal Top-{TOP_K} |âˆ‚L/âˆ‚Î¸| scalars:\")\n",
        "for rank, (val, name, coord) in enumerate(topk_entries, 1):\n",
        "    print(f\"  #{rank}: {name}{tuple(map(int,coord))}  |grad|={val:.3e}\")\n",
        "\n",
        "def normalize_v_select(sel, k):\n",
        "    if sel == \"all\":\n",
        "        return list(range(1, k+1))\n",
        "    if isinstance(sel, int):\n",
        "        return [sel]\n",
        "    if isinstance(sel, (list, tuple)):\n",
        "        return list(sel)\n",
        "    raise ValueError(\"V_SELECT must be 'all', int, or list[int]'\")\n",
        "\n",
        "ranks_to_test = normalize_v_select(V_SELECT, TOP_K)\n",
        "print(f\"\\nTesting ranks: {ranks_to_test}\")\n",
        "\n",
        "# ============================================================\n",
        "# 5) Bit-flip helpers\n",
        "# ============================================================\n",
        "def flip_bit(val_tensor: torch.Tensor, bit: int):\n",
        "    iv = val_tensor.view(torch.int32)\n",
        "    iv ^= (1 << bit)\n",
        "    return iv.view(torch.float32)\n",
        "\n",
        "BIT_CLASSES = {\n",
        "    \"sign\":     [31],\n",
        "    \"exponent\": list(range(23, 31)),\n",
        "    \"mantissa\": list(range(0, 23)),\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# 6) Scoring function with try/except\n",
        "# ============================================================\n",
        "def edit_distance(a: str, b: str):\n",
        "    n, m = len(a), len(b)\n",
        "    dp = list(range(m+1))\n",
        "    for i in range(1, n+1):\n",
        "        prev, dp[0] = dp[0], i\n",
        "        for j in range(1, m+1):\n",
        "            cost = 0 if a[i-1] == b[j-1] else 1\n",
        "            dp[j], prev = min(dp[j] + 1, dp[j-1] + 1, prev + cost), dp[j]\n",
        "    return dp[m]\n",
        "\n",
        "def score_pair(clean: str, corrupt: str):\n",
        "    scores = {}\n",
        "    ed = edit_distance(clean, corrupt)\n",
        "    scores[\"EditDist\"]      = float(ed)\n",
        "    scores[\"EditDist_Norm\"] = float(ed / max(1, len(clean)))\n",
        "\n",
        "    if ENABLE_BLEU:\n",
        "        try:\n",
        "            scores[\"BLEU\"] = sacrebleu.corpus_bleu([corrupt], [[clean]]).score\n",
        "        except Exception as e:\n",
        "            print(\"BLEU compute failed:\", e)\n",
        "\n",
        "    if ENABLE_METEOR and meteor is not None:\n",
        "        try:\n",
        "            scores[\"METEOR\"] = float(\n",
        "                meteor.compute(predictions=[corrupt], references=[clean])[\"meteor\"]\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(\"METEOR compute failed:\", e)\n",
        "\n",
        "    if ENABLE_BERTSCORE and berts is not None:\n",
        "        try:\n",
        "            bs = berts.compute(\n",
        "                predictions=[corrupt], references=[clean], lang=\"en\"\n",
        "            )\n",
        "            scores[\"BERTScore_F1\"] = float(bs[\"f1\"][0])\n",
        "        except Exception as e:\n",
        "            print(\"BERTScore compute failed:\", e)\n",
        "\n",
        "    if ENABLE_ROUGE and rouge is not None:\n",
        "        try:\n",
        "            r = rouge.compute(\n",
        "                predictions=[corrupt], references=[clean], use_stemmer=True\n",
        "            )\n",
        "            scores[\"ROUGE1_F1\"] = float(r[\"rouge1\"])\n",
        "            scores[\"ROUGE2_F1\"] = float(r[\"rouge2\"])\n",
        "            scores[\"ROUGEL_F1\"] = float(r[\"rougeL\"])\n",
        "        except Exception as e:\n",
        "            print(\"ROUGE compute failed:\", e)\n",
        "\n",
        "    return scores\n",
        "\n",
        "# ============================================================\n",
        "# 7) Generation helpers\n",
        "# ============================================================\n",
        "class NanInfDetector(LogitsProcessor):\n",
        "    def __init__(self, flag_dict=None):\n",
        "        self.flag_dict = flag_dict\n",
        "    def __call__(self, input_ids, scores):\n",
        "        if self.flag_dict and (torch.isnan(scores).any() or torch.isinf(scores).any()):\n",
        "            self.flag_dict[\"had_nan\"] = True\n",
        "        return scores\n",
        "\n",
        "class MaxRepeatGuard(LogitsProcessor):\n",
        "    def __init__(self, max_consecutive=6):\n",
        "        self.max_consecutive = max_consecutive\n",
        "    def __call__(self, input_ids, scores):\n",
        "        if input_ids.size(0) != 1 or input_ids.size(1) == 0:\n",
        "            return scores\n",
        "        seq, last = input_ids[0], input_ids[0, -1].item()\n",
        "        run = 0\n",
        "        for t in range(seq.size(0)-1, -1, -1):\n",
        "            if seq[t].item() == last:\n",
        "                run += 1\n",
        "            else:\n",
        "                break\n",
        "        if run >= self.max_consecutive:\n",
        "            scores[:, last] = -1e9\n",
        "        return scores\n",
        "\n",
        "@torch.no_grad()\n",
        "def _generate(prompt: str, corrupt: bool = False):\n",
        "    enc = tok(prompt, return_tensors=\"pt\", return_attention_mask=True)\n",
        "    ids = enc[\"input_ids\"].to(DEVICE)\n",
        "    mask = enc[\"attention_mask\"].to(DEVICE)\n",
        "\n",
        "    do_sample = DECODING_STRATEGY != \"greedy\"\n",
        "    gen_kwargs = {\"temperature\": TEMPERATURE}\n",
        "    if DECODING_STRATEGY == \"top-k\":\n",
        "        gen_kwargs[\"top_k\"] = TOP_K_SAMPLING\n",
        "    elif DECODING_STRATEGY == \"top-p\":\n",
        "        gen_kwargs[\"top_p\"] = TOP_P_SAMPLING\n",
        "\n",
        "    if not corrupt:\n",
        "        out = model.generate(\n",
        "            input_ids=ids,\n",
        "            attention_mask=mask,\n",
        "            do_sample=do_sample,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            eos_token_id=tok.eos_token_id,\n",
        "            pad_token_id=tok.eos_token_id,\n",
        "            **gen_kwargs\n",
        "        )\n",
        "        return tok.decode(out[0, ids.size(1):], skip_special_tokens=True), False\n",
        "\n",
        "    diag = {\"had_nan\": False}\n",
        "    procs = LogitsProcessorList([\n",
        "        NanInfDetector(flag_dict=diag),\n",
        "        MaxRepeatGuard(max_consecutive=6),\n",
        "    ])\n",
        "    out = model.generate(\n",
        "        input_ids=ids,\n",
        "        attention_mask=mask,\n",
        "        do_sample=do_sample,\n",
        "        max_new_tokens=MAX_NEW_TOKENS,\n",
        "        eos_token_id=tok.eos_token_id,\n",
        "        pad_token_id=tok.eos_token_id,\n",
        "        logits_processor=procs,\n",
        "        no_repeat_ngram_size=3,\n",
        "        **gen_kwargs\n",
        "    )\n",
        "    return tok.decode(out[0, ids.size(1):], skip_special_tokens=True), diag[\"had_nan\"]\n",
        "\n",
        "generate_clean   = lambda p: _generate(p, corrupt=False)[0]\n",
        "generate_corrupt = lambda p: _generate(p, corrupt=True)\n",
        "\n",
        "# ============================================================\n",
        "# 8) Run bit-flip trials\n",
        "# ============================================================\n",
        "CLEAN_CACHE = {p: generate_clean(p) for p in TEST_PROMPTS}\n",
        "rows = []\n",
        "for rank in ranks_to_test:\n",
        "    name, coord = coords_list[rank-1]\n",
        "    W, orig = param_dict[name], param_dict[name].data[coord].clone()\n",
        "    for bit_class, pool in BIT_CLASSES.items():\n",
        "        for trial in range(1, N_TRIALS_PER_CLASS + 1):\n",
        "            bit = random.choice(pool)\n",
        "            W.data[coord] = flip_bit(orig, bit)\n",
        "            try:\n",
        "                for prompt in TEST_PROMPTS:\n",
        "                    clean_out = CLEAN_CACHE[prompt]\n",
        "                    corrupt_out, had_nan = generate_corrupt(prompt)\n",
        "                    scores = score_pair(clean_out, corrupt_out)\n",
        "                    rows.append({\n",
        "                        \"rank\": rank,\n",
        "                        \"tensor\": name,\n",
        "                        \"coord\": tuple(map(int, coord)),\n",
        "                        \"bit_class\": bit_class,\n",
        "                        \"bit_index\": bit,\n",
        "                        \"trial\": trial,\n",
        "                        \"prompt\": prompt,\n",
        "                        \"clean\": clean_out,\n",
        "                        \"corrupt\": corrupt_out,\n",
        "                        \"corrupt_logits_had_nan\": had_nan,\n",
        "                        **scores\n",
        "                    })\n",
        "            finally:\n",
        "                W.data[coord] = orig\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# ============================================================\n",
        "# 9) Debug: show final columns & sample\n",
        "# ============================================================\n",
        "print(\"Final DataFrame columns:\", df.columns.tolist())\n",
        "print(df.head(3))\n",
        "# ============================================================\n",
        "# 10) Save CSVs to GitHub repo\n",
        "# ============================================================\n",
        "# Save outputs inside the local clone of:\n",
        "#   https://github.com/kameshr/llm-sensitivity\n",
        "# This assumes the notebook is run from within that cloned repository.\n",
        "try:\n",
        "    repo_root = subprocess.check_output(\n",
        "        ['git', 'rev-parse', '--show-toplevel'],\n",
        "        stderr=subprocess.DEVNULL,\n",
        "    ).decode().strip()\n",
        "except Exception:\n",
        "    repo_root = os.getcwd()\n",
        "\n",
        "OUT_DIR = os.path.join(repo_root, 'bitflip_outputs')\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Per-trial CSV\n",
        "trial_path = os.path.join(OUT_DIR, 'bitflip_per_trial.csv')\n",
        "df.to_csv(trial_path, index=False)\n",
        "print(f\"Saved per-trial CSV â†’ {trial_path}\")\n",
        "\n",
        "# Aggregated CSV\n",
        "metric_cols = [c for c in [\n",
        "    'EditDist','EditDist_Norm','BLEU','METEOR',\n",
        "    'BERTScore_F1','ROUGE1_F1','ROUGE2_F1','ROUGEL_F1'\n",
        "] if c in df.columns]\n",
        "print('Aggregated metrics present:', metric_cols)\n",
        "if metric_cols:\n",
        "    summary = df.groupby(\n",
        "        ['rank','tensor','coord','bit_class','prompt'],\n",
        "        as_index=False\n",
        "    ).agg({m: ['mean','median','std'] for m in metric_cols})\n",
        "    if isinstance(summary.columns, pd.MultiIndex):\n",
        "        summary.columns = ['_'.join(filter(None, c)) for c in summary.columns]\n",
        "    agg_path = os.path.join(OUT_DIR, 'bitflip_aggregated.csv')\n",
        "    summary.to_csv(agg_path, index=False)\n",
        "    print(f\"Saved aggregated CSV â†’ {agg_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 0) One-off setup (installs)\n",
        "# ============================================================\n",
        "%env CUDA_LAUNCH_BLOCKING=1\n",
        "import subprocess, sys\n",
        "\n",
        "subprocess.run(\n",
        "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "     \"--upgrade\",\n",
        "     \"datasets>=2.18.0\", \"fsspec>=2023.6.0\",\n",
        "     \"pandas>=2.0.0\", \"sacrebleu>=2.4.0\",\n",
        "     \"evaluate>=0.4.2\", \"rouge-score>=0.1.2\",\n",
        "     \"bert-score>=0.3.13\", \"tabulate>=0.9.0\"],\n",
        "    check=True\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 1) Imports & Config\n",
        "# ============================================================\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2LMHeadModel\n",
        "from transformers.generation.logits_process import LogitsProcessor, LogitsProcessorList\n",
        "import sacrebleu\n",
        "import evaluate\n",
        "from tabulate import tabulate\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# USER CONFIG\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "SEED     = 123\n",
        "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ðŸ”¥ Switch between GPT-2 or DeepSeek/Qwen or any HF CausalLM:\n",
        "MODEL_ID = \"gpt2\"\n",
        "# MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "SPLIT              = \"train\"\n",
        "SEQ_LEN            = 1024\n",
        "BATCH_SIZE         = 8\n",
        "MAX_STEPS          = 1\n",
        "TOP_K              = 3\n",
        "V_SELECT           = \"all\"\n",
        "N_TRIALS_PER_CLASS = 5\n",
        "MAX_NEW_TOKENS     = 100\n",
        "\n",
        "# Decoding\n",
        "DECODING_STRATEGY = \"greedy\"\n",
        "TOP_K_SAMPLING = 10\n",
        "TOP_P_SAMPLING = 0.9\n",
        "TEMPERATURE = 1.0\n",
        "\n",
        "TEST_PROMPTS = [\n",
        "    \"The weather today is\",\n",
        "    \"The patient should take\",\n",
        "    \"The bank transfer amount is\",\n",
        "    \"The recommended dose for a child is\",\n",
        "    \"The evacuation order status is\",\n",
        "]\n",
        "\n",
        "# Metrics\n",
        "ENABLE_BLEU       = True\n",
        "ENABLE_METEOR     = True\n",
        "ENABLE_BERTSCORE  = True\n",
        "ENABLE_ROUGE      = True\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# NEW FEATURE (1): BITFLIP MODE\n",
        "# ------------------------------------------------------------\n",
        "BITFLIP_MODE = \"all_topk\"   # \"all_topk\" or \"custom\"\n",
        "\n",
        "# For BITFLIP_MODE=\"custom\", specify weights like:\n",
        "CUSTOM_BITFLIP_TARGETS = [\n",
        "    # (\"transformer.h.5.ln_1.weight\", (447,)),\n",
        "    # (\"lm_head.weight\", (1024, 400)),\n",
        "]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2) Model Loader (supports GPT-2 or ANY HF causal model)\n",
        "# ============================================================\n",
        "\n",
        "print(f\"Loading tokenizer for model: {MODEL_ID}\")\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "# ensure padding\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token or tok.convert_ids_to_tokens(0)\n",
        "\n",
        "print(\"Loading model in float32 (safe for gradients)...\")\n",
        "\n",
        "def load_any_model(model_id):\n",
        "    \"\"\"Load GPT-2 or any general causal LM in float32 safely.\"\"\"\n",
        "    if \"gpt2\" in model_id.lower():\n",
        "        return GPT2LMHeadModel.from_pretrained(model_id, torch_dtype=torch.float32)\n",
        "    else:\n",
        "        return AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32)\n",
        "\n",
        "model = load_any_model(MODEL_ID).to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "print(\"Model dtype:\", next(model.parameters()).dtype)\n",
        "\n",
        "# ============================================================\n",
        "# 3) Metrics\n",
        "# ============================================================\n",
        "meteor = evaluate.load(\"meteor\")    if ENABLE_METEOR    else None\n",
        "berts  = evaluate.load(\"bertscore\") if ENABLE_BERTSCORE else None\n",
        "rouge  = evaluate.load(\"rouge\")     if ENABLE_ROUGE     else None\n",
        "\n",
        "print(\"\\nMetrics loaded.\")\n",
        "\n",
        "# ============================================================\n",
        "# 4) Dataset & gradient scan\n",
        "# ============================================================\n",
        "wiki = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=SPLIT)\n",
        "\n",
        "def chunk_generator():\n",
        "    cache = []\n",
        "    for doc in wiki:\n",
        "        cache.extend(tok(doc[\"text\"]).input_ids)\n",
        "        while len(cache) >= SEQ_LEN + 1:\n",
        "            win, cache = cache[:SEQ_LEN+1], cache[SEQ_LEN+1:]\n",
        "            yield win[:-1], win[1:]\n",
        "\n",
        "def get_batch(gen):\n",
        "    buf = []\n",
        "    for x, _ in gen:\n",
        "        buf.append(x)\n",
        "        if len(buf) == BATCH_SIZE:\n",
        "            yield torch.tensor(buf, device=DEVICE)\n",
        "            buf = []\n",
        "\n",
        "param_dict  = {n: p for n, p in model.named_parameters() if p.requires_grad}\n",
        "running_max = {n: torch.zeros_like(p, device=\"cpu\") for n, p in param_dict.items()}\n",
        "\n",
        "print(\"\\nStarting gradient scanâ€¦\")\n",
        "for step, inp in enumerate(get_batch(chunk_generator()), 1):\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    loss = model(inp, labels=inp).loss\n",
        "    loss.backward()\n",
        "\n",
        "    for n, p in param_dict.items():\n",
        "        if p.grad is not None:\n",
        "            running_max[n] = torch.maximum(\n",
        "                running_max[n],\n",
        "                p.grad.detach().abs().cpu()\n",
        "            )\n",
        "\n",
        "    if step >= MAX_STEPS:\n",
        "        break\n",
        "\n",
        "# Collect top-K\n",
        "candidates = []\n",
        "for name, rm in running_max.items():\n",
        "    k_local = min(TOP_K, rm.numel())\n",
        "    vals, idxs = torch.topk(rm.view(-1), k_local)\n",
        "    for v, flat in zip(vals, idxs):\n",
        "        coord = torch.unravel_index(flat, rm.shape)\n",
        "        candidates.append((v.item(), name, coord))\n",
        "\n",
        "candidates.sort(key=lambda t: t[0], reverse=True)\n",
        "topk_entries = candidates[:TOP_K]\n",
        "coords_list = [(name, coord) for _, name, coord in topk_entries]\n",
        "\n",
        "print(\"\\nTop-K global sensitive coords:\")\n",
        "for rank, (val, name, coord) in enumerate(topk_entries, 1):\n",
        "    print(f\"  #{rank}: {name}{tuple(map(int,coord))}  |grad|={val:.3e}\")\n",
        "\n",
        "# ============================================================\n",
        "# CUSTOM OR ALL-TOP-K TARGET SELECTION\n",
        "# ============================================================\n",
        "if BITFLIP_MODE == \"all_topk\":\n",
        "    bitflip_targets = coords_list\n",
        "else:\n",
        "    bitflip_targets = CUSTOM_BITFLIP_TARGETS\n",
        "\n",
        "print(\"\\nBit-flip targets:\")\n",
        "print(bitflip_targets)\n",
        "\n",
        "# ============================================================\n",
        "# 5) Bit-flip helpers\n",
        "# ============================================================\n",
        "def flip_bit(val_tensor: torch.Tensor, bit: int):\n",
        "    iv = val_tensor.view(torch.int32)\n",
        "    iv ^= (1 << bit)\n",
        "    return iv.view(torch.float32)\n",
        "\n",
        "BIT_CLASSES = {\n",
        "    \"sign\":     [31],\n",
        "    \"exponent\": list(range(23, 31)),\n",
        "    \"mantissa\": list(range(0, 23)),\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# 6) Scoring utilities\n",
        "# ============================================================\n",
        "def edit_distance(a: str, b: str):\n",
        "    n, m = len(a), len(b)\n",
        "    dp = list(range(m+1))\n",
        "    for i in range(1, n+1):\n",
        "        prev, dp[0] = dp[0], i\n",
        "        for j in range(1, m+1):\n",
        "            cost = 0 if a[i-1] == b[j-1] else 1\n",
        "            dp[j], prev = min(dp[j] + 1, dp[j-1] + 1, prev + cost), dp[j]\n",
        "    return dp[m]\n",
        "\n",
        "def score_pair(clean, corrupt):\n",
        "    d = {}\n",
        "    d[\"EditDist\"]      = ed = edit_distance(clean, corrupt)\n",
        "    d[\"EditDist_Norm\"] = ed / max(1, len(clean))\n",
        "\n",
        "    if ENABLE_BLEU:\n",
        "        d[\"BLEU\"] = sacrebleu.corpus_bleu([corrupt], [[clean]]).score\n",
        "\n",
        "    if ENABLE_METEOR:\n",
        "        d[\"METEOR\"] = meteor.compute(predictions=[corrupt], references=[clean])[\"meteor\"]\n",
        "\n",
        "    if ENABLE_BERTSCORE:\n",
        "        bs = berts.compute(predictions=[corrupt], references=[clean], lang=\"en\")\n",
        "        d[\"BERTScore_F1\"] = float(bs[\"f1\"][0])\n",
        "\n",
        "    if ENABLE_ROUGE:\n",
        "        r = rouge.compute(predictions=[corrupt], references=[clean])\n",
        "        d[\"ROUGE1_F1\"] = r[\"rouge1\"]\n",
        "        d[\"ROUGE2_F1\"] = r[\"rouge2\"]\n",
        "        d[\"ROUGEL_F1\"] = r[\"rougeL\"]\n",
        "\n",
        "    return d\n",
        "\n",
        "# ============================================================\n",
        "# 7) Generation helpers\n",
        "# ============================================================\n",
        "@torch.no_grad()\n",
        "def _generate(prompt, corrupt=False):\n",
        "    enc = tok(prompt, return_tensors=\"pt\")\n",
        "    ids = enc[\"input_ids\"].to(DEVICE)\n",
        "\n",
        "    out = model.generate(\n",
        "        input_ids=ids,\n",
        "        do_sample=(DECODING_STRATEGY != \"greedy\"),\n",
        "        max_new_tokens=MAX_NEW_TOKENS,\n",
        "        top_k=TOP_K_SAMPLING,\n",
        "        top_p=TOP_P_SAMPLING,\n",
        "        temperature=TEMPERATURE\n",
        "    )\n",
        "    return tok.decode(out[0, ids.size(1):], skip_special_tokens=True)\n",
        "\n",
        "generate_clean   = lambda p: _generate(p, corrupt=False)\n",
        "generate_corrupt = lambda p: _generate(p, corrupt=True)\n",
        "\n",
        "# ============================================================\n",
        "# 8) Run bit-flip trials\n",
        "# ============================================================\n",
        "CLEAN_CACHE = {p: generate_clean(p) for p in TEST_PROMPTS}\n",
        "\n",
        "rows = []\n",
        "for name, coord in bitflip_targets:\n",
        "    W = param_dict[name]\n",
        "    orig = W.data[coord].clone()\n",
        "\n",
        "    for bit_class, pool in BIT_CLASSES.items():\n",
        "        for trial in range(1, N_TRIALS_PER_CLASS + 1):\n",
        "            bit = random.choice(pool)\n",
        "            W.data[coord] = flip_bit(orig, bit)\n",
        "            try:\n",
        "                for prompt in TEST_PROMPTS:\n",
        "                    clean_out = CLEAN_CACHE[prompt]\n",
        "                    corrupt_out = generate_corrupt(prompt)\n",
        "\n",
        "                    scores = score_pair(clean_out, corrupt_out)\n",
        "                    rows.append({\n",
        "                        \"tensor\": name,\n",
        "                        \"coord\": coord,\n",
        "                        \"bit_class\": bit_class,\n",
        "                        \"bit\": bit,\n",
        "                        \"prompt\": prompt,\n",
        "                        \"clean\": clean_out,\n",
        "                        \"corrupt\": corrupt_out,\n",
        "                        **scores\n",
        "                    })\n",
        "            finally:\n",
        "                W.data[coord] = orig\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "print(\"\\nSample results:\")\n",
        "print(df.head(3))\n",
        "\n",
        "# ============================================================\n",
        "# 9) Save results inside GitHub repo\n",
        "# ============================================================\n",
        "try:\n",
        "    repo_root = subprocess.check_output(\n",
        "        ['git', 'rev-parse', '--show-toplevel']\n",
        "    ).decode().strip()\n",
        "except Exception:\n",
        "    repo_root = os.getcwd()\n",
        "\n",
        "OUT_DIR = os.path.join(repo_root, \"bitflip_outputs\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "OUT_PATH = os.path.join(OUT_DIR, \"bitflip_results.csv\")\n",
        "df.to_csv(OUT_PATH, index=False)\n",
        "\n",
        "print(f\"\\nSaved â†’ {OUT_PATH}\")\n"
      ],
      "metadata": {
        "id": "Pn1yN0FfOhOq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}