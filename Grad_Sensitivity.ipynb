{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n7As_GevXe1s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "64b360b0-bd77-4ae9-ce8a-b8168b6eba66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CUDA_LAUNCH_BLOCKING=1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3496383596.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;31m# See Note [Global dependencies]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m_load_global_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;31m# If all above-mentioned conditions are met, preload nvrtc and nvjitlink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0m_preload_cuda_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda_nvrtc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"libnvrtc.so.*[0-9]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m             \u001b[0m_preload_cuda_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda_nvrtc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"libnvrtc-builtins.so.*[0-9]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0m_preload_cuda_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nvjitlink\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"libnvJitLink.so.*[0-9]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m_preload_cuda_deps\u001b[0;34m(lib_folder, lib_name, required)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{lib_name} not found in the system path {sys.path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlib_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 0) One-off setup (installs)\n",
        "# ============================================================\n",
        "%env CUDA_LAUNCH_BLOCKING=1\n",
        "import subprocess, sys\n",
        "\n",
        "subprocess.run(\n",
        "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "     \"--upgrade\",\n",
        "     \"datasets>=2.18.0\", \"fsspec>=2023.6.0\",\n",
        "     \"pandas>=2.0.0\", \"sacrebleu>=2.4.0\",\n",
        "     \"evaluate>=0.4.2\", \"rouge-score>=0.1.2\",\n",
        "     \"bert-score>=0.3.13\", \"tabulate>=0.9.0\"],\n",
        "    check=True\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 1) Imports & Config\n",
        "# ============================================================\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
        "from transformers.generation.logits_process import LogitsProcessor, LogitsProcessorList\n",
        "import sacrebleu\n",
        "import evaluate\n",
        "from tabulate import tabulate\n",
        "\n",
        "# ---- Experiment knobs ----\n",
        "SEED               = 123\n",
        "DEVICE             = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_ID           = \"gpt2\"\n",
        "SPLIT              = \"train\"\n",
        "SEQ_LEN            = 1024\n",
        "BATCH_SIZE         = 16\n",
        "MAX_STEPS          = 1100\n",
        "TOP_K              = 3\n",
        "V_SELECT           = \"all\"\n",
        "N_TRIALS_PER_CLASS = 5\n",
        "MAX_NEW_TOKENS     = 100\n",
        "MAX_COL_WIDTH      = 100\n",
        "\n",
        "# Decoding strategy: \"greedy\", \"top-k\", or \"top-p\"\n",
        "DECODING_STRATEGY = \"greedy\"\n",
        "TOP_K_SAMPLING    = 10\n",
        "TOP_P_SAMPLING    = 0.9\n",
        "TEMPERATURE       = 1.0\n",
        "\n",
        "TEST_PROMPTS = [\n",
        "    \"The weather today is\",\n",
        "    \"The patient should take\",\n",
        "    \"The bank transfer amount is\",\n",
        "    \"The recommended dose for a child is\",\n",
        "    \"The evacuation order status is\",\n",
        "]\n",
        "\n",
        "# Metric toggles\n",
        "ENABLE_BLEU       = True\n",
        "ENABLE_METEOR     = True\n",
        "ENABLE_BERTSCORE  = True\n",
        "ENABLE_ROUGE      = True\n",
        "\n",
        "# Reproducibility\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# ============================================================\n",
        "# 2) Model & Tokenizer\n",
        "# ============================================================\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "tok.pad_token = tok.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# ============================================================\n",
        "# 3) Load & check metrics\n",
        "# ============================================================\n",
        "meteor = evaluate.load(\"meteor\")    if ENABLE_METEOR    else None\n",
        "berts  = evaluate.load(\"bertscore\") if ENABLE_BERTSCORE else None\n",
        "rouge  = evaluate.load(\"rouge\")     if ENABLE_ROUGE    else None\n",
        "\n",
        "print(f\"METEOR loaded: {meteor}\")\n",
        "print(f\"BERTScore loaded: {berts}\")\n",
        "print(f\"ROUGE loaded:   {rouge}\")\n",
        "\n",
        "# ============================================================\n",
        "# 4) Data & gradient scan for top-K sensitive coords\n",
        "# ============================================================\n",
        "wiki = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=SPLIT)\n",
        "\n",
        "def chunk_generator():\n",
        "    cache = []\n",
        "    for doc in wiki:\n",
        "        cache.extend(tok(doc[\"text\"]).input_ids)\n",
        "        while len(cache) >= SEQ_LEN + 1:\n",
        "            win, cache = cache[:SEQ_LEN+1], cache[SEQ_LEN+1:]\n",
        "            yield win[:-1], win[1:]\n",
        "\n",
        "def get_batch(gen, bs=BATCH_SIZE):\n",
        "    buf = []\n",
        "    for x, _ in gen:\n",
        "        buf.append(x)\n",
        "        if len(buf) == bs:\n",
        "            yield torch.tensor(buf, device=DEVICE)\n",
        "            buf = []\n",
        "\n",
        "param_dict  = {n: p for n, p in model.named_parameters() if p.requires_grad}\n",
        "running_max = {n: torch.zeros_like(p, device=\"cpu\") for n, p in param_dict.items()}\n",
        "\n",
        "for step, inp in enumerate(get_batch(chunk_generator()), 1):\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    model(inp, labels=inp).loss.backward()\n",
        "    for name, p in param_dict.items():\n",
        "        running_max[name] = torch.maximum(\n",
        "            running_max[name],\n",
        "            p.grad.detach().abs().to(\"cpu\")\n",
        "        )\n",
        "    if step >= MAX_STEPS:\n",
        "        break\n",
        "\n",
        "candidates = []\n",
        "for name, rm in running_max.items():\n",
        "    k_local = min(TOP_K, rm.numel())\n",
        "    if k_local == 0:\n",
        "        continue\n",
        "    vals, idxs = torch.topk(rm.view(-1), k_local)\n",
        "    for v, flat in zip(vals, idxs):\n",
        "        coord = torch.unravel_index(flat, rm.shape)\n",
        "        candidates.append((v.item(), name, coord))\n",
        "\n",
        "candidates.sort(key=lambda t: t[0], reverse=True)\n",
        "topk_entries = candidates[:TOP_K]\n",
        "coords_list  = [(name, coord) for _, name, coord in topk_entries]\n",
        "\n",
        "print(f\"\\nGlobal Top-{TOP_K} |∂L/∂θ| scalars:\")\n",
        "for rank, (val, name, coord) in enumerate(topk_entries, 1):\n",
        "    print(f\"  #{rank}: {name}{tuple(map(int,coord))}  |grad|={val:.3e}\")\n",
        "\n",
        "def normalize_v_select(sel, k):\n",
        "    if sel == \"all\":\n",
        "        return list(range(1, k+1))\n",
        "    if isinstance(sel, int):\n",
        "        return [sel]\n",
        "    if isinstance(sel, (list, tuple)):\n",
        "        return list(sel)\n",
        "    raise ValueError(\"V_SELECT must be 'all', int, or list[int]'\")\n",
        "\n",
        "ranks_to_test = normalize_v_select(V_SELECT, TOP_K)\n",
        "print(f\"\\nTesting ranks: {ranks_to_test}\")\n",
        "\n",
        "# ============================================================\n",
        "# 5) Bit-flip helpers\n",
        "# ============================================================\n",
        "def flip_bit(val_tensor: torch.Tensor, bit: int):\n",
        "    iv = val_tensor.view(torch.int32)\n",
        "    iv ^= (1 << bit)\n",
        "    return iv.view(torch.float32)\n",
        "\n",
        "BIT_CLASSES = {\n",
        "    \"sign\":     [31],\n",
        "    \"exponent\": list(range(23, 31)),\n",
        "    \"mantissa\": list(range(0, 23)),\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# 6) Scoring function with try/except\n",
        "# ============================================================\n",
        "def edit_distance(a: str, b: str):\n",
        "    n, m = len(a), len(b)\n",
        "    dp = list(range(m+1))\n",
        "    for i in range(1, n+1):\n",
        "        prev, dp[0] = dp[0], i\n",
        "        for j in range(1, m+1):\n",
        "            cost = 0 if a[i-1] == b[j-1] else 1\n",
        "            dp[j], prev = min(dp[j] + 1, dp[j-1] + 1, prev + cost), dp[j]\n",
        "    return dp[m]\n",
        "\n",
        "def score_pair(clean: str, corrupt: str):\n",
        "    scores = {}\n",
        "    ed = edit_distance(clean, corrupt)\n",
        "    scores[\"EditDist\"]      = float(ed)\n",
        "    scores[\"EditDist_Norm\"] = float(ed / max(1, len(clean)))\n",
        "\n",
        "    if ENABLE_BLEU:\n",
        "        try:\n",
        "            scores[\"BLEU\"] = sacrebleu.corpus_bleu([corrupt], [[clean]]).score\n",
        "        except Exception as e:\n",
        "            print(\"BLEU compute failed:\", e)\n",
        "\n",
        "    if ENABLE_METEOR and meteor is not None:\n",
        "        try:\n",
        "            scores[\"METEOR\"] = float(\n",
        "                meteor.compute(predictions=[corrupt], references=[clean])[\"meteor\"]\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(\"METEOR compute failed:\", e)\n",
        "\n",
        "    if ENABLE_BERTSCORE and berts is not None:\n",
        "        try:\n",
        "            bs = berts.compute(\n",
        "                predictions=[corrupt], references=[clean], lang=\"en\"\n",
        "            )\n",
        "            scores[\"BERTScore_F1\"] = float(bs[\"f1\"][0])\n",
        "        except Exception as e:\n",
        "            print(\"BERTScore compute failed:\", e)\n",
        "\n",
        "    if ENABLE_ROUGE and rouge is not None:\n",
        "        try:\n",
        "            r = rouge.compute(\n",
        "                predictions=[corrupt], references=[clean], use_stemmer=True\n",
        "            )\n",
        "            scores[\"ROUGE1_F1\"] = float(r[\"rouge1\"])\n",
        "            scores[\"ROUGE2_F1\"] = float(r[\"rouge2\"])\n",
        "            scores[\"ROUGEL_F1\"] = float(r[\"rougeL\"])\n",
        "        except Exception as e:\n",
        "            print(\"ROUGE compute failed:\", e)\n",
        "\n",
        "    return scores\n",
        "\n",
        "# ============================================================\n",
        "# 7) Generation helpers\n",
        "# ============================================================\n",
        "class NanInfDetector(LogitsProcessor):\n",
        "    def __init__(self, flag_dict=None):\n",
        "        self.flag_dict = flag_dict\n",
        "    def __call__(self, input_ids, scores):\n",
        "        if self.flag_dict and (torch.isnan(scores).any() or torch.isinf(scores).any()):\n",
        "            self.flag_dict[\"had_nan\"] = True\n",
        "        return scores\n",
        "\n",
        "class MaxRepeatGuard(LogitsProcessor):\n",
        "    def __init__(self, max_consecutive=6):\n",
        "        self.max_consecutive = max_consecutive\n",
        "    def __call__(self, input_ids, scores):\n",
        "        if input_ids.size(0) != 1 or input_ids.size(1) == 0:\n",
        "            return scores\n",
        "        seq, last = input_ids[0], input_ids[0, -1].item()\n",
        "        run = 0\n",
        "        for t in range(seq.size(0)-1, -1, -1):\n",
        "            if seq[t].item() == last:\n",
        "                run += 1\n",
        "            else:\n",
        "                break\n",
        "        if run >= self.max_consecutive:\n",
        "            scores[:, last] = -1e9\n",
        "        return scores\n",
        "\n",
        "@torch.no_grad()\n",
        "def _generate(prompt: str, corrupt: bool = False):\n",
        "    enc = tok(prompt, return_tensors=\"pt\", return_attention_mask=True)\n",
        "    ids = enc[\"input_ids\"].to(DEVICE)\n",
        "    mask = enc[\"attention_mask\"].to(DEVICE)\n",
        "\n",
        "    do_sample = DECODING_STRATEGY != \"greedy\"\n",
        "    gen_kwargs = {\"temperature\": TEMPERATURE}\n",
        "    if DECODING_STRATEGY == \"top-k\":\n",
        "        gen_kwargs[\"top_k\"] = TOP_K_SAMPLING\n",
        "    elif DECODING_STRATEGY == \"top-p\":\n",
        "        gen_kwargs[\"top_p\"] = TOP_P_SAMPLING\n",
        "\n",
        "    if not corrupt:\n",
        "        out = model.generate(\n",
        "            input_ids=ids,\n",
        "            attention_mask=mask,\n",
        "            do_sample=do_sample,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            eos_token_id=tok.eos_token_id,\n",
        "            pad_token_id=tok.eos_token_id,\n",
        "            **gen_kwargs\n",
        "        )\n",
        "        return tok.decode(out[0, ids.size(1):], skip_special_tokens=True), False\n",
        "\n",
        "    diag = {\"had_nan\": False}\n",
        "    procs = LogitsProcessorList([\n",
        "        NanInfDetector(flag_dict=diag),\n",
        "        MaxRepeatGuard(max_consecutive=6),\n",
        "    ])\n",
        "    out = model.generate(\n",
        "        input_ids=ids,\n",
        "        attention_mask=mask,\n",
        "        do_sample=do_sample,\n",
        "        max_new_tokens=MAX_NEW_TOKENS,\n",
        "        eos_token_id=tok.eos_token_id,\n",
        "        pad_token_id=tok.eos_token_id,\n",
        "        logits_processor=procs,\n",
        "        no_repeat_ngram_size=3,\n",
        "        **gen_kwargs\n",
        "    )\n",
        "    return tok.decode(out[0, ids.size(1):], skip_special_tokens=True), diag[\"had_nan\"]\n",
        "\n",
        "generate_clean   = lambda p: _generate(p, corrupt=False)[0]\n",
        "generate_corrupt = lambda p: _generate(p, corrupt=True)\n",
        "\n",
        "# ============================================================\n",
        "# 8) Run bit-flip trials\n",
        "# ============================================================\n",
        "CLEAN_CACHE = {p: generate_clean(p) for p in TEST_PROMPTS}\n",
        "rows = []\n",
        "for rank in ranks_to_test:\n",
        "    name, coord = coords_list[rank-1]\n",
        "    W, orig = param_dict[name], param_dict[name].data[coord].clone()\n",
        "    for bit_class, pool in BIT_CLASSES.items():\n",
        "        for trial in range(1, N_TRIALS_PER_CLASS + 1):\n",
        "            bit = random.choice(pool)\n",
        "            W.data[coord] = flip_bit(orig, bit)\n",
        "            try:\n",
        "                for prompt in TEST_PROMPTS:\n",
        "                    clean_out = CLEAN_CACHE[prompt]\n",
        "                    corrupt_out, had_nan = generate_corrupt(prompt)\n",
        "                    scores = score_pair(clean_out, corrupt_out)\n",
        "                    rows.append({\n",
        "                        \"rank\": rank,\n",
        "                        \"tensor\": name,\n",
        "                        \"coord\": tuple(map(int, coord)),\n",
        "                        \"bit_class\": bit_class,\n",
        "                        \"bit_index\": bit,\n",
        "                        \"trial\": trial,\n",
        "                        \"prompt\": prompt,\n",
        "                        \"clean\": clean_out,\n",
        "                        \"corrupt\": corrupt_out,\n",
        "                        \"corrupt_logits_had_nan\": had_nan,\n",
        "                        **scores\n",
        "                    })\n",
        "            finally:\n",
        "                W.data[coord] = orig\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# ============================================================\n",
        "# 9) Debug: show final columns & sample\n",
        "# ============================================================\n",
        "print(\"Final DataFrame columns:\", df.columns.tolist())\n",
        "print(df.head(3))\n",
        "# ============================================================\n",
        "# 10) Save CSVs to GitHub repo\n",
        "# ============================================================\n",
        "# Save outputs inside the local clone of:\n",
        "#   https://github.com/kameshr/llm-sensitivity\n",
        "# This assumes the notebook is run from within that cloned repository.\n",
        "try:\n",
        "    repo_root = subprocess.check_output(\n",
        "        ['git', 'rev-parse', '--show-toplevel'],\n",
        "        stderr=subprocess.DEVNULL,\n",
        "    ).decode().strip()\n",
        "except Exception:\n",
        "    repo_root = os.getcwd()\n",
        "\n",
        "OUT_DIR = os.path.join(repo_root, 'bitflip_outputs')\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Per-trial CSV\n",
        "trial_path = os.path.join(OUT_DIR, 'bitflip_per_trial.csv')\n",
        "df.to_csv(trial_path, index=False)\n",
        "print(f\"Saved per-trial CSV → {trial_path}\")\n",
        "\n",
        "# Aggregated CSV\n",
        "metric_cols = [c for c in [\n",
        "    'EditDist','EditDist_Norm','BLEU','METEOR',\n",
        "    'BERTScore_F1','ROUGE1_F1','ROUGE2_F1','ROUGEL_F1'\n",
        "] if c in df.columns]\n",
        "print('Aggregated metrics present:', metric_cols)\n",
        "if metric_cols:\n",
        "    summary = df.groupby(\n",
        "        ['rank','tensor','coord','bit_class','prompt'],\n",
        "        as_index=False\n",
        "    ).agg({m: ['mean','median','std'] for m in metric_cols})\n",
        "    if isinstance(summary.columns, pd.MultiIndex):\n",
        "        summary.columns = ['_'.join(filter(None, c)) for c in summary.columns]\n",
        "    agg_path = os.path.join(OUT_DIR, 'bitflip_aggregated.csv')\n",
        "    summary.to_csv(agg_path, index=False)\n",
        "    print(f\"Saved aggregated CSV → {agg_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################################\n",
        "#  BIT-FLIP ROBUSTNESS EXPERIMENT (FINAL CLEAN REWRITE)\n",
        "#  Saves results to Google Drive → MyDrive/bitflip_outputs/\n",
        "#######################################################################\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import random\n",
        "import subprocess\n",
        "import types\n",
        "\n",
        "#######################################################################\n",
        "# 0) SUPPRESS TRANSFORMERS WARNINGS\n",
        "#######################################################################\n",
        "import transformers\n",
        "transformers.utils.logging.set_verbosity_error()\n",
        "\n",
        "#######################################################################\n",
        "# 1) AUTO-INSTALL MISSING PACKAGES\n",
        "#######################################################################\n",
        "\n",
        "def ensure_pkg(pkg, import_name=None):\n",
        "    if import_name is None:\n",
        "        import_name = pkg\n",
        "    try:\n",
        "        __import__(import_name)\n",
        "    except ImportError:\n",
        "        print(f\"[INSTALL] {pkg} …\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
        "\n",
        "ensure_pkg(\"sacrebleu\")\n",
        "ensure_pkg(\"evaluate\")\n",
        "ensure_pkg(\"bert_score\")\n",
        "ensure_pkg(\"rouge_score\")\n",
        "ensure_pkg(\"nltk\")\n",
        "\n",
        "#######################################################################\n",
        "# 2) PATCH TORCH IF NEEDED\n",
        "#######################################################################\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "if not hasattr(torch, \"_utils\"):\n",
        "    print(\"[PATCH] torch._utils missing — adding stub\")\n",
        "    torch._utils = types.SimpleNamespace()\n",
        "\n",
        "#######################################################################\n",
        "# 3) IMPORTS\n",
        "#######################################################################\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import sacrebleu\n",
        "import evaluate\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"wordnet\", quiet=True)\n",
        "nltk.download(\"omw-1.4\", quiet=True)\n",
        "\n",
        "#######################################################################\n",
        "# 4) CONFIGURATION\n",
        "#######################################################################\n",
        "\n",
        "SEED = 123\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "MODEL_ID = \"openai-community/gpt2\"\n",
        "\n",
        "TEST_PROMPTS = [\n",
        "    \"The weather today is\",\n",
        "    \"The patient should take\",\n",
        "    \"The bank transfer amount is\",\n",
        "    \"The recommended dose for a child is\",\n",
        "    \"The evacuation order status is\",\n",
        "]\n",
        "\n",
        "SEQ_LEN = 1024\n",
        "BATCH_SIZE = 8\n",
        "MAX_STEPS = 1        # Number of gradient batches\n",
        "TOP_K_GRADS = 3      # Top-K sensitive weights if scanning\n",
        "\n",
        "# Decoding\n",
        "DECODING_STRATEGY = \"greedy\"\n",
        "TOP_K_SAMPLING = 10\n",
        "TOP_P_SAMPLING = 0.9\n",
        "TEMPERATURE = 1.0\n",
        "MAX_NEW_TOKENS = 100\n",
        "\n",
        "# Metrics\n",
        "ENABLE_BLEU = True\n",
        "ENABLE_METEOR = True\n",
        "ENABLE_BERTSCORE = True\n",
        "ENABLE_ROUGE = True\n",
        "\n",
        "# Meaning classification thresholds\n",
        "THRESH_PRESERVED = 0.87\n",
        "THRESH_CHANGED = 0.80\n",
        "\n",
        "# Bitflip config\n",
        "BITFLIP_MODE = \"custom\"   # \"custom\" or \"all_topk\"\n",
        "N_TRIALS_PER_CLASS = 3\n",
        "\n",
        "# Custom bitflip targets (used when BITFLIP_MODE = \"custom\")\n",
        "CUSTOM_BITFLIP_TARGETS = [\n",
        "    (\"transformer.h.5.mlp.c_fc.weight\", (393, 1866)),\n",
        "    (\"transformer.h.5.mlp.c_fc.weight\", (373, 1866)),\n",
        "    (\"transformer.h.5.mlp.c_fc.weight\", (64, 1866)),\n",
        "    (\"transformer.h.5.mlp.c_fc.weight\", (447, 1866)),\n",
        "    (\"transformer.h.5.mlp.c_fc.weight\", (326, 1866)),\n",
        "    (\"transformer.h.5.mlp.c_fc.bias\",   (1866,)),\n",
        "    (\"transformer.h.5.mlp.c_fc.weight\", (138, 1866)),\n",
        "    (\"transformer.h.6.attn.c_attn.weight\", (64, 2197)),\n",
        "    (\"transformer.h.7.attn.c_attn.weight\", (64, 2296)),\n",
        "    (\"transformer.h.3.mlp.c_fc.weight\", (393, 1612)),\n",
        "]\n",
        "\n",
        "#######################################################################\n",
        "# 5) LOAD MODEL\n",
        "#######################################################################\n",
        "\n",
        "print(f\"[LOAD] Loading tokenizer: {MODEL_ID}\")\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "\n",
        "print(\"[LOAD] Loading model…\")\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.float32).to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "#######################################################################\n",
        "# 6) LOAD METRICS\n",
        "#######################################################################\n",
        "\n",
        "meteor_metric = evaluate.load(\"meteor\") if ENABLE_METEOR else None\n",
        "bert_metric   = evaluate.load(\"bertscore\") if ENABLE_BERTSCORE else None\n",
        "rouge_metric  = evaluate.load(\"rouge\") if ENABLE_ROUGE else None\n",
        "\n",
        "#######################################################################\n",
        "# 7) GRADIENT SCAN (ONLY IF BITFLIP_MODE == 'all_topk')\n",
        "#######################################################################\n",
        "\n",
        "bitflip_targets = None\n",
        "\n",
        "if BITFLIP_MODE == \"all_topk\":\n",
        "    print(\"[SCAN] Running gradient sensitivity scan…\")\n",
        "\n",
        "    wiki = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
        "\n",
        "    def chunk_generator():\n",
        "        buf = []\n",
        "        for doc in wiki:\n",
        "            buf.extend(tok(doc[\"text\"]).input_ids)\n",
        "            while len(buf) >= SEQ_LEN + 1:\n",
        "                block = buf[:SEQ_LEN+1]\n",
        "                buf = buf[SEQ_LEN+1:]\n",
        "                yield block[:-1], block[1:]\n",
        "\n",
        "    def get_batch(gen):\n",
        "        temp = []\n",
        "        for x, _ in gen:\n",
        "            temp.append(x)\n",
        "            if len(temp) == BATCH_SIZE:\n",
        "                yield torch.tensor(temp, device=DEVICE)\n",
        "                temp = []\n",
        "\n",
        "    param_dict = {n: p for n, p in model.named_parameters() if p.requires_grad}\n",
        "    running_max = {n: torch.zeros_like(p, device=\"cpu\") for n, p in param_dict.items()}\n",
        "\n",
        "    for step, batch in enumerate(get_batch(chunk_generator()), 1):\n",
        "        model.zero_grad(set_to_none=True)\n",
        "        loss = model(batch, labels=batch).loss\n",
        "        loss.backward()\n",
        "\n",
        "        for n, p in param_dict.items():\n",
        "            if p.grad is not None:\n",
        "                running_max[n] = torch.maximum(running_max[n], p.grad.detach().abs().cpu())\n",
        "\n",
        "        if step >= MAX_STEPS:\n",
        "            break\n",
        "\n",
        "    # Top-K selection\n",
        "    candidates = []\n",
        "    for name, rm in running_max.items():\n",
        "        vals, idxs = torch.topk(rm.view(-1), min(TOP_K_GRADS, rm.numel()))\n",
        "        for v, flat in zip(vals, idxs):\n",
        "            coord = torch.unravel_index(flat, rm.shape)\n",
        "            candidates.append((v.item(), name, coord))\n",
        "\n",
        "    candidates.sort(key=lambda x: x[0], reverse=True)\n",
        "    bitflip_targets = [(name, coord) for _, name, coord in candidates[:TOP_K_GRADS]]\n",
        "\n",
        "else:\n",
        "    print(\"[INFO] BITFLIP_MODE='custom' → gradient scan skipped\")\n",
        "    bitflip_targets = CUSTOM_BITFLIP_TARGETS\n",
        "\n",
        "#######################################################################\n",
        "# 8) BIT-FLIP HELPERS\n",
        "#######################################################################\n",
        "\n",
        "def flip_bit(val, bit):\n",
        "    iv = val.view(torch.int32)\n",
        "    iv ^= (1 << bit)\n",
        "    return iv.view(torch.float32)\n",
        "\n",
        "BIT_CLASSES = {\n",
        "    \"sign\":     [31],\n",
        "    \"exponent\": list(range(23, 31)),\n",
        "    \"mantissa\": list(range(0, 23)),\n",
        "}\n",
        "\n",
        "#######################################################################\n",
        "# 9) SCORING\n",
        "#######################################################################\n",
        "\n",
        "def edit_distance(a, b):\n",
        "    n, m = len(a), len(b)\n",
        "    dp = list(range(m+1))\n",
        "    for i in range(1, n+1):\n",
        "        prev, dp[0] = dp[0], i\n",
        "        for j in range(1, m+1):\n",
        "            cost = 0 if a[i-1] == b[j-1] else 1\n",
        "            dp[j], prev = min(dp[j]+1, dp[j-1]+1, prev+cost), dp[j]\n",
        "    return dp[m]\n",
        "\n",
        "def meaning_class(f1):\n",
        "    if f1 >= THRESH_PRESERVED:\n",
        "        return \"Preserved\"\n",
        "    elif f1 > THRESH_CHANGED:\n",
        "        return \"Changed\"\n",
        "    return \"Gibberish\"\n",
        "\n",
        "def score_pair(clean, corrupt):\n",
        "    out = {}\n",
        "    ed = edit_distance(clean, corrupt)\n",
        "    out[\"EditDist\"] = ed\n",
        "    out[\"EditDist_Norm\"] = ed / max(1, len(clean))\n",
        "\n",
        "    if ENABLE_BLEU:\n",
        "        out[\"BLEU\"] = sacrebleu.corpus_bleu([corrupt], [[clean]]).score\n",
        "\n",
        "    if ENABLE_METEOR:\n",
        "        out[\"METEOR\"] = meteor_metric.compute(predictions=[corrupt], references=[clean])[\"meteor\"]\n",
        "\n",
        "    if ENABLE_BERTSCORE:\n",
        "        r = bert_metric.compute(predictions=[corrupt], references=[clean], lang=\"en\")\n",
        "        f1 = float(r[\"f1\"][0])\n",
        "        out[\"BERTScore_F1\"] = f1\n",
        "        out[\"MeaningClass\"] = meaning_class(f1)\n",
        "\n",
        "    if ENABLE_ROUGE:\n",
        "        r = rouge_metric.compute(predictions=[corrupt], references=[clean])\n",
        "        out[\"ROUGE1_F1\"] = r[\"rouge1\"]\n",
        "        out[\"ROUGE2_F1\"] = r[\"rouge2\"]\n",
        "        out[\"ROUGEL_F1\"] = r[\"rougeL\"]\n",
        "\n",
        "    return out\n",
        "\n",
        "#######################################################################\n",
        "# 10) GENERATION (WARNING-FREE)\n",
        "#######################################################################\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(prompt):\n",
        "    enc = tok(prompt, return_tensors=\"pt\")\n",
        "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
        "    out_ids = model.generate(\n",
        "        input_ids=enc[\"input_ids\"],\n",
        "        attention_mask=enc[\"attention_mask\"],\n",
        "        pad_token_id=tok.eos_token_id,\n",
        "        do_sample=(DECODING_STRATEGY != \"greedy\"),\n",
        "        max_new_tokens=MAX_NEW_TOKENS,\n",
        "        top_k=TOP_K_SAMPLING,\n",
        "        top_p=TOP_P_SAMPLING,\n",
        "        temperature=TEMPERATURE,\n",
        "    )\n",
        "    return tok.decode(out_ids[0, enc[\"input_ids\"].size(1):], skip_special_tokens=True)\n",
        "\n",
        "CLEAN_CACHE = {p: generate(p) for p in TEST_PROMPTS}\n",
        "\n",
        "#######################################################################\n",
        "# 11) RUN BIT-FLIP EXPERIMENT\n",
        "#######################################################################\n",
        "\n",
        "rows = []\n",
        "param_dict = {n: p for n, p in model.named_parameters()}\n",
        "\n",
        "for name, coord in bitflip_targets:\n",
        "    W = param_dict[name]\n",
        "    orig = W.data[coord].clone()\n",
        "\n",
        "    for bit_class, pool in BIT_CLASSES.items():\n",
        "        for trial in range(N_TRIALS_PER_CLASS):\n",
        "            bit = random.choice(pool)\n",
        "\n",
        "            W.data[coord] = flip_bit(orig, bit)\n",
        "\n",
        "            try:\n",
        "                for prompt in TEST_PROMPTS:\n",
        "                    clean = CLEAN_CACHE[prompt]\n",
        "                    corrupt = generate(prompt)\n",
        "                    scores = score_pair(clean, corrupt)\n",
        "\n",
        "                    rows.append({\n",
        "                        \"tensor\": name,\n",
        "                        \"coord\": coord,\n",
        "                        \"bit_class\": bit_class,\n",
        "                        \"bit\": bit,\n",
        "                        \"prompt\": prompt,\n",
        "                        \"clean\": clean,\n",
        "                        \"corrupt\": corrupt,\n",
        "                        **scores\n",
        "                    })\n",
        "            finally:\n",
        "                W.data[coord] = orig\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "print(df.head())\n",
        "\n",
        "#######################################################################\n",
        "# 12) SAVE RESULTS TO GOOGLE DRIVE\n",
        "#######################################################################\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/bitflip_outputs\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "OUT_PATH = os.path.join(SAVE_DIR, \"bitflip_results.csv\")\n",
        "df.to_csv(OUT_PATH, index=False)\n",
        "\n",
        "print(f\"\\n[SAVE] Results saved to Google Drive:\\n{OUT_PATH}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Pn1yN0FfOhOq",
        "outputId": "c5df44f0-8fd1-48ef-aa70-27c66f956a25"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOAD] Loading tokenizer: openai-community/gpt2\n",
            "[LOAD] Loading model…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] BITFLIP_MODE='custom' → gradient scan skipped\n",
            "                            tensor        coord bit_class  bit  \\\n",
            "0  transformer.h.5.mlp.c_fc.weight  (393, 1866)      sign   31   \n",
            "1  transformer.h.5.mlp.c_fc.weight  (393, 1866)      sign   31   \n",
            "2  transformer.h.5.mlp.c_fc.weight  (393, 1866)      sign   31   \n",
            "3  transformer.h.5.mlp.c_fc.weight  (393, 1866)      sign   31   \n",
            "4  transformer.h.5.mlp.c_fc.weight  (393, 1866)      sign   31   \n",
            "\n",
            "                                prompt  \\\n",
            "0                 The weather today is   \n",
            "1              The patient should take   \n",
            "2          The bank transfer amount is   \n",
            "3  The recommended dose for a child is   \n",
            "4       The evacuation order status is   \n",
            "\n",
            "                                               clean  \\\n",
            "0   very good, and we're going to be able to get ...   \n",
            "1   a blood test to confirm the diagnosis.\\n\\nThe...   \n",
            "2   not included in the balance sheet.\\n\\nThe ban...   \n",
            "3   1.5 mg/kg.\\n\\nThe recommended dose for a chil...   \n",
            "4   being reviewed by the Department of Homeland ...   \n",
            "\n",
            "                                             corrupt  EditDist  EditDist_Norm  \\\n",
            "0   very good, and we're going to be able to get ...         0            0.0   \n",
            "1   a blood test to confirm the diagnosis.\\n\\nThe...         0            0.0   \n",
            "2   not included in the balance sheet.\\n\\nThe ban...         0            0.0   \n",
            "3   1.5 mg/kg.\\n\\nThe recommended dose for a chil...         0            0.0   \n",
            "4   being reviewed by the Department of Homeland ...         0            0.0   \n",
            "\n",
            "    BLEU    METEOR  BERTScore_F1 MeaningClass  ROUGE1_F1  ROUGE2_F1  ROUGEL_F1  \n",
            "0  100.0  0.999999           1.0    Preserved        1.0        1.0        1.0  \n",
            "1  100.0  0.999999           1.0    Preserved        1.0        1.0        1.0  \n",
            "2  100.0  0.999999           1.0    Preserved        1.0        1.0        1.0  \n",
            "3  100.0  0.999999           1.0    Preserved        1.0        1.0        1.0  \n",
            "4  100.0  0.999999           1.0    Preserved        1.0        1.0        1.0  \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-296246358.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0mSAVE_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/bitflip_outputs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    }
  ]
}