{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# GPT-2 Prompt Runner\n",
        "\n",
        "This Colab notebook installs Hugging Face Transformers, loads a GPT-2 model, and lets you enter a free-form prompt. Run the cells from top to bottom, type your prompt when asked, and the model's continuation will be printed in the final cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "# Install the lightweight dependencies needed for GPT-2.\n",
        "!pip install --quiet transformers torch accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "MODEL_NAME = \"gpt2\"\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "device_name = 'GPU' if device == 0 else 'CPU'\n",
        "print(f\"Loaded {MODEL_NAME} on {device_name}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_prompt"
      },
      "outputs": [],
      "source": [
        "prompt = input(\"Enter a prompt for GPT-2: \").strip()\n",
        "if not prompt:\n",
        "    raise ValueError(\"Please enter a non-empty prompt.\")\n",
        "\n",
        "max_new_tokens = 120  # Adjust to control continuation length\n",
        "outputs = generator(\n",
        "    prompt,\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    do_sample=True,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        ")\n",
        "\n",
        "generated_text = outputs[0][\"generated_text\"]\n",
        "print(\"\\n=== Model Output ===\")\n",
        "print(generated_text)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "name": "gpt2_prompt_runner.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}