{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOZ0K7r0hjRmdQZZf9KLOb6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzboobhcXnuD",
        "outputId": "1407ba0f-97a8-4125-e3a7-94ada1807642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top-250 most sensitive tensor elements:\n",
            "  #1: transformer.wte.weight(2488, 496)  |grad|=5.235e+00\n",
            "  #2: transformer.wte.weight(837, 496)  |grad|=4.481e+00\n",
            "  #3: transformer.wte.weight(198, 496)  |grad|=3.247e+00\n",
            "  #4: transformer.wte.weight(11, 496)  |grad|=2.898e+00\n",
            "  #5: transformer.wte.weight(34315, 496)  |grad|=2.669e+00\n",
            "  #6: transformer.wte.weight(796, 496)  |grad|=2.517e+00\n",
            "  #7: transformer.wte.weight(764, 496)  |grad|=2.499e+00\n",
            "  #8: transformer.wte.weight(13, 496)  |grad|=2.427e+00\n",
            "  #9: transformer.wte.weight(220, 496)  |grad|=2.242e+00\n",
            "  #10: transformer.wte.weight(2488, 430)  |grad|=2.207e+00\n",
            "  #11: transformer.wte.weight(198, 430)  |grad|=2.154e+00\n",
            "  #12: transformer.wte.weight(31, 496)  |grad|=2.059e+00\n",
            "  #13: transformer.wte.weight(5187, 496)  |grad|=2.046e+00\n",
            "  #14: transformer.wte.weight(49063, 496)  |grad|=2.029e+00\n",
            "  #15: transformer.wte.weight(27583, 496)  |grad|=2.001e+00\n",
            "  #16: transformer.wte.weight(837, 430)  |grad|=1.993e+00\n",
            "  #17: transformer.wte.weight(6645, 496)  |grad|=1.939e+00\n",
            "  #18: transformer.wte.weight(21165, 496)  |grad|=1.789e+00\n",
            "  #19: transformer.wte.weight(15281, 496)  |grad|=1.745e+00\n",
            "  #20: transformer.wte.weight(3056, 496)  |grad|=1.702e+00\n",
            "  #21: transformer.wte.weight(12164, 496)  |grad|=1.672e+00\n",
            "  #22: transformer.wte.weight(198, 36)  |grad|=1.671e+00\n",
            "  #23: transformer.wte.weight(497, 496)  |grad|=1.666e+00\n",
            "  #24: transformer.wte.weight(2488, 36)  |grad|=1.663e+00\n",
            "  #25: transformer.wte.weight(21371, 496)  |grad|=1.626e+00\n",
            "  #26: transformer.wte.weight(449, 496)  |grad|=1.624e+00\n",
            "  #27: transformer.wte.weight(40902, 496)  |grad|=1.597e+00\n",
            "  #28: transformer.wte.weight(8670, 496)  |grad|=1.562e+00\n",
            "  #29: transformer.wte.weight(3999, 496)  |grad|=1.561e+00\n",
            "  #30: transformer.wte.weight(5178, 496)  |grad|=1.556e+00\n",
            "  #31: transformer.wte.weight(8942, 496)  |grad|=1.540e+00\n",
            "  #32: transformer.wte.weight(837, 36)  |grad|=1.508e+00\n",
            "  #33: transformer.wte.weight(4544, 496)  |grad|=1.505e+00\n",
            "  #34: transformer.wte.weight(262, 496)  |grad|=1.470e+00\n",
            "  #35: transformer.wte.weight(8118, 496)  |grad|=1.467e+00\n",
            "  #36: transformer.wte.weight(18884, 496)  |grad|=1.432e+00\n",
            "  #37: transformer.wte.weight(12108, 496)  |grad|=1.377e+00\n",
            "  #38: transformer.wte.weight(27512, 496)  |grad|=1.364e+00\n",
            "  #39: transformer.wte.weight(44665, 496)  |grad|=1.353e+00\n",
            "  #40: transformer.wte.weight(23214, 496)  |grad|=1.348e+00\n",
            "  #41: transformer.wte.weight(366, 496)  |grad|=1.336e+00\n",
            "  #42: transformer.wte.weight(796, 430)  |grad|=1.317e+00\n",
            "  #43: transformer.wte.weight(45674, 496)  |grad|=1.313e+00\n",
            "  #44: transformer.wte.weight(16825, 496)  |grad|=1.310e+00\n",
            "  #45: transformer.wte.weight(19498, 496)  |grad|=1.309e+00\n",
            "  #46: transformer.wte.weight(15767, 496)  |grad|=1.303e+00\n",
            "  #47: transformer.wte.weight(28972, 496)  |grad|=1.301e+00\n",
            "  #48: transformer.wte.weight(11, 430)  |grad|=1.279e+00\n",
            "  #49: transformer.wte.weight(34315, 430)  |grad|=1.277e+00\n",
            "  #50: transformer.wte.weight(3248, 496)  |grad|=1.268e+00\n",
            "  #51: transformer.wte.weight(37470, 496)  |grad|=1.264e+00\n",
            "  #52: transformer.wte.weight(10849, 496)  |grad|=1.261e+00\n",
            "  #53: transformer.wte.weight(11849, 496)  |grad|=1.254e+00\n",
            "  #54: transformer.wte.weight(37885, 496)  |grad|=1.243e+00\n",
            "  #55: transformer.wte.weight(39452, 496)  |grad|=1.242e+00\n",
            "  #56: transformer.wte.weight(10278, 496)  |grad|=1.232e+00\n",
            "  #57: transformer.wte.weight(29285, 496)  |grad|=1.228e+00\n",
            "  #58: transformer.wte.weight(2448, 496)  |grad|=1.222e+00\n",
            "  #59: transformer.wte.weight(9398, 496)  |grad|=1.221e+00\n",
            "  #60: transformer.wte.weight(4960, 496)  |grad|=1.216e+00\n",
            "  #61: transformer.wte.weight(6445, 496)  |grad|=1.216e+00\n",
            "  #62: transformer.wte.weight(12592, 496)  |grad|=1.210e+00\n",
            "  #63: transformer.wte.weight(11565, 496)  |grad|=1.206e+00\n",
            "  #64: transformer.wte.weight(2039, 496)  |grad|=1.196e+00\n",
            "  #65: transformer.wte.weight(286, 496)  |grad|=1.191e+00\n",
            "  #66: transformer.wte.weight(19439, 496)  |grad|=1.183e+00\n",
            "  #67: transformer.wte.weight(564, 496)  |grad|=1.180e+00\n",
            "  #68: transformer.wte.weight(26451, 496)  |grad|=1.178e+00\n",
            "  #69: transformer.wte.weight(14818, 496)  |grad|=1.177e+00\n",
            "  #70: transformer.wte.weight(39261, 496)  |grad|=1.170e+00\n",
            "  #71: transformer.wte.weight(40929, 496)  |grad|=1.163e+00\n",
            "  #72: transformer.wte.weight(11852, 496)  |grad|=1.161e+00\n",
            "  #73: transformer.wte.weight(4302, 496)  |grad|=1.153e+00\n",
            "  #74: transformer.wte.weight(39601, 496)  |grad|=1.144e+00\n",
            "  #75: transformer.wte.weight(220, 430)  |grad|=1.144e+00\n",
            "  #76: transformer.wte.weight(15518, 496)  |grad|=1.142e+00\n",
            "  #77: transformer.wte.weight(1279, 496)  |grad|=1.133e+00\n",
            "  #78: transformer.wte.weight(8225, 496)  |grad|=1.132e+00\n",
            "  #79: transformer.wte.weight(13, 430)  |grad|=1.120e+00\n",
            "  #80: transformer.wte.weight(18551, 496)  |grad|=1.119e+00\n",
            "  #81: transformer.wte.weight(38237, 496)  |grad|=1.114e+00\n",
            "  #82: transformer.wte.weight(2635, 496)  |grad|=1.112e+00\n",
            "  #83: transformer.wte.weight(25656, 496)  |grad|=1.112e+00\n",
            "  #84: transformer.wte.weight(338, 496)  |grad|=1.103e+00\n",
            "  #85: transformer.wte.weight(764, 430)  |grad|=1.102e+00\n",
            "  #86: transformer.wte.weight(3240, 496)  |grad|=1.102e+00\n",
            "  #87: transformer.wte.weight(30498, 496)  |grad|=1.095e+00\n",
            "  #88: transformer.wte.weight(35259, 496)  |grad|=1.094e+00\n",
            "  #89: transformer.wte.weight(24023, 496)  |grad|=1.087e+00\n",
            "  #90: transformer.wte.weight(44302, 496)  |grad|=1.085e+00\n",
            "  #91: transformer.wte.weight(27583, 430)  |grad|=1.084e+00\n",
            "  #92: transformer.wte.weight(28437, 496)  |grad|=1.083e+00\n",
            "  #93: transformer.wte.weight(3592, 496)  |grad|=1.083e+00\n",
            "  #94: transformer.wte.weight(39644, 496)  |grad|=1.082e+00\n",
            "  #95: transformer.wte.weight(33911, 496)  |grad|=1.077e+00\n",
            "  #96: transformer.wte.weight(10880, 496)  |grad|=1.076e+00\n",
            "  #97: transformer.wte.weight(20404, 496)  |grad|=1.074e+00\n",
            "  #98: transformer.wte.weight(705, 496)  |grad|=1.072e+00\n",
            "  #99: transformer.wte.weight(796, 36)  |grad|=1.065e+00\n",
            "  #100: transformer.wte.weight(29490, 496)  |grad|=1.061e+00\n",
            "  #101: transformer.wte.weight(46906, 496)  |grad|=1.060e+00\n",
            "  #102: transformer.wte.weight(24872, 496)  |grad|=1.057e+00\n",
            "  #103: transformer.wte.weight(287, 496)  |grad|=1.055e+00\n",
            "  #104: transformer.wte.weight(47891, 496)  |grad|=1.049e+00\n",
            "  #105: transformer.wte.weight(257, 496)  |grad|=1.048e+00\n",
            "  #106: transformer.wte.weight(1375, 496)  |grad|=1.042e+00\n",
            "  #107: transformer.wte.weight(978, 496)  |grad|=1.041e+00\n",
            "  #108: transformer.wte.weight(17099, 496)  |grad|=1.037e+00\n",
            "  #109: transformer.wte.weight(10769, 496)  |grad|=1.036e+00\n",
            "  #110: transformer.wte.weight(1665, 496)  |grad|=1.030e+00\n",
            "  #111: transformer.wte.weight(9719, 496)  |grad|=1.022e+00\n",
            "  #112: transformer.wte.weight(8599, 496)  |grad|=1.017e+00\n",
            "  #113: transformer.wte.weight(27132, 496)  |grad|=1.015e+00\n",
            "  #114: transformer.wte.weight(11, 36)  |grad|=1.015e+00\n",
            "  #115: transformer.wte.weight(25979, 496)  |grad|=1.012e+00\n",
            "  #116: transformer.wte.weight(3261, 496)  |grad|=1.007e+00\n",
            "  #117: transformer.wte.weight(38580, 496)  |grad|=9.963e-01\n",
            "  #118: transformer.wte.weight(11523, 496)  |grad|=9.962e-01\n",
            "  #119: transformer.wte.weight(3687, 496)  |grad|=9.959e-01\n",
            "  #120: transformer.wte.weight(32684, 496)  |grad|=9.945e-01\n",
            "  #121: transformer.wte.weight(14780, 496)  |grad|=9.882e-01\n",
            "  #122: transformer.wte.weight(32831, 496)  |grad|=9.881e-01\n",
            "  #123: transformer.wte.weight(20531, 496)  |grad|=9.869e-01\n",
            "  #124: transformer.wte.weight(9718, 496)  |grad|=9.805e-01\n",
            "  #125: transformer.wte.weight(1379, 496)  |grad|=9.805e-01\n",
            "  #126: transformer.wte.weight(20842, 496)  |grad|=9.728e-01\n",
            "  #127: transformer.wte.weight(7096, 496)  |grad|=9.698e-01\n",
            "  #128: transformer.wte.weight(13709, 496)  |grad|=9.697e-01\n",
            "  #129: transformer.wte.weight(14074, 496)  |grad|=9.689e-01\n",
            "  #130: transformer.wte.weight(46547, 496)  |grad|=9.649e-01\n",
            "  #131: transformer.wte.weight(304, 496)  |grad|=9.643e-01\n",
            "  #132: transformer.wte.weight(49063, 430)  |grad|=9.596e-01\n",
            "  #133: transformer.wte.weight(220, 36)  |grad|=9.595e-01\n",
            "  #134: transformer.wte.weight(34315, 36)  |grad|=9.577e-01\n",
            "  #135: transformer.wte.weight(32164, 496)  |grad|=9.575e-01\n",
            "  #136: transformer.wte.weight(21679, 496)  |grad|=9.564e-01\n",
            "  #137: transformer.wte.weight(46619, 496)  |grad|=9.518e-01\n",
            "  #138: transformer.wte.weight(46446, 496)  |grad|=9.491e-01\n",
            "  #139: transformer.wte.weight(2046, 496)  |grad|=9.488e-01\n",
            "  #140: transformer.wte.weight(6252, 496)  |grad|=9.469e-01\n",
            "  #141: transformer.wte.weight(4991, 496)  |grad|=9.458e-01\n",
            "  #142: transformer.wte.weight(9502, 496)  |grad|=9.456e-01\n",
            "  #143: transformer.wte.weight(5686, 496)  |grad|=9.453e-01\n",
            "  #144: transformer.wte.weight(40580, 496)  |grad|=9.438e-01\n",
            "  #145: transformer.wte.weight(3905, 496)  |grad|=9.426e-01\n",
            "  #146: transformer.wte.weight(46811, 496)  |grad|=9.419e-01\n",
            "  #147: transformer.wte.weight(5838, 496)  |grad|=9.387e-01\n",
            "  #148: transformer.wte.weight(12686, 496)  |grad|=9.378e-01\n",
            "  #149: transformer.wte.weight(15608, 496)  |grad|=9.364e-01\n",
            "  #150: transformer.wte.weight(6285, 496)  |grad|=9.331e-01\n",
            "  #151: transformer.wte.weight(314, 496)  |grad|=9.325e-01\n",
            "  #152: transformer.wte.weight(10343, 496)  |grad|=9.322e-01\n",
            "  #153: transformer.wte.weight(37449, 496)  |grad|=9.305e-01\n",
            "  #154: transformer.wte.weight(15281, 430)  |grad|=9.274e-01\n",
            "  #155: transformer.wte.weight(27599, 496)  |grad|=9.240e-01\n",
            "  #156: transformer.wte.weight(43579, 496)  |grad|=9.174e-01\n",
            "  #157: transformer.wte.weight(47922, 496)  |grad|=9.140e-01\n",
            "  #158: transformer.wte.weight(5108, 496)  |grad|=9.011e-01\n",
            "  #159: transformer.wte.weight(13098, 496)  |grad|=8.999e-01\n",
            "  #160: transformer.wte.weight(1757, 496)  |grad|=8.985e-01\n",
            "  #161: transformer.wte.weight(14328, 496)  |grad|=8.973e-01\n",
            "  #162: transformer.wte.weight(13, 36)  |grad|=8.964e-01\n",
            "  #163: transformer.wte.weight(5187, 430)  |grad|=8.959e-01\n",
            "  #164: transformer.wte.weight(520, 496)  |grad|=8.954e-01\n",
            "  #165: transformer.wte.weight(290, 496)  |grad|=8.946e-01\n",
            "  #166: transformer.wte.weight(2646, 496)  |grad|=8.944e-01\n",
            "  #167: transformer.wte.weight(9552, 496)  |grad|=8.927e-01\n",
            "  #168: transformer.wte.weight(764, 36)  |grad|=8.926e-01\n",
            "  #169: transformer.wte.weight(31, 430)  |grad|=8.916e-01\n",
            "  #170: transformer.wte.weight(20603, 496)  |grad|=8.907e-01\n",
            "  #171: transformer.wte.weight(5628, 496)  |grad|=8.890e-01\n",
            "  #172: transformer.wte.weight(5706, 496)  |grad|=8.883e-01\n",
            "  #173: transformer.wte.weight(47489, 496)  |grad|=8.847e-01\n",
            "  #174: transformer.wte.weight(289, 496)  |grad|=8.846e-01\n",
            "  #175: transformer.wte.weight(7158, 496)  |grad|=8.829e-01\n",
            "  #176: transformer.wte.weight(645, 496)  |grad|=8.827e-01\n",
            "  #177: transformer.wte.weight(26249, 496)  |grad|=8.803e-01\n",
            "  #178: transformer.wte.weight(8765, 496)  |grad|=8.784e-01\n",
            "  #179: transformer.wte.weight(46626, 496)  |grad|=8.773e-01\n",
            "  #180: transformer.wte.weight(7653, 496)  |grad|=8.723e-01\n",
            "  #181: transformer.wte.weight(49931, 496)  |grad|=8.721e-01\n",
            "  #182: transformer.wte.weight(45630, 496)  |grad|=8.713e-01\n",
            "  #183: transformer.wte.weight(9450, 496)  |grad|=8.711e-01\n",
            "  #184: transformer.wte.weight(10977, 496)  |grad|=8.705e-01\n",
            "  #185: transformer.wte.weight(16140, 496)  |grad|=8.688e-01\n",
            "  #186: transformer.wte.weight(1210, 496)  |grad|=8.686e-01\n",
            "  #187: transformer.h.5.ln_1.weight(447,)  |grad|=8.676e-01\n",
            "  #188: transformer.wte.weight(34927, 496)  |grad|=8.674e-01\n",
            "  #189: transformer.wte.weight(33298, 496)  |grad|=8.673e-01\n",
            "  #190: transformer.wte.weight(449, 430)  |grad|=8.654e-01\n",
            "  #191: transformer.wte.weight(5915, 496)  |grad|=8.639e-01\n",
            "  #192: transformer.wte.weight(9666, 496)  |grad|=8.585e-01\n",
            "  #193: transformer.wte.weight(6645, 430)  |grad|=8.585e-01\n",
            "  #194: transformer.wte.weight(15945, 496)  |grad|=8.571e-01\n",
            "  #195: transformer.wte.weight(10169, 496)  |grad|=8.543e-01\n",
            "  #196: transformer.wte.weight(8050, 496)  |grad|=8.541e-01\n",
            "  #197: transformer.wte.weight(42192, 496)  |grad|=8.536e-01\n",
            "  #198: transformer.wte.weight(13922, 496)  |grad|=8.536e-01\n",
            "  #199: transformer.wte.weight(784, 496)  |grad|=8.513e-01\n",
            "  #200: transformer.wte.weight(284, 496)  |grad|=8.504e-01\n",
            "  #201: transformer.wte.weight(4847, 496)  |grad|=8.500e-01\n",
            "  #202: transformer.wte.weight(2089, 496)  |grad|=8.476e-01\n",
            "  #203: transformer.wte.weight(23975, 496)  |grad|=8.460e-01\n",
            "  #204: transformer.wte.weight(554, 496)  |grad|=8.457e-01\n",
            "  #205: transformer.wte.weight(31210, 496)  |grad|=8.455e-01\n",
            "  #206: transformer.wte.weight(12469, 496)  |grad|=8.453e-01\n",
            "  #207: transformer.wte.weight(5780, 496)  |grad|=8.446e-01\n",
            "  #208: transformer.wte.weight(7459, 496)  |grad|=8.426e-01\n",
            "  #209: transformer.wte.weight(19305, 496)  |grad|=8.424e-01\n",
            "  #210: transformer.wte.weight(4319, 496)  |grad|=8.416e-01\n",
            "  #211: transformer.wte.weight(12164, 430)  |grad|=8.411e-01\n",
            "  #212: transformer.wte.weight(1971, 496)  |grad|=8.409e-01\n",
            "  #213: transformer.wte.weight(20400, 496)  |grad|=8.388e-01\n",
            "  #214: transformer.wte.weight(983, 496)  |grad|=8.379e-01\n",
            "  #215: transformer.wte.weight(47268, 496)  |grad|=8.378e-01\n",
            "  #216: transformer.wte.weight(35293, 496)  |grad|=8.359e-01\n",
            "  #217: transformer.wte.weight(3831, 496)  |grad|=8.320e-01\n",
            "  #218: transformer.wte.weight(5030, 496)  |grad|=8.316e-01\n",
            "  #219: transformer.wte.weight(791, 496)  |grad|=8.315e-01\n",
            "  #220: transformer.wte.weight(5825, 496)  |grad|=8.294e-01\n",
            "  #221: transformer.wte.weight(32355, 496)  |grad|=8.285e-01\n",
            "  #222: transformer.wte.weight(25176, 496)  |grad|=8.264e-01\n",
            "  #223: transformer.wte.weight(8685, 496)  |grad|=8.260e-01\n",
            "  #224: transformer.wte.weight(6046, 496)  |grad|=8.231e-01\n",
            "  #225: transformer.wte.weight(49203, 496)  |grad|=8.198e-01\n",
            "  #226: transformer.wte.weight(13257, 496)  |grad|=8.195e-01\n",
            "  #227: transformer.wte.weight(3208, 496)  |grad|=8.150e-01\n",
            "  #228: transformer.wte.weight(6881, 496)  |grad|=8.148e-01\n",
            "  #229: transformer.wte.weight(9634, 496)  |grad|=8.129e-01\n",
            "  #230: transformer.wte.weight(2644, 496)  |grad|=8.085e-01\n",
            "  #231: transformer.wte.weight(7164, 496)  |grad|=8.080e-01\n",
            "  #232: transformer.wte.weight(20716, 496)  |grad|=8.070e-01\n",
            "  #233: transformer.wte.weight(2284, 496)  |grad|=8.067e-01\n",
            "  #234: transformer.wte.weight(11287, 496)  |grad|=8.064e-01\n",
            "  #235: transformer.wte.weight(37884, 496)  |grad|=8.057e-01\n",
            "  #236: transformer.wte.weight(12551, 496)  |grad|=8.045e-01\n",
            "  #237: transformer.wte.weight(6107, 496)  |grad|=8.045e-01\n",
            "  #238: transformer.wte.weight(7979, 496)  |grad|=8.044e-01\n",
            "  #239: transformer.wte.weight(12099, 496)  |grad|=8.011e-01\n",
            "  #240: transformer.wte.weight(2688, 496)  |grad|=7.983e-01\n",
            "  #241: transformer.wte.weight(7897, 496)  |grad|=7.979e-01\n",
            "  #242: transformer.wte.weight(18501, 496)  |grad|=7.958e-01\n",
            "  #243: transformer.wte.weight(9281, 496)  |grad|=7.921e-01\n",
            "  #244: transformer.wte.weight(21165, 430)  |grad|=7.920e-01\n",
            "  #245: transformer.wte.weight(1126, 496)  |grad|=7.908e-01\n",
            "  #246: transformer.wte.weight(27583, 36)  |grad|=7.901e-01\n",
            "  #247: transformer.wte.weight(39775, 496)  |grad|=7.892e-01\n",
            "  #248: transformer.wte.weight(7920, 496)  |grad|=7.873e-01\n",
            "  #249: transformer.wte.weight(37053, 496)  |grad|=7.863e-01\n",
            "  #250: transformer.wte.weight(12862, 496)  |grad|=7.852e-01\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 0) Setup\n",
        "# ============================================================\n",
        "import torch, random\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
        "\n",
        "# ---- Config ----\n",
        "SEED       = 123\n",
        "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_ID   = \"gpt2\"\n",
        "SPLIT      = \"train\"\n",
        "SEQ_LEN    = 1024\n",
        "BATCH_SIZE = 16\n",
        "MAX_STEPS  = 1100\n",
        "TOP_K      = 250  # report top-k most sensitive elements\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# ============================================================\n",
        "# 1) Model & Tokenizer\n",
        "# ============================================================\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "tok.pad_token = tok.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
        "model.train()  # need gradients\n",
        "\n",
        "# ============================================================\n",
        "# 2) Dataset & Chunking\n",
        "# ============================================================\n",
        "wiki = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=SPLIT)\n",
        "\n",
        "def chunk_generator():\n",
        "    cache = []\n",
        "    for doc in wiki:\n",
        "        cache.extend(tok(doc[\"text\"]).input_ids)\n",
        "        while len(cache) >= SEQ_LEN + 1:\n",
        "            win, cache = cache[:SEQ_LEN+1], cache[SEQ_LEN+1:]  # non-overlap\n",
        "            yield win[:-1], win[1:]\n",
        "\n",
        "def get_batch(gen, bs=BATCH_SIZE):\n",
        "    buf = []\n",
        "    for x, _ in gen:\n",
        "        buf.append(x)\n",
        "        if len(buf) == bs:\n",
        "            yield torch.tensor(buf, device=DEVICE)\n",
        "            buf = []\n",
        "\n",
        "# ============================================================\n",
        "# 3) Gradient Scan\n",
        "# ============================================================\n",
        "param_dict  = {n: p for n, p in model.named_parameters() if p.requires_grad}\n",
        "running_max = {n: torch.zeros_like(p, device=\"cpu\") for n, p in param_dict.items()}\n",
        "\n",
        "for step, inp in enumerate(get_batch(chunk_generator()), 1):\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    loss = model(inp, labels=inp).loss\n",
        "    loss.backward()\n",
        "    for name, p in param_dict.items():\n",
        "        running_max[name] = torch.maximum(\n",
        "            running_max[name],\n",
        "            p.grad.detach().abs().to(\"cpu\")\n",
        "        )\n",
        "    if step >= MAX_STEPS:\n",
        "        break\n",
        "\n",
        "# ============================================================\n",
        "# 4) Find Top-K Sensitive Coordinates\n",
        "# ============================================================\n",
        "candidates = []\n",
        "for name, rm in running_max.items():\n",
        "    k_local = min(TOP_K, rm.numel())\n",
        "    if k_local > 0:\n",
        "        vals, idxs = torch.topk(rm.view(-1), k_local)\n",
        "        for v, flat in zip(vals, idxs):\n",
        "            coord = torch.unravel_index(flat, rm.shape)\n",
        "            candidates.append((v.item(), name, coord))\n",
        "\n",
        "candidates.sort(key=lambda t: t[0], reverse=True)\n",
        "topk_entries = candidates[:TOP_K]\n",
        "\n",
        "print(f\"\\nTop-{TOP_K} most sensitive tensor elements:\")\n",
        "for rank, (val, name, coord) in enumerate(topk_entries, 1):\n",
        "    print(f\"  #{rank}: {name}{tuple(map(int,coord))}  |grad|={val:.3e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 0) Setup & Config\n",
        "# ============================================================\n",
        "import random\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# ---- Experiment knobs ----\n",
        "SEED       = 123\n",
        "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "MODEL_ID   = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"   # or \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "SPLIT      = \"train\"\n",
        "SEQ_LEN    = 1024\n",
        "BATCH_SIZE = 16\n",
        "MAX_STEPS  = 100\n",
        "TOP_K      = 250\n",
        "\n",
        "# ---- Filter list for second top-K (local ranks) ----\n",
        "FILTER_ENABLED = True\n",
        "NUM_LAYERS = 28   # Qwen1.5B / Qwen2.5-1.5B architecture\n",
        "FILTER_PARAM_NAMES = [\n",
        "    # Embeddings\n",
        "    \"model.embed_tokens.weight\",\n",
        "\n",
        "    # Layer norms: input LN\n",
        "    *[f\"model.layers.{i}.input_layernorm.weight\" for i in range(NUM_LAYERS)],\n",
        "    *[f\"model.layers.{i}.input_layernorm.bias\"   for i in range(NUM_LAYERS)],\n",
        "\n",
        "    # Layer norms: post-attention LN\n",
        "    *[f\"model.layers.{i}.post_attention_layernorm.weight\" for i in range(NUM_LAYERS)],\n",
        "    *[f\"model.layers.{i}.post_attention_layernorm.bias\"   for i in range(NUM_LAYERS)],\n",
        "\n",
        "    # Final RMSNorm\n",
        "    \"model.norm.weight\"\n",
        "]\n",
        "\n",
        "\n",
        "# Reproducibility\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# ============================================================\n",
        "# 1) Model & Tokenizer\n",
        "# ============================================================\n",
        "print(\"Loading tokenizer...\")\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token or tok.convert_ids_to_tokens(0)\n",
        "\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    dtype=torch.float16,\n",
        "    device_map={\"\": DEVICE},\n",
        ")\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model.train()  # gradients required for scan\n",
        "\n",
        "print(f\"Model loaded on {DEVICE} with dtype {next(model.parameters()).dtype}\")\n",
        "\n",
        "# ============================================================\n",
        "# 2) Dataset & Chunking\n",
        "# ============================================================\n",
        "print(\"Loading dataset...\")\n",
        "wiki = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=SPLIT)\n",
        "\n",
        "def chunk_generator():\n",
        "    \"\"\"Yield (inp, labels) windows of size SEQ_LEN.\"\"\"\n",
        "    cache = []\n",
        "    for doc in wiki:\n",
        "        ids = tok(doc[\"text\"]).input_ids\n",
        "        cache.extend(ids)\n",
        "        while len(cache) >= SEQ_LEN + 1:\n",
        "            window = cache[:SEQ_LEN+1]\n",
        "            cache = cache[SEQ_LEN+1:]\n",
        "            yield window[:-1], window[1:]\n",
        "\n",
        "def get_batch(gen, bs=BATCH_SIZE):\n",
        "    \"\"\"Stack windows into batches.\"\"\"\n",
        "    buf = []\n",
        "    for inp, _ in gen:\n",
        "        buf.append(inp)\n",
        "        if len(buf) == bs:\n",
        "            yield torch.tensor(buf, dtype=torch.long, device=DEVICE)\n",
        "            buf = []\n",
        "\n",
        "# ============================================================\n",
        "# 3) Parameter Dictionary\n",
        "# ============================================================\n",
        "param_dict = {name: p for name, p in model.named_parameters() if p.requires_grad}\n",
        "\n",
        "print(f\"Total tracked tensors: {len(param_dict)}\")\n",
        "print(f\"Total elements: {sum(p.numel() for p in param_dict.values()):,}\")\n",
        "\n",
        "running_max = {\n",
        "    name: torch.zeros_like(p, device=\"cpu\", dtype=torch.float32)\n",
        "    for name, p in param_dict.items()\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# 4) Gradient Scan\n",
        "# ============================================================\n",
        "print(\"Starting gradient scan...\")\n",
        "stream = get_batch(chunk_generator(), bs=BATCH_SIZE)\n",
        "\n",
        "for step, inp in enumerate(stream, 1):\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    out = model(inp, labels=inp)\n",
        "    loss = out.loss\n",
        "    loss.backward()\n",
        "\n",
        "    for name, p in param_dict.items():\n",
        "        if p.grad is not None:\n",
        "            grad_abs = p.grad.detach().abs().to(\"cpu\", torch.float32)\n",
        "            running_max[name] = torch.maximum(running_max[name], grad_abs)\n",
        "\n",
        "    print(f\"[step {step}] loss={loss.item():.4f}\")\n",
        "\n",
        "    if step >= MAX_STEPS:\n",
        "        print(\"Reached MAX_STEPS — stopping scan.\")\n",
        "        break\n",
        "\n",
        "print(\"Gradient scan complete.\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# 5) Build Global TOP-K\n",
        "# ============================================================\n",
        "print(f\"Selecting global Top-{TOP_K} by |grad|...\")\n",
        "\n",
        "candidates = []\n",
        "for name, rm in running_max.items():\n",
        "    rm_flat = rm.view(-1)\n",
        "    k_local = min(TOP_K, rm_flat.numel())\n",
        "    vals, idxs = torch.topk(rm_flat, k_local)\n",
        "    for v, flat_idx in zip(vals, idxs):\n",
        "        coord = torch.unravel_index(flat_idx, rm.shape)\n",
        "        candidates.append((v.item(), name, coord))\n",
        "\n",
        "# Sort globally\n",
        "candidates.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "ranked_global = [\n",
        "    {\n",
        "        \"global_rank\": i + 1,\n",
        "        \"value\": val,\n",
        "        \"name\": name,\n",
        "        \"coord\": coord,\n",
        "    }\n",
        "    for i, (val, name, coord) in enumerate(candidates)\n",
        "]\n",
        "\n",
        "global_topk = ranked_global[:TOP_K]\n",
        "\n",
        "# ============================================================\n",
        "# 6) Filtered TOP-K (local + global ranks)\n",
        "# ============================================================\n",
        "def allowed(name):\n",
        "    return not FILTER_ENABLED or name not in FILTER_PARAM_NAMES\n",
        "\n",
        "filtered_list = [item for item in ranked_global if allowed(item[\"name\"])]\n",
        "filtered_topk = filtered_list[:TOP_K]\n",
        "\n",
        "for i, item in enumerate(filtered_topk, 1):\n",
        "    item[\"local_rank\"] = i\n",
        "\n",
        "# ============================================================\n",
        "# 7) Print Results\n",
        "# ============================================================\n",
        "print(f\"\\n=== Global Top-{TOP_K} most sensitive parameters ===\")\n",
        "for item in global_topk:\n",
        "    coord = \"(\" + \", \".join(str(int(x)) for x in item[\"coord\"]) + \")\"\n",
        "    print(f\"  global #{item['global_rank']:3d}: {item['name']}{coord}  |grad|={item['value']:.3e}\")\n",
        "\n",
        "print(f\"\\n=== Filtered Top-{TOP_K} (local + global ranks) ===\")\n",
        "print(\"Filter:\", FILTER_PARAM_NAMES if FILTER_ENABLED else \"None\")\n",
        "for item in filtered_topk:\n",
        "    coord = \"(\" + \", \".join(str(int(x)) for x in item[\"coord\"]) + \")\"\n",
        "    print(\n",
        "        f\"  local #{item['local_rank']:3d} | global #{item['global_rank']:3d}:  \"\n",
        "        f\"{item['name']}{coord}  |grad|={item['value']:.3e}\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "SgRGEo_e2N94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41dc51dc-76a4-4c9b-f60d-270c6c340a1b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer...\n",
            "Loading model...\n",
            "Model loaded on cuda with dtype torch.float16\n",
            "Loading dataset...\n",
            "Total tracked tensors: 339\n",
            "Total elements: 1,777,088,000\n",
            "Starting gradient scan...\n",
            "[step 1] loss=3.7354\n",
            "[step 2] loss=3.6691\n",
            "[step 3] loss=3.7742\n",
            "[step 4] loss=3.7177\n",
            "[step 5] loss=3.6300\n",
            "[step 6] loss=3.6891\n",
            "[step 7] loss=3.8463\n",
            "[step 8] loss=3.3649\n",
            "[step 9] loss=3.7301\n",
            "[step 10] loss=3.6542\n",
            "[step 11] loss=3.7277\n",
            "[step 12] loss=3.7875\n",
            "[step 13] loss=3.5501\n",
            "[step 14] loss=3.7428\n",
            "[step 15] loss=3.7047\n",
            "[step 16] loss=3.2469\n",
            "[step 17] loss=3.9071\n",
            "[step 18] loss=3.4903\n",
            "[step 19] loss=3.3947\n",
            "[step 20] loss=3.5306\n",
            "[step 21] loss=3.7135\n",
            "[step 22] loss=3.6956\n",
            "[step 23] loss=4.0239\n",
            "[step 24] loss=3.3613\n",
            "[step 25] loss=3.6670\n",
            "[step 26] loss=3.4049\n",
            "[step 27] loss=3.8173\n",
            "[step 28] loss=4.0070\n",
            "[step 29] loss=3.6343\n",
            "[step 30] loss=3.9703\n",
            "[step 31] loss=3.7230\n",
            "[step 32] loss=3.7623\n",
            "[step 33] loss=3.8226\n",
            "[step 34] loss=3.6987\n",
            "[step 35] loss=3.5843\n",
            "[step 36] loss=3.7525\n",
            "[step 37] loss=3.8072\n",
            "[step 38] loss=3.6393\n",
            "[step 39] loss=3.4580\n",
            "[step 40] loss=3.8740\n",
            "[step 41] loss=3.3131\n",
            "[step 42] loss=3.6584\n",
            "[step 43] loss=3.5362\n",
            "[step 44] loss=3.8483\n",
            "[step 45] loss=3.5485\n",
            "[step 46] loss=3.6340\n",
            "[step 47] loss=3.5731\n",
            "[step 48] loss=3.7861\n",
            "[step 49] loss=3.5580\n",
            "[step 50] loss=3.6637\n",
            "[step 51] loss=3.5927\n",
            "[step 52] loss=3.6839\n",
            "[step 53] loss=3.6016\n",
            "[step 54] loss=3.9051\n",
            "[step 55] loss=3.7166\n",
            "[step 56] loss=3.8290\n",
            "[step 57] loss=3.6405\n",
            "[step 58] loss=3.6046\n",
            "[step 59] loss=3.4931\n",
            "[step 60] loss=3.6625\n",
            "[step 61] loss=3.5776\n",
            "[step 62] loss=3.8683\n",
            "[step 63] loss=3.7619\n",
            "[step 64] loss=3.5280\n",
            "[step 65] loss=3.7217\n",
            "[step 66] loss=3.7220\n",
            "[step 67] loss=3.7623\n",
            "[step 68] loss=3.7704\n",
            "[step 69] loss=3.6876\n",
            "[step 70] loss=3.4921\n",
            "[step 71] loss=3.7928\n",
            "[step 72] loss=3.2349\n",
            "[step 73] loss=3.6262\n",
            "[step 74] loss=3.6010\n",
            "[step 75] loss=4.0333\n",
            "[step 76] loss=3.7521\n",
            "[step 77] loss=3.5062\n",
            "[step 78] loss=3.7284\n",
            "[step 79] loss=3.5859\n",
            "[step 80] loss=3.5673\n",
            "[step 81] loss=3.3833\n",
            "[step 82] loss=3.3929\n",
            "[step 83] loss=3.9419\n",
            "[step 84] loss=3.5526\n",
            "[step 85] loss=3.6391\n",
            "[step 86] loss=3.8722\n",
            "[step 87] loss=3.7882\n",
            "[step 88] loss=3.8345\n",
            "[step 89] loss=3.4803\n",
            "[step 90] loss=3.8540\n",
            "[step 91] loss=3.7537\n",
            "[step 92] loss=3.2399\n",
            "[step 93] loss=3.6372\n",
            "[step 94] loss=3.8242\n",
            "[step 95] loss=3.7319\n",
            "[step 96] loss=3.6207\n",
            "[step 97] loss=3.2061\n",
            "[step 98] loss=3.8503\n",
            "[step 99] loss=3.7602\n",
            "[step 100] loss=3.6838\n",
            "Reached MAX_STEPS — stopping scan.\n",
            "Gradient scan complete.\n",
            "\n",
            "Selecting global Top-250 by |grad|...\n",
            "\n",
            "=== Global Top-250 most sensitive parameters ===\n",
            "  global #  1: model.embed_tokens.weight(31113, 1105)  |grad|=1.497e+00\n",
            "  global #  2: model.layers.3.mlp.gate_proj.weight(8521, 713)  |grad|=1.480e+00\n",
            "  global #  3: model.embed_tokens.weight(31113, 793)  |grad|=1.456e+00\n",
            "  global #  4: model.embed_tokens.weight(31113, 507)  |grad|=1.354e+00\n",
            "  global #  5: model.embed_tokens.weight(151646, 507)  |grad|=1.157e+00\n",
            "  global #  6: lm_head.weight(1154, 507)  |grad|=1.152e+00\n",
            "  global #  7: model.embed_tokens.weight(31113, 113)  |grad|=1.021e+00\n",
            "  global #  8: model.embed_tokens.weight(569, 458)  |grad|=1.015e+00\n",
            "  global #  9: model.layers.3.mlp.gate_proj.weight(8521, 872)  |grad|=9.546e-01\n",
            "  global # 10: lm_head.weight(11, 507)  |grad|=9.478e-01\n",
            "  global # 11: model.layers.1.mlp.gate_proj.weight(6988, 147)  |grad|=9.453e-01\n",
            "  global # 12: model.layers.0.self_attn.v_proj.weight(98, 900)  |grad|=9.424e-01\n",
            "  global # 13: lm_head.weight(1154, 1283)  |grad|=9.375e-01\n",
            "  global # 14: model.embed_tokens.weight(569, 793)  |grad|=9.351e-01\n",
            "  global # 15: model.embed_tokens.weight(31113, 1423)  |grad|=9.282e-01\n",
            "  global # 16: model.embed_tokens.weight(1154, 609)  |grad|=9.238e-01\n",
            "  global # 17: model.layers.3.mlp.gate_proj.weight(8521, 520)  |grad|=9.028e-01\n",
            "  global # 18: model.embed_tokens.weight(31113, 1272)  |grad|=8.491e-01\n",
            "  global # 19: lm_head.weight(11, 1283)  |grad|=8.203e-01\n",
            "  global # 20: model.embed_tokens.weight(31, 609)  |grad|=8.198e-01\n",
            "  global # 21: model.layers.0.mlp.down_proj.weight(147, 2385)  |grad|=8.120e-01\n",
            "  global # 22: model.layers.1.mlp.gate_proj.weight(1180, 147)  |grad|=8.110e-01\n",
            "  global # 23: model.embed_tokens.weight(31113, 1475)  |grad|=8.008e-01\n",
            "  global # 24: model.layers.1.mlp.up_proj.weight(1828, 147)  |grad|=7.993e-01\n",
            "  global # 25: model.layers.1.mlp.gate_proj.weight(1740, 147)  |grad|=7.881e-01\n",
            "  global # 26: model.layers.0.self_attn.v_proj.weight(176, 900)  |grad|=7.671e-01\n",
            "  global # 27: model.embed_tokens.weight(31113, 713)  |grad|=7.627e-01\n",
            "  global # 28: model.embed_tokens.weight(569, 1105)  |grad|=7.598e-01\n",
            "  global # 29: model.embed_tokens.weight(151646, 900)  |grad|=7.568e-01\n",
            "  global # 30: model.embed_tokens.weight(569, 609)  |grad|=7.441e-01\n",
            "  global # 31: model.embed_tokens.weight(569, 1423)  |grad|=7.402e-01\n",
            "  global # 32: model.layers.0.mlp.down_proj.weight(147, 3295)  |grad|=7.114e-01\n",
            "  global # 33: model.embed_tokens.weight(1154, 1421)  |grad|=6.963e-01\n",
            "  global # 34: model.embed_tokens.weight(31113, 1050)  |grad|=6.855e-01\n",
            "  global # 35: model.layers.1.mlp.up_proj.weight(6667, 147)  |grad|=6.748e-01\n",
            "  global # 36: model.embed_tokens.weight(31113, 354)  |grad|=6.680e-01\n",
            "  global # 37: model.embed_tokens.weight(569, 750)  |grad|=6.665e-01\n",
            "  global # 38: model.embed_tokens.weight(569, 507)  |grad|=6.641e-01\n",
            "  global # 39: model.embed_tokens.weight(31113, 969)  |grad|=6.636e-01\n",
            "  global # 40: model.embed_tokens.weight(31113, 1361)  |grad|=6.567e-01\n",
            "  global # 41: model.embed_tokens.weight(151646, 609)  |grad|=6.548e-01\n",
            "  global # 42: model.embed_tokens.weight(31113, 1209)  |grad|=6.309e-01\n",
            "  global # 43: model.embed_tokens.weight(31113, 110)  |grad|=6.274e-01\n",
            "  global # 44: model.embed_tokens.weight(151646, 653)  |grad|=6.206e-01\n",
            "  global # 45: model.embed_tokens.weight(151646, 458)  |grad|=6.201e-01\n",
            "  global # 46: model.embed_tokens.weight(31113, 858)  |grad|=6.201e-01\n",
            "  global # 47: model.layers.1.mlp.gate_proj.weight(5792, 147)  |grad|=6.177e-01\n",
            "  global # 48: model.embed_tokens.weight(569, 1441)  |grad|=6.084e-01\n",
            "  global # 49: lm_head.weight(1154, 592)  |grad|=6.069e-01\n",
            "  global # 50: model.layers.0.self_attn.v_proj.weight(29, 900)  |grad|=6.040e-01\n",
            "  global # 51: lm_head.weight(1154, 793)  |grad|=5.967e-01\n",
            "  global # 52: model.embed_tokens.weight(31, 1421)  |grad|=5.913e-01\n",
            "  global # 53: model.layers.0.self_attn.v_proj.weight(233, 900)  |grad|=5.869e-01\n",
            "  global # 54: model.embed_tokens.weight(569, 113)  |grad|=5.815e-01\n",
            "  global # 55: model.layers.0.self_attn.v_proj.weight(207, 900)  |grad|=5.815e-01\n",
            "  global # 56: model.embed_tokens.weight(569, 1248)  |grad|=5.771e-01\n",
            "  global # 57: model.embed_tokens.weight(31113, 1243)  |grad|=5.762e-01\n",
            "  global # 58: model.layers.3.mlp.gate_proj.weight(8521, 553)  |grad|=5.737e-01\n",
            "  global # 59: model.layers.0.self_attn.v_proj.weight(221, 900)  |grad|=5.688e-01\n",
            "  global # 60: model.embed_tokens.weight(569, 1295)  |grad|=5.601e-01\n",
            "  global # 61: model.embed_tokens.weight(715, 609)  |grad|=5.586e-01\n",
            "  global # 62: model.embed_tokens.weight(151646, 592)  |grad|=5.552e-01\n",
            "  global # 63: model.embed_tokens.weight(569, 900)  |grad|=5.498e-01\n",
            "  global # 64: model.embed_tokens.weight(151646, 1248)  |grad|=5.488e-01\n",
            "  global # 65: model.layers.0.self_attn.v_proj.weight(154, 900)  |grad|=5.483e-01\n",
            "  global # 66: model.layers.0.self_attn.v_proj.weight(131, 900)  |grad|=5.483e-01\n",
            "  global # 67: model.embed_tokens.weight(151646, 1155)  |grad|=5.464e-01\n",
            "  global # 68: model.embed_tokens.weight(569, 510)  |grad|=5.459e-01\n",
            "  global # 69: model.layers.1.mlp.gate_proj.weight(3894, 147)  |grad|=5.444e-01\n",
            "  global # 70: model.embed_tokens.weight(151646, 1105)  |grad|=5.400e-01\n",
            "  global # 71: model.layers.0.self_attn.v_proj.weight(103, 900)  |grad|=5.337e-01\n",
            "  global # 72: model.embed_tokens.weight(569, 1343)  |grad|=5.308e-01\n",
            "  global # 73: model.embed_tokens.weight(569, 616)  |grad|=5.293e-01\n",
            "  global # 74: model.embed_tokens.weight(31113, 1112)  |grad|=5.278e-01\n",
            "  global # 75: model.layers.0.self_attn.v_proj.weight(246, 900)  |grad|=5.278e-01\n",
            "  global # 76: model.layers.1.self_attn.v_proj.weight(90, 147)  |grad|=5.273e-01\n",
            "  global # 77: model.embed_tokens.weight(569, 1272)  |grad|=5.234e-01\n",
            "  global # 78: model.layers.1.mlp.up_proj.weight(1520, 147)  |grad|=5.234e-01\n",
            "  global # 79: model.embed_tokens.weight(569, 1155)  |grad|=5.229e-01\n",
            "  global # 80: model.embed_tokens.weight(358, 1441)  |grad|=5.229e-01\n",
            "  global # 81: model.embed_tokens.weight(569, 1421)  |grad|=5.225e-01\n",
            "  global # 82: model.embed_tokens.weight(31113, 868)  |grad|=5.161e-01\n",
            "  global # 83: model.embed_tokens.weight(569, 1283)  |grad|=5.132e-01\n",
            "  global # 84: model.layers.3.mlp.gate_proj.weight(8521, 147)  |grad|=5.127e-01\n",
            "  global # 85: model.layers.0.self_attn.v_proj.weight(170, 900)  |grad|=5.117e-01\n",
            "  global # 86: model.layers.1.mlp.gate_proj.weight(5792, 553)  |grad|=5.107e-01\n",
            "  global # 87: model.layers.0.self_attn.v_proj.weight(98, 865)  |grad|=5.088e-01\n",
            "  global # 88: model.layers.0.self_attn.v_proj.weight(198, 900)  |grad|=5.078e-01\n",
            "  global # 89: model.layers.0.self_attn.v_proj.weight(228, 900)  |grad|=5.078e-01\n",
            "  global # 90: model.embed_tokens.weight(31113, 518)  |grad|=5.049e-01\n",
            "  global # 91: lm_head.weight(13, 507)  |grad|=5.049e-01\n",
            "  global # 92: model.embed_tokens.weight(151646, 397)  |grad|=5.044e-01\n",
            "  global # 93: model.embed_tokens.weight(569, 518)  |grad|=5.034e-01\n",
            "  global # 94: model.embed_tokens.weight(31113, 1248)  |grad|=5.034e-01\n",
            "  global # 95: model.embed_tokens.weight(31113, 1301)  |grad|=4.978e-01\n",
            "  global # 96: lm_head.weight(21099, 465)  |grad|=4.954e-01\n",
            "  global # 97: model.embed_tokens.weight(31113, 514)  |grad|=4.939e-01\n",
            "  global # 98: model.embed_tokens.weight(1154, 865)  |grad|=4.934e-01\n",
            "  global # 99: model.layers.1.mlp.gate_proj.weight(7641, 147)  |grad|=4.897e-01\n",
            "  global #100: model.layers.0.self_attn.v_proj.weight(236, 900)  |grad|=4.895e-01\n",
            "  global #101: model.embed_tokens.weight(31, 865)  |grad|=4.888e-01\n",
            "  global #102: lm_head.weight(11, 793)  |grad|=4.885e-01\n",
            "  global #103: model.layers.0.self_attn.v_proj.weight(171, 900)  |grad|=4.873e-01\n",
            "  global #104: model.embed_tokens.weight(151646, 752)  |grad|=4.854e-01\n",
            "  global #105: model.embed_tokens.weight(358, 507)  |grad|=4.841e-01\n",
            "  global #106: model.layers.0.self_attn.v_proj.weight(117, 900)  |grad|=4.834e-01\n",
            "  global #107: model.layers.0.self_attn.v_proj.weight(149, 900)  |grad|=4.832e-01\n",
            "  global #108: model.embed_tokens.weight(151646, 1272)  |grad|=4.829e-01\n",
            "  global #109: model.layers.0.self_attn.v_proj.weight(161, 900)  |grad|=4.819e-01\n",
            "  global #110: lm_head.weight(659, 507)  |grad|=4.817e-01\n",
            "  global #111: model.embed_tokens.weight(31113, 503)  |grad|=4.797e-01\n",
            "  global #112: model.embed_tokens.weight(659, 612)  |grad|=4.780e-01\n",
            "  global #113: model.layers.0.self_attn.v_proj.weight(172, 900)  |grad|=4.768e-01\n",
            "  global #114: model.layers.0.self_attn.v_proj.weight(143, 900)  |grad|=4.753e-01\n",
            "  global #115: model.embed_tokens.weight(151646, 589)  |grad|=4.744e-01\n",
            "  global #116: model.embed_tokens.weight(151646, 200)  |grad|=4.741e-01\n",
            "  global #117: model.layers.0.self_attn.v_proj.weight(84, 900)  |grad|=4.717e-01\n",
            "  global #118: model.embed_tokens.weight(31113, 776)  |grad|=4.709e-01\n",
            "  global #119: model.embed_tokens.weight(569, 1209)  |grad|=4.705e-01\n",
            "  global #120: model.embed_tokens.weight(31113, 1471)  |grad|=4.697e-01\n",
            "  global #121: model.embed_tokens.weight(31113, 465)  |grad|=4.673e-01\n",
            "  global #122: model.layers.0.self_attn.v_proj.weight(101, 900)  |grad|=4.668e-01\n",
            "  global #123: model.embed_tokens.weight(31113, 1449)  |grad|=4.658e-01\n",
            "  global #124: lm_head.weight(11, 592)  |grad|=4.648e-01\n",
            "  global #125: model.embed_tokens.weight(358, 458)  |grad|=4.644e-01\n",
            "  global #126: model.layers.0.mlp.down_proj.weight(589, 2385)  |grad|=4.626e-01\n",
            "  global #127: model.layers.0.self_attn.v_proj.weight(225, 900)  |grad|=4.622e-01\n",
            "  global #128: model.embed_tokens.weight(151646, 1285)  |grad|=4.619e-01\n",
            "  global #129: model.embed_tokens.weight(31113, 573)  |grad|=4.614e-01\n",
            "  global #130: model.embed_tokens.weight(569, 1480)  |grad|=4.604e-01\n",
            "  global #131: model.embed_tokens.weight(31113, 437)  |grad|=4.597e-01\n",
            "  global #132: model.embed_tokens.weight(151646, 181)  |grad|=4.565e-01\n",
            "  global #133: model.embed_tokens.weight(151646, 510)  |grad|=4.536e-01\n",
            "  global #134: model.embed_tokens.weight(151646, 1441)  |grad|=4.531e-01\n",
            "  global #135: model.layers.0.self_attn.v_proj.weight(188, 900)  |grad|=4.531e-01\n",
            "  global #136: model.layers.0.self_attn.v_proj.weight(70, 900)  |grad|=4.529e-01\n",
            "  global #137: model.layers.0.self_attn.v_proj.weight(204, 900)  |grad|=4.519e-01\n",
            "  global #138: model.embed_tokens.weight(151646, 969)  |grad|=4.514e-01\n",
            "  global #139: model.layers.27.mlp.up_proj.weight(761, 1248)  |grad|=4.500e-01\n",
            "  global #140: lm_head.weight(43358, 465)  |grad|=4.492e-01\n",
            "  global #141: model.embed_tokens.weight(151646, 477)  |grad|=4.478e-01\n",
            "  global #142: model.embed_tokens.weight(358, 1105)  |grad|=4.456e-01\n",
            "  global #143: model.embed_tokens.weight(31113, 1012)  |grad|=4.436e-01\n",
            "  global #144: model.embed_tokens.weight(31113, 463)  |grad|=4.407e-01\n",
            "  global #145: model.embed_tokens.weight(151646, 573)  |grad|=4.363e-01\n",
            "  global #146: model.embed_tokens.weight(304, 609)  |grad|=4.343e-01\n",
            "  global #147: model.layers.2.mlp.gate_proj.weight(4742, 713)  |grad|=4.343e-01\n",
            "  global #148: model.embed_tokens.weight(1154, 1285)  |grad|=4.341e-01\n",
            "  global #149: model.layers.3.mlp.gate_proj.weight(8521, 1248)  |grad|=4.338e-01\n",
            "  global #150: model.embed_tokens.weight(151646, 779)  |grad|=4.331e-01\n",
            "  global #151: model.embed_tokens.weight(31113, 844)  |grad|=4.312e-01\n",
            "  global #152: model.layers.2.mlp.gate_proj.weight(4742, 520)  |grad|=4.312e-01\n",
            "  global #153: model.embed_tokens.weight(1154, 507)  |grad|=4.309e-01\n",
            "  global #154: model.embed_tokens.weight(31113, 1385)  |grad|=4.280e-01\n",
            "  global #155: model.layers.0.self_attn.v_proj.weight(155, 900)  |grad|=4.260e-01\n",
            "  global #156: model.embed_tokens.weight(151646, 1361)  |grad|=4.258e-01\n",
            "  global #157: model.embed_tokens.weight(569, 1285)  |grad|=4.250e-01\n",
            "  global #158: model.embed_tokens.weight(31113, 138)  |grad|=4.248e-01\n",
            "  global #159: model.embed_tokens.weight(569, 465)  |grad|=4.221e-01\n",
            "  global #160: model.layers.0.self_attn.v_proj.weight(244, 900)  |grad|=4.221e-01\n",
            "  global #161: model.embed_tokens.weight(569, 503)  |grad|=4.216e-01\n",
            "  global #162: model.layers.0.self_attn.v_proj.weight(4, 900)  |grad|=4.216e-01\n",
            "  global #163: model.embed_tokens.weight(151646, 465)  |grad|=4.214e-01\n",
            "  global #164: model.embed_tokens.weight(151646, 199)  |grad|=4.209e-01\n",
            "  global #165: model.embed_tokens.weight(31113, 947)  |grad|=4.197e-01\n",
            "  global #166: model.embed_tokens.weight(31113, 108)  |grad|=4.192e-01\n",
            "  global #167: model.embed_tokens.weight(659, 1157)  |grad|=4.192e-01\n",
            "  global #168: model.embed_tokens.weight(31113, 699)  |grad|=4.192e-01\n",
            "  global #169: lm_head.weight(1154, 1272)  |grad|=4.185e-01\n",
            "  global #170: model.embed_tokens.weight(31113, 938)  |grad|=4.167e-01\n",
            "  global #171: model.embed_tokens.weight(569, 1134)  |grad|=4.160e-01\n",
            "  global #172: model.embed_tokens.weight(1154, 113)  |grad|=4.148e-01\n",
            "  global #173: model.embed_tokens.weight(1154, 900)  |grad|=4.138e-01\n",
            "  global #174: model.embed_tokens.weight(31113, 1196)  |grad|=4.124e-01\n",
            "  global #175: model.embed_tokens.weight(569, 418)  |grad|=4.114e-01\n",
            "  global #176: model.layers.0.self_attn.v_proj.weight(98, 1381)  |grad|=4.104e-01\n",
            "  global #177: model.embed_tokens.weight(31113, 763)  |grad|=4.092e-01\n",
            "  global #178: model.embed_tokens.weight(151646, 929)  |grad|=4.092e-01\n",
            "  global #179: model.embed_tokens.weight(151646, 1421)  |grad|=4.077e-01\n",
            "  global #180: model.embed_tokens.weight(151646, 113)  |grad|=4.067e-01\n",
            "  global #181: model.embed_tokens.weight(151646, 793)  |grad|=4.065e-01\n",
            "  global #182: model.layers.0.self_attn.v_proj.weight(111, 900)  |grad|=4.055e-01\n",
            "  global #183: model.layers.27.mlp.up_proj.weight(761, 1361)  |grad|=4.055e-01\n",
            "  global #184: model.embed_tokens.weight(569, 865)  |grad|=4.038e-01\n",
            "  global #185: model.embed_tokens.weight(31113, 662)  |grad|=4.036e-01\n",
            "  global #186: model.layers.1.mlp.gate_proj.weight(8310, 147)  |grad|=4.031e-01\n",
            "  global #187: model.embed_tokens.weight(31113, 46)  |grad|=4.028e-01\n",
            "  global #188: model.embed_tokens.weight(151646, 514)  |grad|=4.026e-01\n",
            "  global #189: model.layers.1.mlp.gate_proj.weight(2696, 147)  |grad|=4.026e-01\n",
            "  global #190: model.layers.0.self_attn.v_proj.weight(248, 900)  |grad|=4.011e-01\n",
            "  global #191: model.embed_tokens.weight(151646, 1073)  |grad|=4.001e-01\n",
            "  global #192: model.layers.0.self_attn.v_proj.weight(218, 900)  |grad|=3.994e-01\n",
            "  global #193: model.embed_tokens.weight(151646, 435)  |grad|=3.992e-01\n",
            "  global #194: model.layers.0.self_attn.v_proj.weight(81, 900)  |grad|=3.982e-01\n",
            "  global #195: model.embed_tokens.weight(151646, 1129)  |grad|=3.977e-01\n",
            "  global #196: model.layers.0.self_attn.v_proj.weight(119, 900)  |grad|=3.977e-01\n",
            "  global #197: model.embed_tokens.weight(151646, 59)  |grad|=3.970e-01\n",
            "  global #198: model.embed_tokens.weight(31113, 155)  |grad|=3.967e-01\n",
            "  global #199: model.layers.0.self_attn.v_proj.weight(36, 900)  |grad|=3.967e-01\n",
            "  global #200: model.layers.0.self_attn.v_proj.weight(90, 900)  |grad|=3.965e-01\n",
            "  global #201: lm_head.weight(715, 465)  |grad|=3.962e-01\n",
            "  global #202: model.embed_tokens.weight(151646, 28)  |grad|=3.960e-01\n",
            "  global #203: model.embed_tokens.weight(31113, 147)  |grad|=3.955e-01\n",
            "  global #204: model.embed_tokens.weight(715, 1421)  |grad|=3.955e-01\n",
            "  global #205: model.embed_tokens.weight(58462, 507)  |grad|=3.950e-01\n",
            "  global #206: model.layers.0.self_attn.v_proj.weight(116, 900)  |grad|=3.950e-01\n",
            "  global #207: lm_head.weight(1154, 1248)  |grad|=3.945e-01\n",
            "  global #208: model.embed_tokens.weight(569, 278)  |grad|=3.940e-01\n",
            "  global #209: model.embed_tokens.weight(31113, 530)  |grad|=3.936e-01\n",
            "  global #210: model.embed_tokens.weight(31113, 1391)  |grad|=3.931e-01\n",
            "  global #211: model.embed_tokens.weight(31113, 1051)  |grad|=3.931e-01\n",
            "  global #212: model.layers.0.self_attn.v_proj.weight(113, 900)  |grad|=3.926e-01\n",
            "  global #213: model.layers.0.self_attn.v_proj.weight(102, 900)  |grad|=3.921e-01\n",
            "  global #214: model.embed_tokens.weight(569, 1468)  |grad|=3.916e-01\n",
            "  global #215: model.embed_tokens.weight(31113, 1417)  |grad|=3.911e-01\n",
            "  global #216: model.embed_tokens.weight(31113, 1169)  |grad|=3.906e-01\n",
            "  global #217: model.embed_tokens.weight(31113, 160)  |grad|=3.904e-01\n",
            "  global #218: model.embed_tokens.weight(151646, 110)  |grad|=3.901e-01\n",
            "  global #219: model.embed_tokens.weight(31113, 1522)  |grad|=3.899e-01\n",
            "  global #220: model.layers.1.mlp.up_proj.weight(827, 147)  |grad|=3.889e-01\n",
            "  global #221: model.layers.0.self_attn.v_proj.weight(1, 900)  |grad|=3.884e-01\n",
            "  global #222: model.embed_tokens.weight(569, 75)  |grad|=3.879e-01\n",
            "  global #223: model.layers.0.self_attn.v_proj.weight(213, 900)  |grad|=3.879e-01\n",
            "  global #224: model.layers.0.self_attn.v_proj.weight(252, 900)  |grad|=3.853e-01\n",
            "  global #225: model.embed_tokens.weight(151646, 508)  |grad|=3.850e-01\n",
            "  global #226: model.layers.0.self_attn.v_proj.weight(44, 900)  |grad|=3.833e-01\n",
            "  global #227: model.embed_tokens.weight(31113, 224)  |grad|=3.826e-01\n",
            "  global #228: model.layers.1.mlp.up_proj.weight(6667, 553)  |grad|=3.818e-01\n",
            "  global #229: lm_head.weight(1154, 868)  |grad|=3.804e-01\n",
            "  global #230: model.layers.0.self_attn.v_proj.weight(215, 900)  |grad|=3.801e-01\n",
            "  global #231: model.embed_tokens.weight(31113, 200)  |grad|=3.794e-01\n",
            "  global #232: model.embed_tokens.weight(569, 940)  |grad|=3.792e-01\n",
            "  global #233: lm_head.weight(279, 1283)  |grad|=3.789e-01\n",
            "  global #234: model.embed_tokens.weight(31113, 11)  |grad|=3.777e-01\n",
            "  global #235: model.embed_tokens.weight(569, 477)  |grad|=3.774e-01\n",
            "  global #236: model.embed_tokens.weight(31113, 733)  |grad|=3.765e-01\n",
            "  global #237: model.embed_tokens.weight(569, 472)  |grad|=3.765e-01\n",
            "  global #238: model.layers.1.mlp.gate_proj.weight(1520, 147)  |grad|=3.762e-01\n",
            "  global #239: model.embed_tokens.weight(151646, 472)  |grad|=3.755e-01\n",
            "  global #240: model.layers.0.self_attn.v_proj.weight(76, 900)  |grad|=3.755e-01\n",
            "  global #241: model.embed_tokens.weight(31113, 653)  |grad|=3.752e-01\n",
            "  global #242: model.embed_tokens.weight(31113, 1297)  |grad|=3.750e-01\n",
            "  global #243: model.embed_tokens.weight(151646, 360)  |grad|=3.750e-01\n",
            "  global #244: model.embed_tokens.weight(569, 1345)  |grad|=3.730e-01\n",
            "  global #245: lm_head.weight(151646, 929)  |grad|=3.730e-01\n",
            "  global #246: model.layers.2.mlp.gate_proj.weight(1104, 713)  |grad|=3.716e-01\n",
            "  global #247: model.layers.0.self_attn.v_proj.weight(69, 900)  |grad|=3.711e-01\n",
            "  global #248: model.embed_tokens.weight(82, 1105)  |grad|=3.704e-01\n",
            "  global #249: model.embed_tokens.weight(569, 102)  |grad|=3.704e-01\n",
            "  global #250: model.layers.0.self_attn.v_proj.weight(146, 900)  |grad|=3.689e-01\n",
            "\n",
            "=== Filtered Top-250 (local + global ranks) ===\n",
            "Filter: ['model.embed_tokens.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.0.input_layernorm.bias', 'model.layers.1.input_layernorm.bias', 'model.layers.2.input_layernorm.bias', 'model.layers.3.input_layernorm.bias', 'model.layers.4.input_layernorm.bias', 'model.layers.5.input_layernorm.bias', 'model.layers.6.input_layernorm.bias', 'model.layers.7.input_layernorm.bias', 'model.layers.8.input_layernorm.bias', 'model.layers.9.input_layernorm.bias', 'model.layers.10.input_layernorm.bias', 'model.layers.11.input_layernorm.bias', 'model.layers.12.input_layernorm.bias', 'model.layers.13.input_layernorm.bias', 'model.layers.14.input_layernorm.bias', 'model.layers.15.input_layernorm.bias', 'model.layers.16.input_layernorm.bias', 'model.layers.17.input_layernorm.bias', 'model.layers.18.input_layernorm.bias', 'model.layers.19.input_layernorm.bias', 'model.layers.20.input_layernorm.bias', 'model.layers.21.input_layernorm.bias', 'model.layers.22.input_layernorm.bias', 'model.layers.23.input_layernorm.bias', 'model.layers.24.input_layernorm.bias', 'model.layers.25.input_layernorm.bias', 'model.layers.26.input_layernorm.bias', 'model.layers.27.input_layernorm.bias', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.0.post_attention_layernorm.bias', 'model.layers.1.post_attention_layernorm.bias', 'model.layers.2.post_attention_layernorm.bias', 'model.layers.3.post_attention_layernorm.bias', 'model.layers.4.post_attention_layernorm.bias', 'model.layers.5.post_attention_layernorm.bias', 'model.layers.6.post_attention_layernorm.bias', 'model.layers.7.post_attention_layernorm.bias', 'model.layers.8.post_attention_layernorm.bias', 'model.layers.9.post_attention_layernorm.bias', 'model.layers.10.post_attention_layernorm.bias', 'model.layers.11.post_attention_layernorm.bias', 'model.layers.12.post_attention_layernorm.bias', 'model.layers.13.post_attention_layernorm.bias', 'model.layers.14.post_attention_layernorm.bias', 'model.layers.15.post_attention_layernorm.bias', 'model.layers.16.post_attention_layernorm.bias', 'model.layers.17.post_attention_layernorm.bias', 'model.layers.18.post_attention_layernorm.bias', 'model.layers.19.post_attention_layernorm.bias', 'model.layers.20.post_attention_layernorm.bias', 'model.layers.21.post_attention_layernorm.bias', 'model.layers.22.post_attention_layernorm.bias', 'model.layers.23.post_attention_layernorm.bias', 'model.layers.24.post_attention_layernorm.bias', 'model.layers.25.post_attention_layernorm.bias', 'model.layers.26.post_attention_layernorm.bias', 'model.layers.27.post_attention_layernorm.bias', 'model.norm.weight']\n",
            "  local #  1 | global #  2:  model.layers.3.mlp.gate_proj.weight(8521, 713)  |grad|=1.480e+00\n",
            "  local #  2 | global #  6:  lm_head.weight(1154, 507)  |grad|=1.152e+00\n",
            "  local #  3 | global #  9:  model.layers.3.mlp.gate_proj.weight(8521, 872)  |grad|=9.546e-01\n",
            "  local #  4 | global # 10:  lm_head.weight(11, 507)  |grad|=9.478e-01\n",
            "  local #  5 | global # 11:  model.layers.1.mlp.gate_proj.weight(6988, 147)  |grad|=9.453e-01\n",
            "  local #  6 | global # 12:  model.layers.0.self_attn.v_proj.weight(98, 900)  |grad|=9.424e-01\n",
            "  local #  7 | global # 13:  lm_head.weight(1154, 1283)  |grad|=9.375e-01\n",
            "  local #  8 | global # 17:  model.layers.3.mlp.gate_proj.weight(8521, 520)  |grad|=9.028e-01\n",
            "  local #  9 | global # 19:  lm_head.weight(11, 1283)  |grad|=8.203e-01\n",
            "  local # 10 | global # 21:  model.layers.0.mlp.down_proj.weight(147, 2385)  |grad|=8.120e-01\n",
            "  local # 11 | global # 22:  model.layers.1.mlp.gate_proj.weight(1180, 147)  |grad|=8.110e-01\n",
            "  local # 12 | global # 24:  model.layers.1.mlp.up_proj.weight(1828, 147)  |grad|=7.993e-01\n",
            "  local # 13 | global # 25:  model.layers.1.mlp.gate_proj.weight(1740, 147)  |grad|=7.881e-01\n",
            "  local # 14 | global # 26:  model.layers.0.self_attn.v_proj.weight(176, 900)  |grad|=7.671e-01\n",
            "  local # 15 | global # 32:  model.layers.0.mlp.down_proj.weight(147, 3295)  |grad|=7.114e-01\n",
            "  local # 16 | global # 35:  model.layers.1.mlp.up_proj.weight(6667, 147)  |grad|=6.748e-01\n",
            "  local # 17 | global # 47:  model.layers.1.mlp.gate_proj.weight(5792, 147)  |grad|=6.177e-01\n",
            "  local # 18 | global # 49:  lm_head.weight(1154, 592)  |grad|=6.069e-01\n",
            "  local # 19 | global # 50:  model.layers.0.self_attn.v_proj.weight(29, 900)  |grad|=6.040e-01\n",
            "  local # 20 | global # 51:  lm_head.weight(1154, 793)  |grad|=5.967e-01\n",
            "  local # 21 | global # 53:  model.layers.0.self_attn.v_proj.weight(233, 900)  |grad|=5.869e-01\n",
            "  local # 22 | global # 55:  model.layers.0.self_attn.v_proj.weight(207, 900)  |grad|=5.815e-01\n",
            "  local # 23 | global # 58:  model.layers.3.mlp.gate_proj.weight(8521, 553)  |grad|=5.737e-01\n",
            "  local # 24 | global # 59:  model.layers.0.self_attn.v_proj.weight(221, 900)  |grad|=5.688e-01\n",
            "  local # 25 | global # 65:  model.layers.0.self_attn.v_proj.weight(154, 900)  |grad|=5.483e-01\n",
            "  local # 26 | global # 66:  model.layers.0.self_attn.v_proj.weight(131, 900)  |grad|=5.483e-01\n",
            "  local # 27 | global # 69:  model.layers.1.mlp.gate_proj.weight(3894, 147)  |grad|=5.444e-01\n",
            "  local # 28 | global # 71:  model.layers.0.self_attn.v_proj.weight(103, 900)  |grad|=5.337e-01\n",
            "  local # 29 | global # 75:  model.layers.0.self_attn.v_proj.weight(246, 900)  |grad|=5.278e-01\n",
            "  local # 30 | global # 76:  model.layers.1.self_attn.v_proj.weight(90, 147)  |grad|=5.273e-01\n",
            "  local # 31 | global # 78:  model.layers.1.mlp.up_proj.weight(1520, 147)  |grad|=5.234e-01\n",
            "  local # 32 | global # 84:  model.layers.3.mlp.gate_proj.weight(8521, 147)  |grad|=5.127e-01\n",
            "  local # 33 | global # 85:  model.layers.0.self_attn.v_proj.weight(170, 900)  |grad|=5.117e-01\n",
            "  local # 34 | global # 86:  model.layers.1.mlp.gate_proj.weight(5792, 553)  |grad|=5.107e-01\n",
            "  local # 35 | global # 87:  model.layers.0.self_attn.v_proj.weight(98, 865)  |grad|=5.088e-01\n",
            "  local # 36 | global # 88:  model.layers.0.self_attn.v_proj.weight(198, 900)  |grad|=5.078e-01\n",
            "  local # 37 | global # 89:  model.layers.0.self_attn.v_proj.weight(228, 900)  |grad|=5.078e-01\n",
            "  local # 38 | global # 91:  lm_head.weight(13, 507)  |grad|=5.049e-01\n",
            "  local # 39 | global # 96:  lm_head.weight(21099, 465)  |grad|=4.954e-01\n",
            "  local # 40 | global # 99:  model.layers.1.mlp.gate_proj.weight(7641, 147)  |grad|=4.897e-01\n",
            "  local # 41 | global #100:  model.layers.0.self_attn.v_proj.weight(236, 900)  |grad|=4.895e-01\n",
            "  local # 42 | global #102:  lm_head.weight(11, 793)  |grad|=4.885e-01\n",
            "  local # 43 | global #103:  model.layers.0.self_attn.v_proj.weight(171, 900)  |grad|=4.873e-01\n",
            "  local # 44 | global #106:  model.layers.0.self_attn.v_proj.weight(117, 900)  |grad|=4.834e-01\n",
            "  local # 45 | global #107:  model.layers.0.self_attn.v_proj.weight(149, 900)  |grad|=4.832e-01\n",
            "  local # 46 | global #109:  model.layers.0.self_attn.v_proj.weight(161, 900)  |grad|=4.819e-01\n",
            "  local # 47 | global #110:  lm_head.weight(659, 507)  |grad|=4.817e-01\n",
            "  local # 48 | global #113:  model.layers.0.self_attn.v_proj.weight(172, 900)  |grad|=4.768e-01\n",
            "  local # 49 | global #114:  model.layers.0.self_attn.v_proj.weight(143, 900)  |grad|=4.753e-01\n",
            "  local # 50 | global #117:  model.layers.0.self_attn.v_proj.weight(84, 900)  |grad|=4.717e-01\n",
            "  local # 51 | global #122:  model.layers.0.self_attn.v_proj.weight(101, 900)  |grad|=4.668e-01\n",
            "  local # 52 | global #124:  lm_head.weight(11, 592)  |grad|=4.648e-01\n",
            "  local # 53 | global #126:  model.layers.0.mlp.down_proj.weight(589, 2385)  |grad|=4.626e-01\n",
            "  local # 54 | global #127:  model.layers.0.self_attn.v_proj.weight(225, 900)  |grad|=4.622e-01\n",
            "  local # 55 | global #135:  model.layers.0.self_attn.v_proj.weight(188, 900)  |grad|=4.531e-01\n",
            "  local # 56 | global #136:  model.layers.0.self_attn.v_proj.weight(70, 900)  |grad|=4.529e-01\n",
            "  local # 57 | global #137:  model.layers.0.self_attn.v_proj.weight(204, 900)  |grad|=4.519e-01\n",
            "  local # 58 | global #139:  model.layers.27.mlp.up_proj.weight(761, 1248)  |grad|=4.500e-01\n",
            "  local # 59 | global #140:  lm_head.weight(43358, 465)  |grad|=4.492e-01\n",
            "  local # 60 | global #147:  model.layers.2.mlp.gate_proj.weight(4742, 713)  |grad|=4.343e-01\n",
            "  local # 61 | global #149:  model.layers.3.mlp.gate_proj.weight(8521, 1248)  |grad|=4.338e-01\n",
            "  local # 62 | global #152:  model.layers.2.mlp.gate_proj.weight(4742, 520)  |grad|=4.312e-01\n",
            "  local # 63 | global #155:  model.layers.0.self_attn.v_proj.weight(155, 900)  |grad|=4.260e-01\n",
            "  local # 64 | global #160:  model.layers.0.self_attn.v_proj.weight(244, 900)  |grad|=4.221e-01\n",
            "  local # 65 | global #162:  model.layers.0.self_attn.v_proj.weight(4, 900)  |grad|=4.216e-01\n",
            "  local # 66 | global #169:  lm_head.weight(1154, 1272)  |grad|=4.185e-01\n",
            "  local # 67 | global #176:  model.layers.0.self_attn.v_proj.weight(98, 1381)  |grad|=4.104e-01\n",
            "  local # 68 | global #182:  model.layers.0.self_attn.v_proj.weight(111, 900)  |grad|=4.055e-01\n",
            "  local # 69 | global #183:  model.layers.27.mlp.up_proj.weight(761, 1361)  |grad|=4.055e-01\n",
            "  local # 70 | global #186:  model.layers.1.mlp.gate_proj.weight(8310, 147)  |grad|=4.031e-01\n",
            "  local # 71 | global #189:  model.layers.1.mlp.gate_proj.weight(2696, 147)  |grad|=4.026e-01\n",
            "  local # 72 | global #190:  model.layers.0.self_attn.v_proj.weight(248, 900)  |grad|=4.011e-01\n",
            "  local # 73 | global #192:  model.layers.0.self_attn.v_proj.weight(218, 900)  |grad|=3.994e-01\n",
            "  local # 74 | global #194:  model.layers.0.self_attn.v_proj.weight(81, 900)  |grad|=3.982e-01\n",
            "  local # 75 | global #196:  model.layers.0.self_attn.v_proj.weight(119, 900)  |grad|=3.977e-01\n",
            "  local # 76 | global #199:  model.layers.0.self_attn.v_proj.weight(36, 900)  |grad|=3.967e-01\n",
            "  local # 77 | global #200:  model.layers.0.self_attn.v_proj.weight(90, 900)  |grad|=3.965e-01\n",
            "  local # 78 | global #201:  lm_head.weight(715, 465)  |grad|=3.962e-01\n",
            "  local # 79 | global #206:  model.layers.0.self_attn.v_proj.weight(116, 900)  |grad|=3.950e-01\n",
            "  local # 80 | global #207:  lm_head.weight(1154, 1248)  |grad|=3.945e-01\n",
            "  local # 81 | global #212:  model.layers.0.self_attn.v_proj.weight(113, 900)  |grad|=3.926e-01\n",
            "  local # 82 | global #213:  model.layers.0.self_attn.v_proj.weight(102, 900)  |grad|=3.921e-01\n",
            "  local # 83 | global #220:  model.layers.1.mlp.up_proj.weight(827, 147)  |grad|=3.889e-01\n",
            "  local # 84 | global #221:  model.layers.0.self_attn.v_proj.weight(1, 900)  |grad|=3.884e-01\n",
            "  local # 85 | global #223:  model.layers.0.self_attn.v_proj.weight(213, 900)  |grad|=3.879e-01\n",
            "  local # 86 | global #224:  model.layers.0.self_attn.v_proj.weight(252, 900)  |grad|=3.853e-01\n",
            "  local # 87 | global #226:  model.layers.0.self_attn.v_proj.weight(44, 900)  |grad|=3.833e-01\n",
            "  local # 88 | global #228:  model.layers.1.mlp.up_proj.weight(6667, 553)  |grad|=3.818e-01\n",
            "  local # 89 | global #229:  lm_head.weight(1154, 868)  |grad|=3.804e-01\n",
            "  local # 90 | global #230:  model.layers.0.self_attn.v_proj.weight(215, 900)  |grad|=3.801e-01\n",
            "  local # 91 | global #233:  lm_head.weight(279, 1283)  |grad|=3.789e-01\n",
            "  local # 92 | global #238:  model.layers.1.mlp.gate_proj.weight(1520, 147)  |grad|=3.762e-01\n",
            "  local # 93 | global #240:  model.layers.0.self_attn.v_proj.weight(76, 900)  |grad|=3.755e-01\n",
            "  local # 94 | global #245:  lm_head.weight(151646, 929)  |grad|=3.730e-01\n",
            "  local # 95 | global #246:  model.layers.2.mlp.gate_proj.weight(1104, 713)  |grad|=3.716e-01\n",
            "  local # 96 | global #247:  model.layers.0.self_attn.v_proj.weight(69, 900)  |grad|=3.711e-01\n",
            "  local # 97 | global #250:  model.layers.0.self_attn.v_proj.weight(146, 900)  |grad|=3.689e-01\n",
            "  local # 98 | global #252:  lm_head.weight(151646, 1105)  |grad|=3.684e-01\n",
            "  local # 99 | global #253:  model.layers.0.self_attn.v_proj.weight(35, 900)  |grad|=3.682e-01\n",
            "  local #100 | global #257:  model.layers.0.self_attn.v_proj.weight(153, 900)  |grad|=3.657e-01\n",
            "  local #101 | global #259:  model.layers.0.self_attn.v_proj.weight(173, 900)  |grad|=3.652e-01\n",
            "  local #102 | global #263:  lm_head.weight(60346, 1283)  |grad|=3.618e-01\n",
            "  local #103 | global #264:  model.layers.0.self_attn.v_proj.weight(185, 900)  |grad|=3.613e-01\n",
            "  local #104 | global #265:  model.layers.0.self_attn.v_proj.weight(171, 865)  |grad|=3.611e-01\n",
            "  local #105 | global #266:  model.layers.0.self_attn.v_proj.weight(98, 465)  |grad|=3.608e-01\n",
            "  local #106 | global #268:  model.layers.0.self_attn.v_proj.weight(98, 458)  |grad|=3.604e-01\n",
            "  local #107 | global #270:  lm_head.weight(11, 1272)  |grad|=3.599e-01\n",
            "  local #108 | global #271:  lm_head.weight(1154, 662)  |grad|=3.591e-01\n",
            "  local #109 | global #273:  model.layers.0.self_attn.v_proj.weight(0, 900)  |grad|=3.582e-01\n",
            "  local #110 | global #278:  model.layers.0.self_attn.q_proj.weight(256, 900)  |grad|=3.567e-01\n",
            "  local #111 | global #279:  model.layers.0.self_attn.v_proj.weight(60, 900)  |grad|=3.567e-01\n",
            "  local #112 | global #285:  lm_head.weight(11, 1248)  |grad|=3.525e-01\n",
            "  local #113 | global #288:  model.layers.0.self_attn.o_proj.weight(147, 226)  |grad|=3.508e-01\n",
            "  local #114 | global #289:  model.layers.3.mlp.gate_proj.weight(8521, 408)  |grad|=3.506e-01\n",
            "  local #115 | global #291:  model.layers.0.self_attn.o_proj.weight(147, 482)  |grad|=3.503e-01\n",
            "  local #116 | global #293:  model.layers.0.self_attn.o_proj.weight(147, 738)  |grad|=3.494e-01\n",
            "  local #117 | global #295:  model.layers.0.self_attn.v_proj.weight(83, 900)  |grad|=3.489e-01\n",
            "  local #118 | global #304:  model.layers.0.self_attn.v_proj.weight(114, 900)  |grad|=3.469e-01\n",
            "  local #119 | global #308:  model.layers.0.self_attn.v_proj.weight(42, 900)  |grad|=3.450e-01\n",
            "  local #120 | global #309:  model.layers.0.self_attn.o_proj.weight(147, 98)  |grad|=3.447e-01\n",
            "  local #121 | global #312:  model.layers.1.mlp.up_proj.weight(1170, 147)  |grad|=3.442e-01\n",
            "  local #122 | global #313:  model.layers.2.mlp.gate_proj.weight(1104, 520)  |grad|=3.440e-01\n",
            "  local #123 | global #314:  model.layers.0.self_attn.v_proj.weight(224, 900)  |grad|=3.433e-01\n",
            "  local #124 | global #315:  lm_head.weight(659, 1283)  |grad|=3.433e-01\n",
            "  local #125 | global #317:  model.layers.0.self_attn.o_proj.weight(147, 610)  |grad|=3.430e-01\n",
            "  local #126 | global #319:  model.layers.1.mlp.up_proj.weight(2458, 147)  |grad|=3.420e-01\n",
            "  local #127 | global #322:  model.layers.0.self_attn.v_proj.weight(29, 865)  |grad|=3.413e-01\n",
            "  local #128 | global #324:  lm_head.weight(60346, 465)  |grad|=3.403e-01\n",
            "  local #129 | global #326:  lm_head.weight(43358, 1283)  |grad|=3.386e-01\n",
            "  local #130 | global #327:  model.layers.1.mlp.gate_proj.weight(1170, 147)  |grad|=3.376e-01\n",
            "  local #131 | global #330:  lm_head.weight(1154, 1475)  |grad|=3.364e-01\n",
            "  local #132 | global #336:  model.layers.0.self_attn.v_proj.weight(98, 1157)  |grad|=3.330e-01\n",
            "  local #133 | global #338:  lm_head.weight(13, 1283)  |grad|=3.328e-01\n",
            "  local #134 | global #343:  model.layers.0.self_attn.v_proj.weight(47, 900)  |grad|=3.318e-01\n",
            "  local #135 | global #344:  model.layers.0.self_attn.o_proj.weight(147, 354)  |grad|=3.303e-01\n",
            "  local #136 | global #345:  model.layers.1.mlp.gate_proj.weight(1180, 553)  |grad|=3.293e-01\n",
            "  local #137 | global #346:  model.layers.1.self_attn.v_proj.weight(23, 147)  |grad|=3.291e-01\n",
            "  local #138 | global #347:  model.layers.0.self_attn.v_proj.weight(179, 900)  |grad|=3.289e-01\n",
            "  local #139 | global #349:  model.layers.0.self_attn.v_proj.weight(103, 865)  |grad|=3.284e-01\n",
            "  local #140 | global #351:  model.layers.0.self_attn.v_proj.weight(228, 865)  |grad|=3.281e-01\n",
            "  local #141 | global #352:  model.layers.0.self_attn.v_proj.weight(161, 865)  |grad|=3.281e-01\n",
            "  local #142 | global #353:  lm_head.weight(576, 465)  |grad|=3.279e-01\n",
            "  local #143 | global #355:  model.layers.0.self_attn.v_proj.weight(170, 865)  |grad|=3.274e-01\n",
            "  local #144 | global #357:  model.layers.0.self_attn.v_proj.weight(176, 865)  |grad|=3.271e-01\n",
            "  local #145 | global #359:  model.layers.0.self_attn.v_proj.weight(223, 900)  |grad|=3.262e-01\n",
            "  local #146 | global #370:  model.layers.1.mlp.up_proj.weight(7925, 147)  |grad|=3.208e-01\n",
            "  local #147 | global #372:  model.layers.1.mlp.up_proj.weight(1828, 553)  |grad|=3.198e-01\n",
            "  local #148 | global #376:  model.layers.0.self_attn.v_proj.weight(26, 900)  |grad|=3.184e-01\n",
            "  local #149 | global #377:  model.layers.0.self_attn.v_proj.weight(206, 900)  |grad|=3.181e-01\n",
            "  local #150 | global #380:  model.layers.0.self_attn.v_proj.weight(52, 900)  |grad|=3.167e-01\n",
            "  local #151 | global #381:  model.layers.0.self_attn.v_proj.weight(98, 1073)  |grad|=3.162e-01\n",
            "  local #152 | global #382:  model.layers.0.self_attn.v_proj.weight(241, 900)  |grad|=3.154e-01\n",
            "  local #153 | global #383:  model.layers.0.self_attn.v_proj.weight(255, 900)  |grad|=3.152e-01\n",
            "  local #154 | global #386:  model.layers.0.self_attn.v_proj.weight(176, 1381)  |grad|=3.149e-01\n",
            "  local #155 | global #393:  lm_head.weight(95682, 465)  |grad|=3.125e-01\n",
            "  local #156 | global #394:  lm_head.weight(11, 868)  |grad|=3.123e-01\n",
            "  local #157 | global #397:  model.layers.0.self_attn.v_proj.weight(92, 900)  |grad|=3.113e-01\n",
            "  local #158 | global #398:  model.layers.0.self_attn.v_proj.weight(50, 900)  |grad|=3.110e-01\n",
            "  local #159 | global #399:  lm_head.weight(21099, 1283)  |grad|=3.108e-01\n",
            "  local #160 | global #400:  model.layers.0.self_attn.v_proj.weight(129, 900)  |grad|=3.105e-01\n",
            "  local #161 | global #409:  lm_head.weight(1154, 1155)  |grad|=3.086e-01\n",
            "  local #162 | global #412:  model.layers.0.self_attn.v_proj.weight(24, 900)  |grad|=3.079e-01\n",
            "  local #163 | global #413:  model.layers.0.mlp.down_proj.weight(589, 3295)  |grad|=3.066e-01\n",
            "  local #164 | global #414:  lm_head.weight(52165, 592)  |grad|=3.054e-01\n",
            "  local #165 | global #415:  model.layers.1.mlp.gate_proj.weight(1504, 147)  |grad|=3.052e-01\n",
            "  local #166 | global #416:  lm_head.weight(364, 507)  |grad|=3.047e-01\n",
            "  local #167 | global #417:  model.layers.0.self_attn.v_proj.weight(236, 865)  |grad|=3.040e-01\n",
            "  local #168 | global #418:  model.layers.0.self_attn.v_proj.weight(98, 573)  |grad|=3.037e-01\n",
            "  local #169 | global #419:  model.layers.0.self_attn.v_proj.weight(221, 865)  |grad|=3.035e-01\n",
            "  local #170 | global #420:  lm_head.weight(364, 1283)  |grad|=3.022e-01\n",
            "  local #171 | global #421:  model.layers.0.self_attn.v_proj.weight(124, 900)  |grad|=3.010e-01\n",
            "  local #172 | global #422:  model.layers.0.self_attn.v_proj.weight(95, 900)  |grad|=3.008e-01\n",
            "  local #173 | global #423:  model.layers.1.mlp.gate_proj.weight(4338, 147)  |grad|=3.005e-01\n",
            "  local #174 | global #424:  lm_head.weight(1154, 1285)  |grad|=2.991e-01\n",
            "  local #175 | global #425:  model.layers.0.self_attn.v_proj.weight(103, 458)  |grad|=2.983e-01\n",
            "  local #176 | global #426:  model.layers.1.mlp.gate_proj.weight(2413, 147)  |grad|=2.961e-01\n",
            "  local #177 | global #427:  model.layers.0.self_attn.v_proj.weight(142, 900)  |grad|=2.954e-01\n",
            "  local #178 | global #428:  model.layers.1.self_attn.v_proj.weight(72, 147)  |grad|=2.954e-01\n",
            "  local #179 | global #429:  model.layers.0.self_attn.v_proj.weight(40, 900)  |grad|=2.947e-01\n",
            "  local #180 | global #430:  model.layers.0.self_attn.v_proj.weight(29, 1381)  |grad|=2.944e-01\n",
            "  local #181 | global #431:  model.layers.0.self_attn.v_proj.weight(98, 793)  |grad|=2.925e-01\n",
            "  local #182 | global #432:  model.layers.0.self_attn.v_proj.weight(156, 900)  |grad|=2.922e-01\n",
            "  local #183 | global #433:  lm_head.weight(1154, 113)  |grad|=2.920e-01\n",
            "  local #184 | global #434:  model.layers.0.mlp.down_proj.weight(1249, 2385)  |grad|=2.913e-01\n",
            "  local #185 | global #435:  model.layers.1.mlp.gate_proj.weight(7238, 147)  |grad|=2.888e-01\n",
            "  local #186 | global #436:  model.layers.0.self_attn.v_proj.weight(226, 900)  |grad|=2.881e-01\n",
            "  local #187 | global #437:  model.layers.0.self_attn.v_proj.weight(29, 458)  |grad|=2.866e-01\n",
            "  local #188 | global #438:  model.layers.0.self_attn.v_proj.weight(103, 1381)  |grad|=2.856e-01\n",
            "  local #189 | global #439:  model.layers.0.self_attn.v_proj.weight(198, 793)  |grad|=2.849e-01\n",
            "  local #190 | global #440:  lm_head.weight(17374, 465)  |grad|=2.834e-01\n",
            "  local #191 | global #441:  model.layers.1.mlp.up_proj.weight(6522, 147)  |grad|=2.832e-01\n",
            "  local #192 | global #442:  model.layers.0.self_attn.v_proj.weight(232, 900)  |grad|=2.830e-01\n",
            "  local #193 | global #443:  model.layers.0.self_attn.v_proj.weight(12, 900)  |grad|=2.830e-01\n",
            "  local #194 | global #444:  model.layers.0.self_attn.v_proj.weight(207, 865)  |grad|=2.822e-01\n",
            "  local #195 | global #445:  model.layers.0.self_attn.v_proj.weight(98, 50)  |grad|=2.820e-01\n",
            "  local #196 | global #446:  model.layers.0.self_attn.v_proj.weight(117, 865)  |grad|=2.815e-01\n",
            "  local #197 | global #447:  model.layers.1.self_attn.v_proj.weight(105, 147)  |grad|=2.798e-01\n",
            "  local #198 | global #448:  lm_head.weight(11, 662)  |grad|=2.798e-01\n",
            "  local #199 | global #449:  model.layers.0.self_attn.v_proj.weight(98, 609)  |grad|=2.793e-01\n",
            "  local #200 | global #450:  lm_head.weight(77018, 465)  |grad|=2.791e-01\n",
            "  local #201 | global #451:  model.layers.0.self_attn.v_proj.weight(165, 900)  |grad|=2.786e-01\n",
            "  local #202 | global #452:  model.layers.0.self_attn.v_proj.weight(44, 865)  |grad|=2.769e-01\n",
            "  local #203 | global #453:  model.layers.1.mlp.gate_proj.weight(6988, 553)  |grad|=2.764e-01\n",
            "  local #204 | global #454:  model.layers.0.self_attn.v_proj.weight(1, 865)  |grad|=2.759e-01\n",
            "  local #205 | global #455:  lm_head.weight(28813, 1283)  |grad|=2.756e-01\n",
            "  local #206 | global #456:  model.layers.0.self_attn.v_proj.weight(198, 865)  |grad|=2.749e-01\n",
            "  local #207 | global #457:  model.layers.0.self_attn.v_proj.weight(120, 900)  |grad|=2.749e-01\n",
            "  local #208 | global #458:  model.layers.27.mlp.up_proj.weight(761, 553)  |grad|=2.747e-01\n",
            "  local #209 | global #459:  model.layers.0.self_attn.v_proj.weight(194, 900)  |grad|=2.734e-01\n",
            "  local #210 | global #460:  model.layers.0.self_attn.v_proj.weight(0, 458)  |grad|=2.734e-01\n",
            "  local #211 | global #461:  model.layers.1.mlp.gate_proj.weight(544, 147)  |grad|=2.734e-01\n",
            "  local #212 | global #462:  model.layers.3.mlp.gate_proj.weight(8521, 940)  |grad|=2.729e-01\n",
            "  local #213 | global #463:  lm_head.weight(81031, 1283)  |grad|=2.722e-01\n",
            "  local #214 | global #464:  model.layers.0.self_attn.v_proj.weight(231, 900)  |grad|=2.720e-01\n",
            "  local #215 | global #465:  model.layers.0.self_attn.v_proj.weight(103, 465)  |grad|=2.720e-01\n",
            "  local #216 | global #466:  model.layers.0.self_attn.v_proj.weight(117, 458)  |grad|=2.695e-01\n",
            "  local #217 | global #467:  model.layers.0.self_attn.v_proj.weight(84, 458)  |grad|=2.676e-01\n",
            "  local #218 | global #468:  model.layers.0.self_attn.v_proj.weight(154, 793)  |grad|=2.676e-01\n",
            "  local #219 | global #469:  lm_head.weight(13, 793)  |grad|=2.676e-01\n",
            "  local #220 | global #470:  lm_head.weight(11, 465)  |grad|=2.671e-01\n",
            "  local #221 | global #471:  model.layers.0.self_attn.v_proj.weight(176, 1073)  |grad|=2.664e-01\n",
            "  local #222 | global #472:  model.layers.0.self_attn.v_proj.weight(198, 458)  |grad|=2.659e-01\n",
            "  local #223 | global #473:  model.layers.0.self_attn.v_proj.weight(35, 865)  |grad|=2.649e-01\n",
            "  local #224 | global #474:  model.layers.0.self_attn.v_proj.weight(221, 1381)  |grad|=2.649e-01\n",
            "  local #225 | global #475:  lm_head.weight(1154, 1105)  |grad|=2.649e-01\n",
            "  local #226 | global #476:  model.layers.0.self_attn.v_proj.weight(176, 458)  |grad|=2.639e-01\n",
            "  local #227 | global #477:  model.layers.0.self_attn.v_proj.weight(224, 865)  |grad|=2.637e-01\n",
            "  local #228 | global #478:  model.layers.0.self_attn.v_proj.weight(150, 900)  |grad|=2.632e-01\n",
            "  local #229 | global #479:  model.layers.0.self_attn.v_proj.weight(29, 465)  |grad|=2.627e-01\n",
            "  local #230 | global #480:  model.layers.2.mlp.gate_proj.weight(4742, 553)  |grad|=2.620e-01\n",
            "  local #231 | global #481:  model.layers.0.self_attn.v_proj.weight(4, 865)  |grad|=2.617e-01\n",
            "  local #232 | global #482:  model.layers.0.self_attn.o_proj.weight(147, 462)  |grad|=2.612e-01\n",
            "  local #233 | global #483:  model.layers.3.self_attn.v_proj.weight(36, 713)  |grad|=2.605e-01\n",
            "  local #234 | global #485:  lm_head.weight(77018, 1283)  |grad|=2.595e-01\n",
            "  local #235 | global #486:  model.layers.0.self_attn.v_proj.weight(112, 900)  |grad|=2.593e-01\n",
            "  local #236 | global #487:  model.layers.0.self_attn.o_proj.weight(147, 206)  |grad|=2.593e-01\n",
            "  local #237 | global #488:  model.layers.0.self_attn.v_proj.weight(74, 900)  |grad|=2.588e-01\n",
            "  local #238 | global #489:  model.layers.0.self_attn.v_proj.weight(221, 458)  |grad|=2.588e-01\n",
            "  local #239 | global #490:  model.layers.0.self_attn.v_proj.weight(142, 865)  |grad|=2.588e-01\n",
            "  local #240 | global #491:  lm_head.weight(151646, 1157)  |grad|=2.583e-01\n",
            "  local #241 | global #492:  model.layers.0.self_attn.v_proj.weight(69, 865)  |grad|=2.581e-01\n",
            "  local #242 | global #493:  model.layers.0.self_attn.v_proj.weight(195, 900)  |grad|=2.578e-01\n",
            "  local #243 | global #494:  model.layers.0.self_attn.v_proj.weight(132, 900)  |grad|=2.576e-01\n",
            "  local #244 | global #495:  model.layers.0.self_attn.v_proj.weight(17, 900)  |grad|=2.576e-01\n",
            "  local #245 | global #496:  lm_head.weight(330, 1283)  |grad|=2.576e-01\n",
            "  local #246 | global #497:  model.layers.0.self_attn.v_proj.weight(82, 900)  |grad|=2.573e-01\n",
            "  local #247 | global #498:  model.layers.1.mlp.gate_proj.weight(1364, 147)  |grad|=2.573e-01\n",
            "  local #248 | global #499:  model.layers.27.mlp.up_proj.weight(761, 520)  |grad|=2.568e-01\n",
            "  local #249 | global #500:  model.layers.1.mlp.gate_proj.weight(8582, 147)  |grad|=2.566e-01\n",
            "  local #250 | global #501:  model.layers.0.self_attn.v_proj.weight(117, 1381)  |grad|=2.563e-01\n"
          ]
        }
      ]
    }
  ]
}