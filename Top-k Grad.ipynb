{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOurZzAUU9gqXHDvvRj3fjo"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzboobhcXnuD",
        "outputId": "1407ba0f-97a8-4125-e3a7-94ada1807642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top-250 most sensitive tensor elements:\n",
            "  #1: transformer.wte.weight(2488, 496)  |grad|=5.235e+00\n",
            "  #2: transformer.wte.weight(837, 496)  |grad|=4.481e+00\n",
            "  #3: transformer.wte.weight(198, 496)  |grad|=3.247e+00\n",
            "  #4: transformer.wte.weight(11, 496)  |grad|=2.898e+00\n",
            "  #5: transformer.wte.weight(34315, 496)  |grad|=2.669e+00\n",
            "  #6: transformer.wte.weight(796, 496)  |grad|=2.517e+00\n",
            "  #7: transformer.wte.weight(764, 496)  |grad|=2.499e+00\n",
            "  #8: transformer.wte.weight(13, 496)  |grad|=2.427e+00\n",
            "  #9: transformer.wte.weight(220, 496)  |grad|=2.242e+00\n",
            "  #10: transformer.wte.weight(2488, 430)  |grad|=2.207e+00\n",
            "  #11: transformer.wte.weight(198, 430)  |grad|=2.154e+00\n",
            "  #12: transformer.wte.weight(31, 496)  |grad|=2.059e+00\n",
            "  #13: transformer.wte.weight(5187, 496)  |grad|=2.046e+00\n",
            "  #14: transformer.wte.weight(49063, 496)  |grad|=2.029e+00\n",
            "  #15: transformer.wte.weight(27583, 496)  |grad|=2.001e+00\n",
            "  #16: transformer.wte.weight(837, 430)  |grad|=1.993e+00\n",
            "  #17: transformer.wte.weight(6645, 496)  |grad|=1.939e+00\n",
            "  #18: transformer.wte.weight(21165, 496)  |grad|=1.789e+00\n",
            "  #19: transformer.wte.weight(15281, 496)  |grad|=1.745e+00\n",
            "  #20: transformer.wte.weight(3056, 496)  |grad|=1.702e+00\n",
            "  #21: transformer.wte.weight(12164, 496)  |grad|=1.672e+00\n",
            "  #22: transformer.wte.weight(198, 36)  |grad|=1.671e+00\n",
            "  #23: transformer.wte.weight(497, 496)  |grad|=1.666e+00\n",
            "  #24: transformer.wte.weight(2488, 36)  |grad|=1.663e+00\n",
            "  #25: transformer.wte.weight(21371, 496)  |grad|=1.626e+00\n",
            "  #26: transformer.wte.weight(449, 496)  |grad|=1.624e+00\n",
            "  #27: transformer.wte.weight(40902, 496)  |grad|=1.597e+00\n",
            "  #28: transformer.wte.weight(8670, 496)  |grad|=1.562e+00\n",
            "  #29: transformer.wte.weight(3999, 496)  |grad|=1.561e+00\n",
            "  #30: transformer.wte.weight(5178, 496)  |grad|=1.556e+00\n",
            "  #31: transformer.wte.weight(8942, 496)  |grad|=1.540e+00\n",
            "  #32: transformer.wte.weight(837, 36)  |grad|=1.508e+00\n",
            "  #33: transformer.wte.weight(4544, 496)  |grad|=1.505e+00\n",
            "  #34: transformer.wte.weight(262, 496)  |grad|=1.470e+00\n",
            "  #35: transformer.wte.weight(8118, 496)  |grad|=1.467e+00\n",
            "  #36: transformer.wte.weight(18884, 496)  |grad|=1.432e+00\n",
            "  #37: transformer.wte.weight(12108, 496)  |grad|=1.377e+00\n",
            "  #38: transformer.wte.weight(27512, 496)  |grad|=1.364e+00\n",
            "  #39: transformer.wte.weight(44665, 496)  |grad|=1.353e+00\n",
            "  #40: transformer.wte.weight(23214, 496)  |grad|=1.348e+00\n",
            "  #41: transformer.wte.weight(366, 496)  |grad|=1.336e+00\n",
            "  #42: transformer.wte.weight(796, 430)  |grad|=1.317e+00\n",
            "  #43: transformer.wte.weight(45674, 496)  |grad|=1.313e+00\n",
            "  #44: transformer.wte.weight(16825, 496)  |grad|=1.310e+00\n",
            "  #45: transformer.wte.weight(19498, 496)  |grad|=1.309e+00\n",
            "  #46: transformer.wte.weight(15767, 496)  |grad|=1.303e+00\n",
            "  #47: transformer.wte.weight(28972, 496)  |grad|=1.301e+00\n",
            "  #48: transformer.wte.weight(11, 430)  |grad|=1.279e+00\n",
            "  #49: transformer.wte.weight(34315, 430)  |grad|=1.277e+00\n",
            "  #50: transformer.wte.weight(3248, 496)  |grad|=1.268e+00\n",
            "  #51: transformer.wte.weight(37470, 496)  |grad|=1.264e+00\n",
            "  #52: transformer.wte.weight(10849, 496)  |grad|=1.261e+00\n",
            "  #53: transformer.wte.weight(11849, 496)  |grad|=1.254e+00\n",
            "  #54: transformer.wte.weight(37885, 496)  |grad|=1.243e+00\n",
            "  #55: transformer.wte.weight(39452, 496)  |grad|=1.242e+00\n",
            "  #56: transformer.wte.weight(10278, 496)  |grad|=1.232e+00\n",
            "  #57: transformer.wte.weight(29285, 496)  |grad|=1.228e+00\n",
            "  #58: transformer.wte.weight(2448, 496)  |grad|=1.222e+00\n",
            "  #59: transformer.wte.weight(9398, 496)  |grad|=1.221e+00\n",
            "  #60: transformer.wte.weight(4960, 496)  |grad|=1.216e+00\n",
            "  #61: transformer.wte.weight(6445, 496)  |grad|=1.216e+00\n",
            "  #62: transformer.wte.weight(12592, 496)  |grad|=1.210e+00\n",
            "  #63: transformer.wte.weight(11565, 496)  |grad|=1.206e+00\n",
            "  #64: transformer.wte.weight(2039, 496)  |grad|=1.196e+00\n",
            "  #65: transformer.wte.weight(286, 496)  |grad|=1.191e+00\n",
            "  #66: transformer.wte.weight(19439, 496)  |grad|=1.183e+00\n",
            "  #67: transformer.wte.weight(564, 496)  |grad|=1.180e+00\n",
            "  #68: transformer.wte.weight(26451, 496)  |grad|=1.178e+00\n",
            "  #69: transformer.wte.weight(14818, 496)  |grad|=1.177e+00\n",
            "  #70: transformer.wte.weight(39261, 496)  |grad|=1.170e+00\n",
            "  #71: transformer.wte.weight(40929, 496)  |grad|=1.163e+00\n",
            "  #72: transformer.wte.weight(11852, 496)  |grad|=1.161e+00\n",
            "  #73: transformer.wte.weight(4302, 496)  |grad|=1.153e+00\n",
            "  #74: transformer.wte.weight(39601, 496)  |grad|=1.144e+00\n",
            "  #75: transformer.wte.weight(220, 430)  |grad|=1.144e+00\n",
            "  #76: transformer.wte.weight(15518, 496)  |grad|=1.142e+00\n",
            "  #77: transformer.wte.weight(1279, 496)  |grad|=1.133e+00\n",
            "  #78: transformer.wte.weight(8225, 496)  |grad|=1.132e+00\n",
            "  #79: transformer.wte.weight(13, 430)  |grad|=1.120e+00\n",
            "  #80: transformer.wte.weight(18551, 496)  |grad|=1.119e+00\n",
            "  #81: transformer.wte.weight(38237, 496)  |grad|=1.114e+00\n",
            "  #82: transformer.wte.weight(2635, 496)  |grad|=1.112e+00\n",
            "  #83: transformer.wte.weight(25656, 496)  |grad|=1.112e+00\n",
            "  #84: transformer.wte.weight(338, 496)  |grad|=1.103e+00\n",
            "  #85: transformer.wte.weight(764, 430)  |grad|=1.102e+00\n",
            "  #86: transformer.wte.weight(3240, 496)  |grad|=1.102e+00\n",
            "  #87: transformer.wte.weight(30498, 496)  |grad|=1.095e+00\n",
            "  #88: transformer.wte.weight(35259, 496)  |grad|=1.094e+00\n",
            "  #89: transformer.wte.weight(24023, 496)  |grad|=1.087e+00\n",
            "  #90: transformer.wte.weight(44302, 496)  |grad|=1.085e+00\n",
            "  #91: transformer.wte.weight(27583, 430)  |grad|=1.084e+00\n",
            "  #92: transformer.wte.weight(28437, 496)  |grad|=1.083e+00\n",
            "  #93: transformer.wte.weight(3592, 496)  |grad|=1.083e+00\n",
            "  #94: transformer.wte.weight(39644, 496)  |grad|=1.082e+00\n",
            "  #95: transformer.wte.weight(33911, 496)  |grad|=1.077e+00\n",
            "  #96: transformer.wte.weight(10880, 496)  |grad|=1.076e+00\n",
            "  #97: transformer.wte.weight(20404, 496)  |grad|=1.074e+00\n",
            "  #98: transformer.wte.weight(705, 496)  |grad|=1.072e+00\n",
            "  #99: transformer.wte.weight(796, 36)  |grad|=1.065e+00\n",
            "  #100: transformer.wte.weight(29490, 496)  |grad|=1.061e+00\n",
            "  #101: transformer.wte.weight(46906, 496)  |grad|=1.060e+00\n",
            "  #102: transformer.wte.weight(24872, 496)  |grad|=1.057e+00\n",
            "  #103: transformer.wte.weight(287, 496)  |grad|=1.055e+00\n",
            "  #104: transformer.wte.weight(47891, 496)  |grad|=1.049e+00\n",
            "  #105: transformer.wte.weight(257, 496)  |grad|=1.048e+00\n",
            "  #106: transformer.wte.weight(1375, 496)  |grad|=1.042e+00\n",
            "  #107: transformer.wte.weight(978, 496)  |grad|=1.041e+00\n",
            "  #108: transformer.wte.weight(17099, 496)  |grad|=1.037e+00\n",
            "  #109: transformer.wte.weight(10769, 496)  |grad|=1.036e+00\n",
            "  #110: transformer.wte.weight(1665, 496)  |grad|=1.030e+00\n",
            "  #111: transformer.wte.weight(9719, 496)  |grad|=1.022e+00\n",
            "  #112: transformer.wte.weight(8599, 496)  |grad|=1.017e+00\n",
            "  #113: transformer.wte.weight(27132, 496)  |grad|=1.015e+00\n",
            "  #114: transformer.wte.weight(11, 36)  |grad|=1.015e+00\n",
            "  #115: transformer.wte.weight(25979, 496)  |grad|=1.012e+00\n",
            "  #116: transformer.wte.weight(3261, 496)  |grad|=1.007e+00\n",
            "  #117: transformer.wte.weight(38580, 496)  |grad|=9.963e-01\n",
            "  #118: transformer.wte.weight(11523, 496)  |grad|=9.962e-01\n",
            "  #119: transformer.wte.weight(3687, 496)  |grad|=9.959e-01\n",
            "  #120: transformer.wte.weight(32684, 496)  |grad|=9.945e-01\n",
            "  #121: transformer.wte.weight(14780, 496)  |grad|=9.882e-01\n",
            "  #122: transformer.wte.weight(32831, 496)  |grad|=9.881e-01\n",
            "  #123: transformer.wte.weight(20531, 496)  |grad|=9.869e-01\n",
            "  #124: transformer.wte.weight(9718, 496)  |grad|=9.805e-01\n",
            "  #125: transformer.wte.weight(1379, 496)  |grad|=9.805e-01\n",
            "  #126: transformer.wte.weight(20842, 496)  |grad|=9.728e-01\n",
            "  #127: transformer.wte.weight(7096, 496)  |grad|=9.698e-01\n",
            "  #128: transformer.wte.weight(13709, 496)  |grad|=9.697e-01\n",
            "  #129: transformer.wte.weight(14074, 496)  |grad|=9.689e-01\n",
            "  #130: transformer.wte.weight(46547, 496)  |grad|=9.649e-01\n",
            "  #131: transformer.wte.weight(304, 496)  |grad|=9.643e-01\n",
            "  #132: transformer.wte.weight(49063, 430)  |grad|=9.596e-01\n",
            "  #133: transformer.wte.weight(220, 36)  |grad|=9.595e-01\n",
            "  #134: transformer.wte.weight(34315, 36)  |grad|=9.577e-01\n",
            "  #135: transformer.wte.weight(32164, 496)  |grad|=9.575e-01\n",
            "  #136: transformer.wte.weight(21679, 496)  |grad|=9.564e-01\n",
            "  #137: transformer.wte.weight(46619, 496)  |grad|=9.518e-01\n",
            "  #138: transformer.wte.weight(46446, 496)  |grad|=9.491e-01\n",
            "  #139: transformer.wte.weight(2046, 496)  |grad|=9.488e-01\n",
            "  #140: transformer.wte.weight(6252, 496)  |grad|=9.469e-01\n",
            "  #141: transformer.wte.weight(4991, 496)  |grad|=9.458e-01\n",
            "  #142: transformer.wte.weight(9502, 496)  |grad|=9.456e-01\n",
            "  #143: transformer.wte.weight(5686, 496)  |grad|=9.453e-01\n",
            "  #144: transformer.wte.weight(40580, 496)  |grad|=9.438e-01\n",
            "  #145: transformer.wte.weight(3905, 496)  |grad|=9.426e-01\n",
            "  #146: transformer.wte.weight(46811, 496)  |grad|=9.419e-01\n",
            "  #147: transformer.wte.weight(5838, 496)  |grad|=9.387e-01\n",
            "  #148: transformer.wte.weight(12686, 496)  |grad|=9.378e-01\n",
            "  #149: transformer.wte.weight(15608, 496)  |grad|=9.364e-01\n",
            "  #150: transformer.wte.weight(6285, 496)  |grad|=9.331e-01\n",
            "  #151: transformer.wte.weight(314, 496)  |grad|=9.325e-01\n",
            "  #152: transformer.wte.weight(10343, 496)  |grad|=9.322e-01\n",
            "  #153: transformer.wte.weight(37449, 496)  |grad|=9.305e-01\n",
            "  #154: transformer.wte.weight(15281, 430)  |grad|=9.274e-01\n",
            "  #155: transformer.wte.weight(27599, 496)  |grad|=9.240e-01\n",
            "  #156: transformer.wte.weight(43579, 496)  |grad|=9.174e-01\n",
            "  #157: transformer.wte.weight(47922, 496)  |grad|=9.140e-01\n",
            "  #158: transformer.wte.weight(5108, 496)  |grad|=9.011e-01\n",
            "  #159: transformer.wte.weight(13098, 496)  |grad|=8.999e-01\n",
            "  #160: transformer.wte.weight(1757, 496)  |grad|=8.985e-01\n",
            "  #161: transformer.wte.weight(14328, 496)  |grad|=8.973e-01\n",
            "  #162: transformer.wte.weight(13, 36)  |grad|=8.964e-01\n",
            "  #163: transformer.wte.weight(5187, 430)  |grad|=8.959e-01\n",
            "  #164: transformer.wte.weight(520, 496)  |grad|=8.954e-01\n",
            "  #165: transformer.wte.weight(290, 496)  |grad|=8.946e-01\n",
            "  #166: transformer.wte.weight(2646, 496)  |grad|=8.944e-01\n",
            "  #167: transformer.wte.weight(9552, 496)  |grad|=8.927e-01\n",
            "  #168: transformer.wte.weight(764, 36)  |grad|=8.926e-01\n",
            "  #169: transformer.wte.weight(31, 430)  |grad|=8.916e-01\n",
            "  #170: transformer.wte.weight(20603, 496)  |grad|=8.907e-01\n",
            "  #171: transformer.wte.weight(5628, 496)  |grad|=8.890e-01\n",
            "  #172: transformer.wte.weight(5706, 496)  |grad|=8.883e-01\n",
            "  #173: transformer.wte.weight(47489, 496)  |grad|=8.847e-01\n",
            "  #174: transformer.wte.weight(289, 496)  |grad|=8.846e-01\n",
            "  #175: transformer.wte.weight(7158, 496)  |grad|=8.829e-01\n",
            "  #176: transformer.wte.weight(645, 496)  |grad|=8.827e-01\n",
            "  #177: transformer.wte.weight(26249, 496)  |grad|=8.803e-01\n",
            "  #178: transformer.wte.weight(8765, 496)  |grad|=8.784e-01\n",
            "  #179: transformer.wte.weight(46626, 496)  |grad|=8.773e-01\n",
            "  #180: transformer.wte.weight(7653, 496)  |grad|=8.723e-01\n",
            "  #181: transformer.wte.weight(49931, 496)  |grad|=8.721e-01\n",
            "  #182: transformer.wte.weight(45630, 496)  |grad|=8.713e-01\n",
            "  #183: transformer.wte.weight(9450, 496)  |grad|=8.711e-01\n",
            "  #184: transformer.wte.weight(10977, 496)  |grad|=8.705e-01\n",
            "  #185: transformer.wte.weight(16140, 496)  |grad|=8.688e-01\n",
            "  #186: transformer.wte.weight(1210, 496)  |grad|=8.686e-01\n",
            "  #187: transformer.h.5.ln_1.weight(447,)  |grad|=8.676e-01\n",
            "  #188: transformer.wte.weight(34927, 496)  |grad|=8.674e-01\n",
            "  #189: transformer.wte.weight(33298, 496)  |grad|=8.673e-01\n",
            "  #190: transformer.wte.weight(449, 430)  |grad|=8.654e-01\n",
            "  #191: transformer.wte.weight(5915, 496)  |grad|=8.639e-01\n",
            "  #192: transformer.wte.weight(9666, 496)  |grad|=8.585e-01\n",
            "  #193: transformer.wte.weight(6645, 430)  |grad|=8.585e-01\n",
            "  #194: transformer.wte.weight(15945, 496)  |grad|=8.571e-01\n",
            "  #195: transformer.wte.weight(10169, 496)  |grad|=8.543e-01\n",
            "  #196: transformer.wte.weight(8050, 496)  |grad|=8.541e-01\n",
            "  #197: transformer.wte.weight(42192, 496)  |grad|=8.536e-01\n",
            "  #198: transformer.wte.weight(13922, 496)  |grad|=8.536e-01\n",
            "  #199: transformer.wte.weight(784, 496)  |grad|=8.513e-01\n",
            "  #200: transformer.wte.weight(284, 496)  |grad|=8.504e-01\n",
            "  #201: transformer.wte.weight(4847, 496)  |grad|=8.500e-01\n",
            "  #202: transformer.wte.weight(2089, 496)  |grad|=8.476e-01\n",
            "  #203: transformer.wte.weight(23975, 496)  |grad|=8.460e-01\n",
            "  #204: transformer.wte.weight(554, 496)  |grad|=8.457e-01\n",
            "  #205: transformer.wte.weight(31210, 496)  |grad|=8.455e-01\n",
            "  #206: transformer.wte.weight(12469, 496)  |grad|=8.453e-01\n",
            "  #207: transformer.wte.weight(5780, 496)  |grad|=8.446e-01\n",
            "  #208: transformer.wte.weight(7459, 496)  |grad|=8.426e-01\n",
            "  #209: transformer.wte.weight(19305, 496)  |grad|=8.424e-01\n",
            "  #210: transformer.wte.weight(4319, 496)  |grad|=8.416e-01\n",
            "  #211: transformer.wte.weight(12164, 430)  |grad|=8.411e-01\n",
            "  #212: transformer.wte.weight(1971, 496)  |grad|=8.409e-01\n",
            "  #213: transformer.wte.weight(20400, 496)  |grad|=8.388e-01\n",
            "  #214: transformer.wte.weight(983, 496)  |grad|=8.379e-01\n",
            "  #215: transformer.wte.weight(47268, 496)  |grad|=8.378e-01\n",
            "  #216: transformer.wte.weight(35293, 496)  |grad|=8.359e-01\n",
            "  #217: transformer.wte.weight(3831, 496)  |grad|=8.320e-01\n",
            "  #218: transformer.wte.weight(5030, 496)  |grad|=8.316e-01\n",
            "  #219: transformer.wte.weight(791, 496)  |grad|=8.315e-01\n",
            "  #220: transformer.wte.weight(5825, 496)  |grad|=8.294e-01\n",
            "  #221: transformer.wte.weight(32355, 496)  |grad|=8.285e-01\n",
            "  #222: transformer.wte.weight(25176, 496)  |grad|=8.264e-01\n",
            "  #223: transformer.wte.weight(8685, 496)  |grad|=8.260e-01\n",
            "  #224: transformer.wte.weight(6046, 496)  |grad|=8.231e-01\n",
            "  #225: transformer.wte.weight(49203, 496)  |grad|=8.198e-01\n",
            "  #226: transformer.wte.weight(13257, 496)  |grad|=8.195e-01\n",
            "  #227: transformer.wte.weight(3208, 496)  |grad|=8.150e-01\n",
            "  #228: transformer.wte.weight(6881, 496)  |grad|=8.148e-01\n",
            "  #229: transformer.wte.weight(9634, 496)  |grad|=8.129e-01\n",
            "  #230: transformer.wte.weight(2644, 496)  |grad|=8.085e-01\n",
            "  #231: transformer.wte.weight(7164, 496)  |grad|=8.080e-01\n",
            "  #232: transformer.wte.weight(20716, 496)  |grad|=8.070e-01\n",
            "  #233: transformer.wte.weight(2284, 496)  |grad|=8.067e-01\n",
            "  #234: transformer.wte.weight(11287, 496)  |grad|=8.064e-01\n",
            "  #235: transformer.wte.weight(37884, 496)  |grad|=8.057e-01\n",
            "  #236: transformer.wte.weight(12551, 496)  |grad|=8.045e-01\n",
            "  #237: transformer.wte.weight(6107, 496)  |grad|=8.045e-01\n",
            "  #238: transformer.wte.weight(7979, 496)  |grad|=8.044e-01\n",
            "  #239: transformer.wte.weight(12099, 496)  |grad|=8.011e-01\n",
            "  #240: transformer.wte.weight(2688, 496)  |grad|=7.983e-01\n",
            "  #241: transformer.wte.weight(7897, 496)  |grad|=7.979e-01\n",
            "  #242: transformer.wte.weight(18501, 496)  |grad|=7.958e-01\n",
            "  #243: transformer.wte.weight(9281, 496)  |grad|=7.921e-01\n",
            "  #244: transformer.wte.weight(21165, 430)  |grad|=7.920e-01\n",
            "  #245: transformer.wte.weight(1126, 496)  |grad|=7.908e-01\n",
            "  #246: transformer.wte.weight(27583, 36)  |grad|=7.901e-01\n",
            "  #247: transformer.wte.weight(39775, 496)  |grad|=7.892e-01\n",
            "  #248: transformer.wte.weight(7920, 496)  |grad|=7.873e-01\n",
            "  #249: transformer.wte.weight(37053, 496)  |grad|=7.863e-01\n",
            "  #250: transformer.wte.weight(12862, 496)  |grad|=7.852e-01\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 0) Setup\n",
        "# ============================================================\n",
        "import torch, random\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
        "\n",
        "# ---- Config ----\n",
        "SEED       = 123\n",
        "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_ID   = \"gpt2\"\n",
        "SPLIT      = \"train\"\n",
        "SEQ_LEN    = 1024\n",
        "BATCH_SIZE = 16\n",
        "MAX_STEPS  = 1100\n",
        "TOP_K      = 250  # report top-k most sensitive elements\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# ============================================================\n",
        "# 1) Model & Tokenizer\n",
        "# ============================================================\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "tok.pad_token = tok.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
        "model.train()  # need gradients\n",
        "\n",
        "# ============================================================\n",
        "# 2) Dataset & Chunking\n",
        "# ============================================================\n",
        "wiki = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=SPLIT)\n",
        "\n",
        "def chunk_generator():\n",
        "    cache = []\n",
        "    for doc in wiki:\n",
        "        cache.extend(tok(doc[\"text\"]).input_ids)\n",
        "        while len(cache) >= SEQ_LEN + 1:\n",
        "            win, cache = cache[:SEQ_LEN+1], cache[SEQ_LEN+1:]  # non-overlap\n",
        "            yield win[:-1], win[1:]\n",
        "\n",
        "def get_batch(gen, bs=BATCH_SIZE):\n",
        "    buf = []\n",
        "    for x, _ in gen:\n",
        "        buf.append(x)\n",
        "        if len(buf) == bs:\n",
        "            yield torch.tensor(buf, device=DEVICE)\n",
        "            buf = []\n",
        "\n",
        "# ============================================================\n",
        "# 3) Gradient Scan\n",
        "# ============================================================\n",
        "param_dict  = {n: p for n, p in model.named_parameters() if p.requires_grad}\n",
        "running_max = {n: torch.zeros_like(p, device=\"cpu\") for n, p in param_dict.items()}\n",
        "\n",
        "for step, inp in enumerate(get_batch(chunk_generator()), 1):\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    loss = model(inp, labels=inp).loss\n",
        "    loss.backward()\n",
        "    for name, p in param_dict.items():\n",
        "        running_max[name] = torch.maximum(\n",
        "            running_max[name],\n",
        "            p.grad.detach().abs().to(\"cpu\")\n",
        "        )\n",
        "    if step >= MAX_STEPS:\n",
        "        break\n",
        "\n",
        "# ============================================================\n",
        "# 4) Find Top-K Sensitive Coordinates\n",
        "# ============================================================\n",
        "candidates = []\n",
        "for name, rm in running_max.items():\n",
        "    k_local = min(TOP_K, rm.numel())\n",
        "    if k_local > 0:\n",
        "        vals, idxs = torch.topk(rm.view(-1), k_local)\n",
        "        for v, flat in zip(vals, idxs):\n",
        "            coord = torch.unravel_index(flat, rm.shape)\n",
        "            candidates.append((v.item(), name, coord))\n",
        "\n",
        "candidates.sort(key=lambda t: t[0], reverse=True)\n",
        "topk_entries = candidates[:TOP_K]\n",
        "\n",
        "print(f\"\\nTop-{TOP_K} most sensitive tensor elements:\")\n",
        "for rank, (val, name, coord) in enumerate(topk_entries, 1):\n",
        "    print(f\"  #{rank}: {name}{tuple(map(int,coord))}  |grad|={val:.3e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 0) Setup & Config\n",
        "# ============================================================\n",
        "import random\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# ---- Experiment knobs ----\n",
        "SEED       = 123\n",
        "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "MODEL_ID   = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"   # or \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "SPLIT      = \"train\"\n",
        "SEQ_LEN    = 1024\n",
        "BATCH_SIZE = 16\n",
        "MAX_STEPS  = 1100\n",
        "TOP_K      = 250\n",
        "\n",
        "# ---- Filter list for second top-K (local ranks) ----\n",
        "FILTER_ENABLED = True\n",
        "FILTER_PARAM_NAMES = [\n",
        "    # Embeddings\n",
        "    \"transformer.wte.weight\",\n",
        "    \"transformer.wpe.weight\",\n",
        "\n",
        "    # LayerNorm weights (ln_1, ln_2)\n",
        "    *[f\"transformer.h.{i}.ln_1.weight\" for i in range(12)],\n",
        "    *[f\"transformer.h.{i}.ln_2.weight\" for i in range(12)],\n",
        "\n",
        "    # LayerNorm biases (ln_1, ln_2)\n",
        "    *[f\"transformer.h.{i}.ln_1.bias\"  for i in range(12)],\n",
        "    *[f\"transformer.h.{i}.ln_2.bias\"  for i in range(12)],\n",
        "\n",
        "    # Final LayerNorm weight + bias\n",
        "    \"transformer.ln_f.weight\",\n",
        "    \"transformer.ln_f.bias\",\n",
        "]\n",
        "\n",
        "# Reproducibility\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# ============================================================\n",
        "# 1) Model & Tokenizer\n",
        "# ============================================================\n",
        "print(\"Loading tokenizer...\")\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token or tok.convert_ids_to_tokens(0)\n",
        "\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    dtype=torch.float16,\n",
        "    device_map={\"\": DEVICE},\n",
        ")\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model.train()  # gradients required for scan\n",
        "\n",
        "print(f\"Model loaded on {DEVICE} with dtype {next(model.parameters()).dtype}\")\n",
        "\n",
        "# ============================================================\n",
        "# 2) Dataset & Chunking\n",
        "# ============================================================\n",
        "print(\"Loading dataset...\")\n",
        "wiki = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=SPLIT)\n",
        "\n",
        "def chunk_generator():\n",
        "    \"\"\"Yield (inp, labels) windows of size SEQ_LEN.\"\"\"\n",
        "    cache = []\n",
        "    for doc in wiki:\n",
        "        ids = tok(doc[\"text\"]).input_ids\n",
        "        cache.extend(ids)\n",
        "        while len(cache) >= SEQ_LEN + 1:\n",
        "            window = cache[:SEQ_LEN+1]\n",
        "            cache = cache[SEQ_LEN+1:]\n",
        "            yield window[:-1], window[1:]\n",
        "\n",
        "def get_batch(gen, bs=BATCH_SIZE):\n",
        "    \"\"\"Stack windows into batches.\"\"\"\n",
        "    buf = []\n",
        "    for inp, _ in gen:\n",
        "        buf.append(inp)\n",
        "        if len(buf) == bs:\n",
        "            yield torch.tensor(buf, dtype=torch.long, device=DEVICE)\n",
        "            buf = []\n",
        "\n",
        "# ============================================================\n",
        "# 3) Parameter Dictionary\n",
        "# ============================================================\n",
        "param_dict = {name: p for name, p in model.named_parameters() if p.requires_grad}\n",
        "\n",
        "print(f\"Total tracked tensors: {len(param_dict)}\")\n",
        "print(f\"Total elements: {sum(p.numel() for p in param_dict.values()):,}\")\n",
        "\n",
        "running_max = {\n",
        "    name: torch.zeros_like(p, device=\"cpu\", dtype=torch.float32)\n",
        "    for name, p in param_dict.items()\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# 4) Gradient Scan\n",
        "# ============================================================\n",
        "print(\"Starting gradient scan...\")\n",
        "stream = get_batch(chunk_generator(), bs=BATCH_SIZE)\n",
        "\n",
        "for step, inp in enumerate(stream, 1):\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    out = model(inp, labels=inp)\n",
        "    loss = out.loss\n",
        "    loss.backward()\n",
        "\n",
        "    for name, p in param_dict.items():\n",
        "        if p.grad is not None:\n",
        "            grad_abs = p.grad.detach().abs().to(\"cpu\", torch.float32)\n",
        "            running_max[name] = torch.maximum(running_max[name], grad_abs)\n",
        "\n",
        "    print(f\"[step {step}] loss={loss.item():.4f}\")\n",
        "\n",
        "    if step >= MAX_STEPS:\n",
        "        print(\"Reached MAX_STEPS â€” stopping scan.\")\n",
        "        break\n",
        "\n",
        "print(\"Gradient scan complete.\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# 5) Build Global TOP-K\n",
        "# ============================================================\n",
        "print(f\"Selecting global Top-{TOP_K} by |grad|...\")\n",
        "\n",
        "candidates = []\n",
        "for name, rm in running_max.items():\n",
        "    rm_flat = rm.view(-1)\n",
        "    k_local = min(TOP_K, rm_flat.numel())\n",
        "    vals, idxs = torch.topk(rm_flat, k_local)\n",
        "    for v, flat_idx in zip(vals, idxs):\n",
        "        coord = torch.unravel_index(flat_idx, rm.shape)\n",
        "        candidates.append((v.item(), name, coord))\n",
        "\n",
        "# Sort globally\n",
        "candidates.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "ranked_global = [\n",
        "    {\n",
        "        \"global_rank\": i + 1,\n",
        "        \"value\": val,\n",
        "        \"name\": name,\n",
        "        \"coord\": coord,\n",
        "    }\n",
        "    for i, (val, name, coord) in enumerate(candidates)\n",
        "]\n",
        "\n",
        "global_topk = ranked_global[:TOP_K]\n",
        "\n",
        "# ============================================================\n",
        "# 6) Filtered TOP-K (local + global ranks)\n",
        "# ============================================================\n",
        "def allowed(name):\n",
        "    return not FILTER_ENABLED or name not in FILTER_PARAM_NAMES\n",
        "\n",
        "filtered_list = [item for item in ranked_global if allowed(item[\"name\"])]\n",
        "filtered_topk = filtered_list[:TOP_K]\n",
        "\n",
        "for i, item in enumerate(filtered_topk, 1):\n",
        "    item[\"local_rank\"] = i\n",
        "\n",
        "# ============================================================\n",
        "# 7) Print Results\n",
        "# ============================================================\n",
        "print(f\"\\n=== Global Top-{TOP_K} most sensitive parameters ===\")\n",
        "for item in global_topk:\n",
        "    coord = \"(\" + \", \".join(str(int(x)) for x in item[\"coord\"]) + \")\"\n",
        "    print(f\"  global #{item['global_rank']:3d}: {item['name']}{coord}  |grad|={item['value']:.3e}\")\n",
        "\n",
        "print(f\"\\n=== Filtered Top-{TOP_K} (local + global ranks) ===\")\n",
        "print(\"Filter:\", FILTER_PARAM_NAMES if FILTER_ENABLED else \"None\")\n",
        "for item in filtered_topk:\n",
        "    coord = \"(\" + \", \".join(str(int(x)) for x in item[\"coord\"]) + \")\"\n",
        "    print(\n",
        "        f\"  local #{item['local_rank']:3d} | global #{item['global_rank']:3d}:  \"\n",
        "        f\"{item['name']}{coord}  |grad|={item['value']:.3e}\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "SgRGEo_e2N94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f39caf8-4c9d-422c-c00f-987686878dac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer...\n",
            "Loading model...\n",
            "Model loaded on cuda with dtype torch.float16\n",
            "Loading dataset...\n",
            "Total tracked tensors: 148\n",
            "Total elements: 124,439,808\n",
            "Starting gradient scan...\n",
            "[step 1] loss=3.7692\n",
            "[step 2] loss=3.7205\n",
            "[step 3] loss=3.8664\n",
            "[step 4] loss=3.8174\n",
            "[step 5] loss=3.7687\n",
            "[step 6] loss=3.7947\n",
            "[step 7] loss=3.6265\n",
            "[step 8] loss=3.6798\n",
            "[step 9] loss=3.7406\n",
            "[step 10] loss=3.7303\n",
            "[step 11] loss=3.7499\n",
            "[step 12] loss=3.7368\n",
            "[step 13] loss=3.7827\n",
            "[step 14] loss=3.7898\n",
            "[step 15] loss=3.5210\n",
            "[step 16] loss=3.9583\n",
            "[step 17] loss=3.5698\n",
            "[step 18] loss=3.6654\n",
            "[step 19] loss=3.6225\n",
            "[step 20] loss=3.6368\n",
            "[step 21] loss=3.9944\n",
            "[step 22] loss=3.7670\n",
            "[step 23] loss=3.6311\n",
            "[step 24] loss=3.7662\n",
            "[step 25] loss=3.7659\n",
            "[step 26] loss=4.0188\n",
            "[step 27] loss=3.8955\n",
            "[step 28] loss=3.9686\n",
            "[step 29] loss=3.8054\n",
            "[step 30] loss=3.8302\n",
            "[step 31] loss=3.7769\n",
            "[step 32] loss=3.7340\n",
            "[step 33] loss=3.6971\n",
            "[step 34] loss=3.7320\n",
            "[step 35] loss=3.7045\n",
            "[step 36] loss=3.7291\n",
            "[step 37] loss=3.7286\n",
            "[step 38] loss=3.7018\n",
            "[step 39] loss=3.5971\n",
            "[step 40] loss=3.7420\n",
            "[step 41] loss=3.8449\n",
            "[step 42] loss=3.6607\n",
            "[step 43] loss=3.6449\n",
            "[step 44] loss=3.6273\n",
            "[step 45] loss=3.8668\n",
            "[step 46] loss=3.8467\n",
            "[step 47] loss=3.6172\n",
            "[step 48] loss=3.6066\n",
            "[step 49] loss=3.7090\n",
            "[step 50] loss=3.6986\n",
            "[step 51] loss=3.8591\n",
            "[step 52] loss=3.7874\n",
            "[step 53] loss=3.7626\n",
            "[step 54] loss=3.7831\n",
            "[step 55] loss=3.7530\n",
            "[step 56] loss=3.6805\n",
            "[step 57] loss=3.7750\n",
            "[step 58] loss=3.8687\n",
            "[step 59] loss=3.7400\n",
            "[step 60] loss=3.6186\n",
            "[step 61] loss=3.8239\n",
            "[step 62] loss=3.6843\n",
            "[step 63] loss=3.9125\n",
            "[step 64] loss=3.7726\n",
            "[step 65] loss=3.6946\n",
            "[step 66] loss=3.7436\n",
            "[step 67] loss=3.5823\n",
            "[step 68] loss=3.5638\n",
            "[step 69] loss=3.6668\n",
            "[step 70] loss=3.8153\n",
            "[step 71] loss=3.8686\n",
            "[step 72] loss=3.8150\n",
            "[step 73] loss=3.7598\n",
            "[step 74] loss=3.7507\n",
            "[step 75] loss=3.5833\n",
            "[step 76] loss=3.5774\n",
            "[step 77] loss=3.6757\n",
            "[step 78] loss=3.7998\n",
            "[step 79] loss=3.7156\n",
            "[step 80] loss=3.8691\n",
            "[step 81] loss=3.9096\n",
            "[step 82] loss=3.8232\n",
            "[step 83] loss=3.5448\n",
            "[step 84] loss=3.7903\n",
            "[step 85] loss=3.9004\n",
            "[step 86] loss=3.4297\n",
            "[step 87] loss=3.6595\n",
            "[step 88] loss=3.8641\n",
            "[step 89] loss=3.7663\n",
            "[step 90] loss=3.6006\n",
            "[step 91] loss=3.5230\n",
            "[step 92] loss=3.7994\n",
            "[step 93] loss=3.7969\n",
            "[step 94] loss=3.7276\n",
            "[step 95] loss=3.4862\n",
            "[step 96] loss=3.7596\n",
            "[step 97] loss=3.6740\n",
            "[step 98] loss=3.8488\n",
            "[step 99] loss=3.8748\n",
            "[step 100] loss=3.4598\n",
            "[step 101] loss=3.7891\n",
            "[step 102] loss=3.5387\n",
            "[step 103] loss=3.8225\n",
            "[step 104] loss=3.5294\n",
            "[step 105] loss=3.8475\n",
            "[step 106] loss=3.7358\n",
            "[step 107] loss=3.7063\n",
            "[step 108] loss=3.6100\n",
            "[step 109] loss=3.7651\n",
            "[step 110] loss=3.8084\n",
            "[step 111] loss=3.6010\n",
            "[step 112] loss=3.6784\n",
            "[step 113] loss=3.7508\n",
            "[step 114] loss=3.7833\n",
            "[step 115] loss=3.7868\n",
            "[step 116] loss=3.6356\n",
            "[step 117] loss=3.7949\n",
            "[step 118] loss=3.7843\n",
            "[step 119] loss=3.7298\n",
            "[step 120] loss=3.8099\n",
            "[step 121] loss=3.8903\n",
            "[step 122] loss=3.8997\n",
            "[step 123] loss=3.8431\n",
            "[step 124] loss=3.5970\n",
            "[step 125] loss=3.8920\n",
            "[step 126] loss=3.6494\n",
            "[step 127] loss=3.8206\n",
            "[step 128] loss=3.7830\n",
            "[step 129] loss=3.5950\n",
            "[step 130] loss=3.8750\n",
            "[step 131] loss=3.8225\n",
            "[step 132] loss=3.7128\n",
            "[step 133] loss=3.6786\n",
            "[step 134] loss=3.6642\n",
            "[step 135] loss=3.6162\n",
            "[step 136] loss=3.6414\n",
            "[step 137] loss=3.5575\n",
            "[step 138] loss=3.6262\n",
            "[step 139] loss=3.9500\n",
            "[step 140] loss=3.9912\n",
            "[step 141] loss=3.7780\n",
            "[step 142] loss=3.8715\n",
            "[step 143] loss=4.1899\n",
            "[step 144] loss=3.6394\n",
            "[step 145] loss=3.9798\n",
            "[step 146] loss=3.7951\n",
            "[step 147] loss=3.8349\n",
            "[step 148] loss=3.5173\n",
            "[step 149] loss=3.7271\n",
            "[step 150] loss=3.5504\n",
            "[step 151] loss=3.7171\n",
            "[step 152] loss=3.6639\n",
            "[step 153] loss=3.7792\n",
            "[step 154] loss=3.8306\n",
            "[step 155] loss=3.5595\n",
            "[step 156] loss=3.7184\n",
            "[step 157] loss=3.7704\n",
            "[step 158] loss=3.7435\n",
            "[step 159] loss=3.7377\n",
            "[step 160] loss=3.5808\n",
            "[step 161] loss=3.8397\n",
            "[step 162] loss=3.7553\n",
            "[step 163] loss=3.8817\n",
            "[step 164] loss=3.7664\n",
            "[step 165] loss=3.6148\n",
            "[step 166] loss=3.6875\n",
            "[step 167] loss=3.8706\n",
            "[step 168] loss=3.8538\n",
            "[step 169] loss=3.7405\n",
            "[step 170] loss=3.6786\n",
            "[step 171] loss=3.6378\n",
            "[step 172] loss=3.7891\n",
            "[step 173] loss=3.9933\n",
            "[step 174] loss=3.8820\n",
            "[step 175] loss=3.4869\n",
            "[step 176] loss=3.4390\n",
            "[step 177] loss=3.6990\n",
            "[step 178] loss=3.7221\n",
            "[step 179] loss=3.7433\n",
            "[step 180] loss=3.7189\n",
            "[step 181] loss=3.7247\n",
            "[step 182] loss=3.7682\n",
            "[step 183] loss=3.7098\n",
            "[step 184] loss=3.6473\n",
            "[step 185] loss=3.6408\n",
            "[step 186] loss=3.7042\n",
            "[step 187] loss=3.9502\n",
            "[step 188] loss=3.9060\n",
            "[step 189] loss=3.5412\n",
            "[step 190] loss=3.6398\n",
            "[step 191] loss=3.4818\n",
            "[step 192] loss=3.6912\n",
            "[step 193] loss=3.7084\n",
            "[step 194] loss=3.5416\n",
            "[step 195] loss=3.5796\n",
            "[step 196] loss=3.6887\n",
            "[step 197] loss=3.6744\n",
            "[step 198] loss=3.7895\n",
            "[step 199] loss=3.5174\n",
            "[step 200] loss=3.5828\n",
            "[step 201] loss=3.9497\n",
            "[step 202] loss=3.7092\n",
            "[step 203] loss=3.9038\n",
            "[step 204] loss=3.7624\n",
            "[step 205] loss=3.8689\n",
            "[step 206] loss=3.5890\n",
            "[step 207] loss=3.8559\n",
            "[step 208] loss=3.7262\n",
            "[step 209] loss=3.7853\n",
            "[step 210] loss=3.9286\n",
            "[step 211] loss=3.8239\n",
            "[step 212] loss=3.7168\n",
            "[step 213] loss=3.7706\n",
            "[step 214] loss=3.6660\n",
            "[step 215] loss=3.6593\n",
            "[step 216] loss=3.8181\n",
            "[step 217] loss=3.8220\n",
            "[step 218] loss=3.6998\n",
            "[step 219] loss=3.8018\n",
            "[step 220] loss=3.5293\n",
            "[step 221] loss=3.7308\n",
            "[step 222] loss=3.7829\n",
            "[step 223] loss=3.8083\n",
            "[step 224] loss=4.0724\n",
            "[step 225] loss=4.0158\n",
            "[step 226] loss=3.7960\n",
            "[step 227] loss=3.8462\n",
            "[step 228] loss=3.6946\n",
            "[step 229] loss=3.7024\n",
            "[step 230] loss=3.7075\n",
            "[step 231] loss=3.6039\n",
            "[step 232] loss=3.8248\n",
            "[step 233] loss=3.8079\n",
            "[step 234] loss=3.8963\n",
            "[step 235] loss=3.7318\n",
            "[step 236] loss=3.7343\n",
            "[step 237] loss=3.7175\n",
            "[step 238] loss=3.8022\n",
            "[step 239] loss=3.7081\n",
            "[step 240] loss=3.8378\n",
            "[step 241] loss=3.6725\n",
            "[step 242] loss=3.6038\n",
            "[step 243] loss=3.8836\n",
            "[step 244] loss=3.7027\n",
            "[step 245] loss=3.7008\n",
            "[step 246] loss=3.9434\n",
            "[step 247] loss=3.6771\n",
            "[step 248] loss=3.7470\n",
            "[step 249] loss=3.8255\n",
            "[step 250] loss=3.7721\n",
            "[step 251] loss=3.7531\n",
            "[step 252] loss=3.8375\n",
            "[step 253] loss=3.9169\n",
            "[step 254] loss=3.6622\n",
            "[step 255] loss=3.7225\n",
            "[step 256] loss=3.7258\n",
            "[step 257] loss=3.6945\n",
            "[step 258] loss=3.9055\n",
            "[step 259] loss=3.8869\n",
            "[step 260] loss=3.8100\n",
            "[step 261] loss=3.7037\n",
            "[step 262] loss=3.6604\n",
            "[step 263] loss=3.6770\n",
            "[step 264] loss=3.6881\n",
            "[step 265] loss=3.9220\n",
            "[step 266] loss=3.7523\n",
            "[step 267] loss=3.8320\n",
            "[step 268] loss=3.9248\n",
            "[step 269] loss=3.6482\n",
            "[step 270] loss=3.7044\n",
            "[step 271] loss=3.9694\n",
            "[step 272] loss=3.7368\n",
            "[step 273] loss=3.3882\n",
            "[step 274] loss=3.6380\n",
            "[step 275] loss=3.7377\n",
            "[step 276] loss=3.2615\n",
            "[step 277] loss=3.7244\n",
            "[step 278] loss=3.4957\n",
            "[step 279] loss=3.8113\n",
            "[step 280] loss=3.7141\n",
            "[step 281] loss=3.6645\n",
            "[step 282] loss=3.8415\n",
            "[step 283] loss=3.5508\n",
            "[step 284] loss=3.7071\n",
            "[step 285] loss=3.6561\n",
            "[step 286] loss=3.7010\n",
            "[step 287] loss=3.7333\n",
            "[step 288] loss=3.7602\n",
            "[step 289] loss=3.7199\n",
            "[step 290] loss=3.7200\n",
            "[step 291] loss=3.5765\n",
            "[step 292] loss=3.7768\n",
            "[step 293] loss=3.7955\n",
            "[step 294] loss=3.6831\n",
            "[step 295] loss=3.6112\n",
            "[step 296] loss=3.8387\n",
            "[step 297] loss=3.6992\n",
            "[step 298] loss=3.8000\n",
            "[step 299] loss=3.8947\n",
            "[step 300] loss=3.7847\n",
            "[step 301] loss=3.7782\n",
            "[step 302] loss=3.7614\n",
            "[step 303] loss=3.6043\n",
            "[step 304] loss=3.6864\n",
            "[step 305] loss=3.5935\n",
            "[step 306] loss=3.7458\n",
            "[step 307] loss=3.5565\n",
            "[step 308] loss=3.5941\n",
            "[step 309] loss=3.6992\n",
            "[step 310] loss=3.6320\n",
            "[step 311] loss=3.6817\n",
            "[step 312] loss=3.7786\n",
            "[step 313] loss=3.9558\n",
            "[step 314] loss=3.6857\n",
            "[step 315] loss=3.6864\n",
            "[step 316] loss=3.7068\n",
            "[step 317] loss=3.6954\n",
            "[step 318] loss=3.8195\n",
            "[step 319] loss=3.7011\n",
            "[step 320] loss=3.5985\n",
            "[step 321] loss=3.6494\n",
            "[step 322] loss=3.7649\n",
            "[step 323] loss=3.7247\n",
            "[step 324] loss=3.7030\n",
            "[step 325] loss=3.8043\n",
            "[step 326] loss=3.7963\n",
            "[step 327] loss=3.2981\n",
            "[step 328] loss=3.6064\n",
            "[step 329] loss=3.8484\n",
            "[step 330] loss=3.8485\n",
            "[step 331] loss=3.5668\n",
            "[step 332] loss=3.7764\n",
            "[step 333] loss=3.7872\n",
            "[step 334] loss=3.8469\n",
            "[step 335] loss=3.8507\n",
            "[step 336] loss=3.9283\n",
            "[step 337] loss=3.8188\n",
            "[step 338] loss=3.6597\n",
            "[step 339] loss=3.4222\n",
            "[step 340] loss=3.7109\n",
            "[step 341] loss=3.6780\n",
            "[step 342] loss=3.7948\n",
            "[step 343] loss=3.7077\n",
            "[step 344] loss=3.6414\n",
            "[step 345] loss=3.7704\n",
            "[step 346] loss=3.8607\n",
            "[step 347] loss=3.7333\n",
            "[step 348] loss=3.8008\n",
            "[step 349] loss=3.5952\n",
            "[step 350] loss=3.6217\n",
            "[step 351] loss=3.6515\n",
            "[step 352] loss=3.6728\n",
            "[step 353] loss=3.7580\n",
            "[step 354] loss=3.7908\n",
            "[step 355] loss=3.7771\n",
            "[step 356] loss=3.8043\n",
            "[step 357] loss=3.9593\n",
            "[step 358] loss=3.7865\n",
            "[step 359] loss=3.5694\n",
            "[step 360] loss=3.6889\n",
            "[step 361] loss=3.9346\n",
            "[step 362] loss=3.8167\n",
            "[step 363] loss=3.6647\n",
            "[step 364] loss=3.8445\n",
            "[step 365] loss=3.7688\n",
            "[step 366] loss=3.9376\n",
            "[step 367] loss=3.5910\n",
            "[step 368] loss=3.8667\n",
            "[step 369] loss=3.6655\n",
            "[step 370] loss=3.5669\n",
            "[step 371] loss=3.6853\n",
            "[step 372] loss=3.9253\n",
            "[step 373] loss=3.7922\n",
            "[step 374] loss=3.8320\n",
            "[step 375] loss=3.7319\n",
            "[step 376] loss=3.6539\n",
            "[step 377] loss=3.8043\n",
            "[step 378] loss=3.6049\n",
            "[step 379] loss=3.6895\n",
            "[step 380] loss=3.3548\n",
            "[step 381] loss=3.5451\n",
            "[step 382] loss=3.8887\n",
            "[step 383] loss=4.0253\n",
            "[step 384] loss=3.9941\n",
            "[step 385] loss=3.7816\n",
            "[step 386] loss=3.6183\n",
            "[step 387] loss=3.7175\n",
            "[step 388] loss=3.7494\n",
            "[step 389] loss=3.5635\n",
            "[step 390] loss=3.8027\n",
            "[step 391] loss=3.7209\n",
            "[step 392] loss=3.8230\n",
            "[step 393] loss=3.6899\n",
            "[step 394] loss=3.7506\n",
            "[step 395] loss=3.9766\n",
            "[step 396] loss=3.7433\n",
            "[step 397] loss=3.7778\n",
            "[step 398] loss=3.8107\n",
            "[step 399] loss=3.5873\n",
            "[step 400] loss=3.8691\n",
            "[step 401] loss=3.6319\n",
            "[step 402] loss=3.6928\n",
            "[step 403] loss=3.8502\n",
            "[step 404] loss=3.7155\n",
            "[step 405] loss=3.7530\n",
            "[step 406] loss=3.8427\n",
            "[step 407] loss=3.8528\n",
            "[step 408] loss=3.6845\n",
            "[step 409] loss=3.7325\n",
            "[step 410] loss=3.8319\n",
            "[step 411] loss=3.6853\n",
            "[step 412] loss=3.8726\n",
            "[step 413] loss=3.8172\n",
            "[step 414] loss=3.8353\n",
            "[step 415] loss=3.7580\n",
            "[step 416] loss=3.9007\n",
            "[step 417] loss=4.0923\n",
            "[step 418] loss=3.7116\n",
            "[step 419] loss=3.8753\n",
            "[step 420] loss=3.8209\n",
            "[step 421] loss=3.8291\n",
            "[step 422] loss=3.5429\n",
            "[step 423] loss=3.7562\n",
            "[step 424] loss=3.6722\n",
            "[step 425] loss=3.4784\n",
            "[step 426] loss=3.8177\n",
            "[step 427] loss=3.8121\n",
            "[step 428] loss=3.9237\n",
            "[step 429] loss=3.8243\n",
            "[step 430] loss=3.6126\n",
            "[step 431] loss=3.7444\n",
            "[step 432] loss=3.6921\n",
            "[step 433] loss=3.3064\n",
            "[step 434] loss=3.7938\n",
            "[step 435] loss=3.6256\n",
            "[step 436] loss=3.7216\n",
            "[step 437] loss=3.5103\n",
            "[step 438] loss=3.7441\n",
            "[step 439] loss=3.6858\n",
            "[step 440] loss=3.5405\n",
            "[step 441] loss=3.6094\n",
            "[step 442] loss=3.8583\n",
            "[step 443] loss=3.6897\n",
            "[step 444] loss=3.7823\n",
            "[step 445] loss=3.7969\n",
            "[step 446] loss=3.7870\n",
            "[step 447] loss=3.7360\n",
            "[step 448] loss=4.1003\n",
            "[step 449] loss=3.7136\n",
            "[step 450] loss=3.6678\n",
            "[step 451] loss=3.6044\n",
            "[step 452] loss=3.7745\n",
            "[step 453] loss=3.6678\n",
            "[step 454] loss=3.7654\n",
            "[step 455] loss=3.3924\n",
            "[step 456] loss=3.0282\n",
            "[step 457] loss=3.6000\n",
            "[step 458] loss=3.6515\n",
            "[step 459] loss=3.9952\n",
            "[step 460] loss=3.5447\n",
            "[step 461] loss=3.8215\n",
            "[step 462] loss=3.6406\n",
            "[step 463] loss=3.8587\n",
            "[step 464] loss=3.8888\n",
            "[step 465] loss=3.6893\n",
            "[step 466] loss=3.5514\n",
            "[step 467] loss=3.6494\n",
            "[step 468] loss=3.9070\n",
            "[step 469] loss=3.7934\n",
            "[step 470] loss=3.8618\n",
            "[step 471] loss=3.7098\n",
            "[step 472] loss=4.0346\n",
            "[step 473] loss=3.6907\n",
            "[step 474] loss=3.5267\n",
            "[step 475] loss=3.6677\n",
            "[step 476] loss=3.6600\n",
            "[step 477] loss=3.7587\n",
            "[step 478] loss=3.6513\n",
            "[step 479] loss=3.5787\n",
            "[step 480] loss=3.7820\n",
            "[step 481] loss=3.9225\n",
            "[step 482] loss=3.6798\n",
            "[step 483] loss=3.7256\n",
            "[step 484] loss=3.7227\n",
            "[step 485] loss=3.8047\n",
            "[step 486] loss=3.8372\n",
            "[step 487] loss=3.6985\n",
            "[step 488] loss=3.9193\n",
            "[step 489] loss=3.6151\n",
            "[step 490] loss=3.7069\n",
            "[step 491] loss=3.5673\n",
            "[step 492] loss=3.7146\n",
            "[step 493] loss=3.8445\n",
            "[step 494] loss=3.6293\n",
            "[step 495] loss=3.6208\n",
            "[step 496] loss=3.6198\n",
            "[step 497] loss=3.7423\n",
            "[step 498] loss=3.7267\n",
            "[step 499] loss=3.6948\n",
            "[step 500] loss=4.0826\n",
            "[step 501] loss=3.6697\n",
            "[step 502] loss=3.7264\n",
            "[step 503] loss=3.8326\n",
            "[step 504] loss=3.7088\n",
            "[step 505] loss=3.7057\n",
            "[step 506] loss=3.7023\n",
            "[step 507] loss=3.6725\n",
            "[step 508] loss=3.7425\n",
            "[step 509] loss=3.7565\n",
            "[step 510] loss=3.6861\n",
            "[step 511] loss=3.8841\n",
            "[step 512] loss=3.6032\n",
            "[step 513] loss=3.6985\n",
            "[step 514] loss=3.9516\n",
            "[step 515] loss=3.6933\n",
            "[step 516] loss=3.7347\n",
            "[step 517] loss=3.7506\n",
            "[step 518] loss=3.6864\n",
            "[step 519] loss=3.6623\n",
            "[step 520] loss=3.8108\n",
            "[step 521] loss=4.0774\n",
            "[step 522] loss=3.9532\n",
            "[step 523] loss=3.8708\n",
            "[step 524] loss=3.9746\n",
            "[step 525] loss=3.3881\n",
            "[step 526] loss=3.6957\n",
            "[step 527] loss=3.7853\n",
            "[step 528] loss=4.0668\n",
            "[step 529] loss=3.5421\n",
            "[step 530] loss=3.6713\n",
            "[step 531] loss=3.6746\n",
            "[step 532] loss=3.8390\n",
            "[step 533] loss=3.6964\n",
            "[step 534] loss=3.7335\n",
            "[step 535] loss=3.7684\n",
            "[step 536] loss=3.7946\n",
            "[step 537] loss=3.6839\n",
            "[step 538] loss=3.8060\n",
            "[step 539] loss=3.8016\n",
            "[step 540] loss=3.7646\n",
            "[step 541] loss=3.7656\n",
            "[step 542] loss=3.7894\n",
            "[step 543] loss=3.8694\n",
            "[step 544] loss=3.6401\n",
            "[step 545] loss=3.9426\n",
            "[step 546] loss=3.8575\n",
            "[step 547] loss=3.6083\n",
            "[step 548] loss=3.7350\n",
            "[step 549] loss=3.6411\n",
            "[step 550] loss=3.7804\n",
            "[step 551] loss=3.8789\n",
            "[step 552] loss=3.8468\n",
            "[step 553] loss=3.7146\n",
            "[step 554] loss=3.5336\n",
            "[step 555] loss=3.6749\n",
            "[step 556] loss=3.7184\n",
            "[step 557] loss=3.5594\n",
            "[step 558] loss=3.7036\n",
            "[step 559] loss=3.6977\n",
            "[step 560] loss=3.7129\n",
            "[step 561] loss=3.7658\n",
            "[step 562] loss=3.8237\n",
            "[step 563] loss=3.7749\n",
            "[step 564] loss=3.7803\n",
            "[step 565] loss=3.8539\n",
            "[step 566] loss=3.5676\n",
            "[step 567] loss=3.5893\n",
            "[step 568] loss=3.7393\n",
            "[step 569] loss=3.8956\n",
            "[step 570] loss=3.8196\n",
            "[step 571] loss=3.8901\n",
            "[step 572] loss=3.6932\n",
            "[step 573] loss=3.7770\n",
            "[step 574] loss=3.6816\n",
            "[step 575] loss=3.7329\n",
            "[step 576] loss=3.8479\n",
            "[step 577] loss=3.7921\n",
            "[step 578] loss=3.8941\n",
            "[step 579] loss=3.7233\n",
            "[step 580] loss=3.5820\n",
            "[step 581] loss=3.8438\n",
            "[step 582] loss=3.9909\n",
            "[step 583] loss=3.7117\n",
            "[step 584] loss=3.5573\n",
            "[step 585] loss=3.7072\n",
            "[step 586] loss=3.6953\n",
            "[step 587] loss=3.5743\n",
            "[step 588] loss=3.8612\n",
            "[step 589] loss=3.7217\n",
            "[step 590] loss=3.8127\n",
            "[step 591] loss=3.6102\n",
            "[step 592] loss=3.7167\n",
            "[step 593] loss=3.7898\n",
            "[step 594] loss=3.6187\n",
            "[step 595] loss=3.6451\n",
            "[step 596] loss=3.7026\n",
            "[step 597] loss=3.4979\n",
            "[step 598] loss=3.4984\n",
            "[step 599] loss=3.8691\n",
            "[step 600] loss=4.0016\n",
            "[step 601] loss=3.6879\n",
            "[step 602] loss=3.7351\n",
            "[step 603] loss=3.7580\n",
            "[step 604] loss=3.7959\n",
            "[step 605] loss=3.4967\n",
            "[step 606] loss=3.8524\n",
            "[step 607] loss=3.9771\n",
            "[step 608] loss=3.6430\n",
            "[step 609] loss=3.8431\n",
            "[step 610] loss=3.9420\n",
            "[step 611] loss=3.6911\n",
            "[step 612] loss=3.4646\n",
            "[step 613] loss=3.8708\n",
            "[step 614] loss=3.6763\n",
            "[step 615] loss=3.8201\n",
            "[step 616] loss=3.7454\n",
            "[step 617] loss=3.6950\n",
            "[step 618] loss=3.7973\n",
            "[step 619] loss=3.7401\n",
            "[step 620] loss=3.6614\n",
            "[step 621] loss=3.9847\n",
            "[step 622] loss=3.7273\n",
            "[step 623] loss=3.6280\n",
            "[step 624] loss=3.9112\n",
            "[step 625] loss=3.7356\n",
            "[step 626] loss=3.7031\n",
            "[step 627] loss=3.8251\n",
            "[step 628] loss=3.8840\n",
            "[step 629] loss=3.7875\n",
            "[step 630] loss=3.4174\n",
            "[step 631] loss=3.5682\n",
            "[step 632] loss=3.5694\n",
            "[step 633] loss=3.5539\n",
            "[step 634] loss=3.7398\n",
            "[step 635] loss=3.4284\n",
            "[step 636] loss=3.8571\n",
            "[step 637] loss=3.6005\n",
            "[step 638] loss=3.7569\n",
            "[step 639] loss=3.7247\n",
            "[step 640] loss=3.9280\n",
            "[step 641] loss=3.4489\n",
            "[step 642] loss=3.6550\n",
            "[step 643] loss=3.5323\n",
            "[step 644] loss=3.6225\n",
            "[step 645] loss=3.8479\n",
            "[step 646] loss=3.5310\n",
            "[step 647] loss=3.6340\n",
            "[step 648] loss=3.7006\n",
            "[step 649] loss=3.3362\n",
            "[step 650] loss=3.6053\n",
            "[step 651] loss=3.7486\n",
            "[step 652] loss=3.6732\n",
            "[step 653] loss=3.8161\n",
            "[step 654] loss=3.8540\n",
            "[step 655] loss=3.7810\n",
            "[step 656] loss=3.7925\n",
            "[step 657] loss=3.5451\n",
            "[step 658] loss=3.7644\n",
            "[step 659] loss=3.7210\n",
            "[step 660] loss=3.7866\n",
            "[step 661] loss=3.6332\n",
            "[step 662] loss=3.8079\n",
            "[step 663] loss=3.6488\n",
            "[step 664] loss=3.8668\n",
            "[step 665] loss=3.8593\n",
            "[step 666] loss=3.7117\n",
            "[step 667] loss=3.9793\n",
            "[step 668] loss=3.7557\n",
            "[step 669] loss=3.6258\n",
            "[step 670] loss=3.7808\n",
            "[step 671] loss=3.6637\n",
            "[step 672] loss=3.7343\n",
            "[step 673] loss=3.7081\n",
            "[step 674] loss=3.8372\n",
            "[step 675] loss=3.9381\n",
            "[step 676] loss=3.6858\n",
            "[step 677] loss=3.6894\n",
            "[step 678] loss=3.8236\n",
            "[step 679] loss=3.6456\n",
            "[step 680] loss=3.8618\n",
            "[step 681] loss=3.7241\n",
            "[step 682] loss=3.6677\n",
            "[step 683] loss=3.8732\n",
            "[step 684] loss=3.5831\n",
            "[step 685] loss=3.9348\n",
            "[step 686] loss=3.8429\n",
            "[step 687] loss=3.7186\n",
            "[step 688] loss=3.7614\n",
            "[step 689] loss=3.6543\n",
            "[step 690] loss=3.6156\n",
            "[step 691] loss=3.7259\n",
            "[step 692] loss=3.9189\n",
            "[step 693] loss=3.8778\n",
            "[step 694] loss=3.9649\n",
            "[step 695] loss=3.8620\n",
            "[step 696] loss=3.8002\n",
            "[step 697] loss=3.6282\n",
            "[step 698] loss=3.8178\n",
            "[step 699] loss=3.6347\n",
            "[step 700] loss=4.0189\n",
            "[step 701] loss=3.9258\n",
            "[step 702] loss=3.7624\n",
            "[step 703] loss=3.6022\n",
            "[step 704] loss=3.8178\n",
            "[step 705] loss=3.8756\n",
            "[step 706] loss=3.6239\n",
            "[step 707] loss=4.0143\n",
            "[step 708] loss=3.8470\n",
            "[step 709] loss=3.7401\n",
            "[step 710] loss=3.8122\n",
            "[step 711] loss=3.7685\n",
            "[step 712] loss=4.0226\n",
            "[step 713] loss=3.5637\n",
            "[step 714] loss=3.8474\n",
            "[step 715] loss=3.8841\n",
            "[step 716] loss=3.8216\n",
            "[step 717] loss=3.7607\n",
            "[step 718] loss=4.0006\n",
            "[step 719] loss=3.7562\n",
            "[step 720] loss=3.7234\n",
            "[step 721] loss=3.8067\n",
            "[step 722] loss=3.7075\n",
            "[step 723] loss=3.6291\n",
            "[step 724] loss=3.6232\n",
            "[step 725] loss=3.8065\n",
            "[step 726] loss=3.7873\n",
            "[step 727] loss=3.8966\n",
            "[step 728] loss=3.7519\n",
            "[step 729] loss=3.7835\n",
            "[step 730] loss=3.7933\n",
            "[step 731] loss=3.7830\n",
            "[step 732] loss=3.7308\n",
            "[step 733] loss=3.7572\n",
            "[step 734] loss=3.7737\n",
            "[step 735] loss=3.9865\n",
            "[step 736] loss=4.0064\n",
            "[step 737] loss=3.8254\n",
            "[step 738] loss=3.7112\n",
            "[step 739] loss=3.6580\n",
            "[step 740] loss=3.7708\n",
            "[step 741] loss=3.9356\n",
            "[step 742] loss=3.7080\n",
            "[step 743] loss=3.8447\n",
            "[step 744] loss=3.7762\n",
            "[step 745] loss=3.7070\n",
            "[step 746] loss=3.6597\n",
            "[step 747] loss=3.7399\n",
            "[step 748] loss=3.6045\n",
            "[step 749] loss=4.0422\n",
            "[step 750] loss=3.7373\n",
            "[step 751] loss=3.7723\n",
            "[step 752] loss=4.0183\n",
            "[step 753] loss=3.6666\n",
            "[step 754] loss=3.6451\n",
            "[step 755] loss=3.6279\n",
            "[step 756] loss=3.8445\n",
            "[step 757] loss=3.7101\n",
            "[step 758] loss=3.7666\n",
            "[step 759] loss=3.6428\n",
            "[step 760] loss=3.7252\n",
            "[step 761] loss=3.8403\n",
            "[step 762] loss=3.6977\n",
            "[step 763] loss=3.5955\n",
            "[step 764] loss=3.8224\n",
            "[step 765] loss=3.7284\n",
            "[step 766] loss=3.8011\n",
            "[step 767] loss=3.7175\n",
            "[step 768] loss=3.6224\n",
            "[step 769] loss=3.8092\n",
            "[step 770] loss=3.8322\n",
            "[step 771] loss=3.7149\n",
            "[step 772] loss=3.6811\n",
            "[step 773] loss=3.6010\n",
            "[step 774] loss=3.7971\n",
            "[step 775] loss=3.7334\n",
            "[step 776] loss=3.5551\n",
            "[step 777] loss=3.6919\n",
            "[step 778] loss=3.9048\n",
            "[step 779] loss=3.4722\n",
            "[step 780] loss=3.7286\n",
            "[step 781] loss=3.6789\n",
            "[step 782] loss=3.8358\n",
            "[step 783] loss=3.7294\n",
            "[step 784] loss=3.7670\n",
            "[step 785] loss=3.4801\n",
            "[step 786] loss=3.8093\n",
            "[step 787] loss=3.7374\n",
            "[step 788] loss=3.7734\n",
            "[step 789] loss=3.7254\n",
            "[step 790] loss=3.6986\n",
            "[step 791] loss=3.7789\n",
            "[step 792] loss=3.5425\n",
            "[step 793] loss=3.9328\n",
            "[step 794] loss=3.6227\n",
            "[step 795] loss=3.8953\n",
            "[step 796] loss=3.6429\n",
            "[step 797] loss=3.5729\n",
            "[step 798] loss=3.7704\n",
            "[step 799] loss=3.7176\n",
            "[step 800] loss=3.7628\n",
            "[step 801] loss=3.7869\n",
            "[step 802] loss=3.6523\n",
            "[step 803] loss=3.7255\n",
            "[step 804] loss=3.8918\n",
            "[step 805] loss=3.7240\n",
            "[step 806] loss=3.7684\n",
            "[step 807] loss=3.3187\n",
            "[step 808] loss=3.6041\n",
            "[step 809] loss=3.8131\n",
            "[step 810] loss=3.6966\n",
            "[step 811] loss=3.8965\n",
            "[step 812] loss=3.8620\n",
            "[step 813] loss=3.5404\n",
            "[step 814] loss=3.7219\n",
            "[step 815] loss=3.5692\n",
            "[step 816] loss=3.7662\n",
            "[step 817] loss=3.7661\n",
            "[step 818] loss=3.7525\n",
            "[step 819] loss=4.0831\n",
            "[step 820] loss=3.7559\n",
            "[step 821] loss=3.7817\n",
            "[step 822] loss=3.6987\n",
            "[step 823] loss=3.4503\n",
            "[step 824] loss=3.7784\n",
            "[step 825] loss=3.6301\n",
            "[step 826] loss=3.6744\n",
            "[step 827] loss=3.8408\n",
            "[step 828] loss=3.9771\n",
            "[step 829] loss=4.0593\n",
            "[step 830] loss=3.7236\n",
            "[step 831] loss=3.7001\n",
            "[step 832] loss=3.6725\n",
            "[step 833] loss=3.6036\n",
            "[step 834] loss=3.7510\n",
            "[step 835] loss=3.6889\n",
            "[step 836] loss=3.8965\n",
            "[step 837] loss=3.8315\n",
            "[step 838] loss=3.6704\n",
            "[step 839] loss=3.9014\n",
            "[step 840] loss=3.8631\n",
            "[step 841] loss=3.9819\n",
            "[step 842] loss=3.4887\n",
            "[step 843] loss=3.5740\n",
            "[step 844] loss=3.4984\n",
            "[step 845] loss=3.7446\n",
            "[step 846] loss=3.8417\n",
            "[step 847] loss=3.5010\n",
            "[step 848] loss=3.6248\n",
            "[step 849] loss=3.8503\n",
            "[step 850] loss=3.7597\n",
            "[step 851] loss=3.6739\n",
            "[step 852] loss=3.8395\n",
            "[step 853] loss=3.7465\n",
            "[step 854] loss=3.7645\n",
            "[step 855] loss=3.7634\n",
            "[step 856] loss=3.8653\n",
            "[step 857] loss=3.7566\n",
            "[step 858] loss=3.8162\n",
            "[step 859] loss=3.7262\n",
            "[step 860] loss=3.8203\n",
            "[step 861] loss=3.5293\n",
            "[step 862] loss=3.3894\n",
            "[step 863] loss=3.7435\n",
            "[step 864] loss=3.8284\n",
            "[step 865] loss=3.7386\n",
            "[step 866] loss=3.6100\n",
            "[step 867] loss=3.5171\n",
            "[step 868] loss=3.8529\n",
            "[step 869] loss=3.7952\n",
            "[step 870] loss=3.7629\n",
            "[step 871] loss=3.7570\n",
            "[step 872] loss=3.7572\n",
            "[step 873] loss=3.6053\n",
            "[step 874] loss=3.9370\n",
            "[step 875] loss=3.8078\n",
            "[step 876] loss=3.8122\n",
            "[step 877] loss=3.7894\n",
            "[step 878] loss=3.7386\n",
            "[step 879] loss=3.6684\n",
            "[step 880] loss=3.7396\n",
            "[step 881] loss=3.6588\n",
            "[step 882] loss=3.8619\n",
            "[step 883] loss=3.7662\n",
            "[step 884] loss=3.8930\n",
            "[step 885] loss=3.7878\n",
            "[step 886] loss=3.7660\n",
            "[step 887] loss=3.8629\n",
            "[step 888] loss=3.8212\n",
            "[step 889] loss=3.6212\n",
            "[step 890] loss=3.6135\n",
            "[step 891] loss=3.7791\n",
            "[step 892] loss=3.5520\n",
            "[step 893] loss=3.7669\n",
            "[step 894] loss=3.8645\n",
            "[step 895] loss=3.8943\n",
            "[step 896] loss=3.5553\n",
            "[step 897] loss=3.8895\n",
            "[step 898] loss=3.8551\n",
            "[step 899] loss=3.8400\n",
            "[step 900] loss=3.5850\n",
            "[step 901] loss=3.8538\n",
            "[step 902] loss=3.9834\n",
            "[step 903] loss=3.9544\n",
            "[step 904] loss=3.9405\n",
            "[step 905] loss=4.0929\n",
            "[step 906] loss=3.9573\n",
            "[step 907] loss=3.8058\n",
            "[step 908] loss=3.9323\n",
            "[step 909] loss=3.5933\n",
            "[step 910] loss=3.8242\n",
            "[step 911] loss=3.6737\n",
            "[step 912] loss=3.7881\n",
            "[step 913] loss=3.6291\n",
            "[step 914] loss=3.6023\n",
            "[step 915] loss=3.8040\n",
            "[step 916] loss=3.8988\n",
            "[step 917] loss=3.4943\n",
            "[step 918] loss=3.5699\n",
            "[step 919] loss=3.7622\n",
            "[step 920] loss=3.7142\n",
            "[step 921] loss=3.8128\n",
            "[step 922] loss=3.7369\n",
            "[step 923] loss=3.7723\n",
            "[step 924] loss=3.8130\n",
            "[step 925] loss=3.5337\n",
            "[step 926] loss=3.7121\n",
            "[step 927] loss=3.9447\n",
            "[step 928] loss=3.8025\n",
            "[step 929] loss=3.7795\n",
            "[step 930] loss=3.8994\n",
            "[step 931] loss=3.7885\n",
            "[step 932] loss=3.7433\n",
            "[step 933] loss=3.7572\n",
            "[step 934] loss=3.5942\n",
            "[step 935] loss=3.8359\n",
            "[step 936] loss=3.7461\n",
            "[step 937] loss=3.7838\n",
            "[step 938] loss=3.6630\n",
            "[step 939] loss=3.6594\n",
            "[step 940] loss=3.7947\n",
            "[step 941] loss=3.5561\n",
            "[step 942] loss=3.7912\n",
            "[step 943] loss=3.8990\n",
            "[step 944] loss=3.6645\n",
            "[step 945] loss=3.8479\n",
            "[step 946] loss=3.8800\n",
            "[step 947] loss=3.6125\n",
            "[step 948] loss=3.4739\n",
            "[step 949] loss=3.8613\n",
            "[step 950] loss=3.6736\n",
            "[step 951] loss=4.1342\n",
            "[step 952] loss=3.8565\n",
            "[step 953] loss=3.7546\n",
            "[step 954] loss=3.7995\n",
            "[step 955] loss=3.7664\n",
            "[step 956] loss=3.5650\n",
            "[step 957] loss=3.5820\n",
            "[step 958] loss=3.6374\n",
            "[step 959] loss=3.7099\n",
            "[step 960] loss=3.7660\n",
            "[step 961] loss=3.6802\n",
            "[step 962] loss=3.7257\n",
            "[step 963] loss=3.9173\n",
            "[step 964] loss=3.6902\n",
            "[step 965] loss=3.6759\n",
            "[step 966] loss=3.7801\n",
            "[step 967] loss=3.7326\n",
            "[step 968] loss=3.8266\n",
            "[step 969] loss=3.6700\n",
            "[step 970] loss=3.6842\n",
            "[step 971] loss=3.6283\n",
            "[step 972] loss=3.4012\n",
            "[step 973] loss=3.7078\n",
            "[step 974] loss=3.7889\n",
            "[step 975] loss=3.6999\n",
            "[step 976] loss=3.5058\n",
            "[step 977] loss=3.7007\n",
            "[step 978] loss=3.9913\n",
            "[step 979] loss=3.6082\n",
            "[step 980] loss=3.7600\n",
            "[step 981] loss=3.7248\n",
            "[step 982] loss=3.6665\n",
            "[step 983] loss=3.7707\n",
            "[step 984] loss=3.4047\n",
            "[step 985] loss=3.8162\n",
            "[step 986] loss=3.5264\n",
            "[step 987] loss=3.7478\n",
            "[step 988] loss=3.6394\n",
            "[step 989] loss=3.6393\n",
            "[step 990] loss=3.6900\n",
            "[step 991] loss=3.8890\n",
            "[step 992] loss=3.9403\n",
            "[step 993] loss=3.7390\n",
            "[step 994] loss=3.6554\n",
            "[step 995] loss=3.8726\n",
            "[step 996] loss=3.6368\n",
            "[step 997] loss=3.7859\n",
            "[step 998] loss=3.6682\n",
            "[step 999] loss=3.3676\n",
            "[step 1000] loss=3.6046\n",
            "[step 1001] loss=3.7145\n",
            "[step 1002] loss=3.7121\n",
            "[step 1003] loss=3.6461\n",
            "[step 1004] loss=3.5337\n",
            "[step 1005] loss=3.7350\n",
            "[step 1006] loss=3.8206\n",
            "[step 1007] loss=3.6831\n",
            "[step 1008] loss=3.6364\n",
            "[step 1009] loss=3.6228\n",
            "[step 1010] loss=3.5633\n",
            "[step 1011] loss=3.6398\n",
            "[step 1012] loss=4.0146\n",
            "[step 1013] loss=3.6981\n",
            "[step 1014] loss=3.8094\n",
            "[step 1015] loss=3.7680\n",
            "[step 1016] loss=3.5622\n",
            "[step 1017] loss=3.9367\n",
            "[step 1018] loss=3.6461\n",
            "[step 1019] loss=3.6300\n",
            "[step 1020] loss=3.6160\n",
            "[step 1021] loss=3.6813\n",
            "[step 1022] loss=4.0637\n",
            "[step 1023] loss=3.8252\n",
            "[step 1024] loss=3.6425\n",
            "[step 1025] loss=3.6791\n",
            "[step 1026] loss=3.7023\n",
            "[step 1027] loss=3.6541\n",
            "[step 1028] loss=3.4086\n",
            "[step 1029] loss=3.7458\n",
            "[step 1030] loss=3.8252\n",
            "[step 1031] loss=3.7605\n",
            "[step 1032] loss=3.7070\n",
            "[step 1033] loss=3.7674\n",
            "[step 1034] loss=3.6865\n",
            "[step 1035] loss=3.8152\n",
            "[step 1036] loss=3.5968\n",
            "[step 1037] loss=3.6681\n",
            "[step 1038] loss=3.7689\n",
            "[step 1039] loss=3.6412\n",
            "[step 1040] loss=3.6321\n",
            "[step 1041] loss=3.4010\n",
            "[step 1042] loss=3.6770\n",
            "[step 1043] loss=3.6501\n",
            "[step 1044] loss=3.7683\n",
            "[step 1045] loss=3.8027\n",
            "[step 1046] loss=3.9113\n",
            "[step 1047] loss=3.8689\n",
            "[step 1048] loss=3.9759\n",
            "[step 1049] loss=3.5713\n",
            "[step 1050] loss=3.7045\n",
            "[step 1051] loss=3.5634\n",
            "[step 1052] loss=3.7180\n",
            "[step 1053] loss=3.5276\n",
            "[step 1054] loss=3.5944\n",
            "[step 1055] loss=3.8044\n",
            "[step 1056] loss=3.5949\n",
            "[step 1057] loss=4.0186\n",
            "[step 1058] loss=3.6303\n",
            "[step 1059] loss=3.6266\n",
            "[step 1060] loss=3.6345\n",
            "[step 1061] loss=3.9134\n",
            "[step 1062] loss=3.7361\n",
            "[step 1063] loss=3.5755\n",
            "[step 1064] loss=3.7164\n",
            "[step 1065] loss=3.6640\n",
            "[step 1066] loss=3.7042\n",
            "[step 1067] loss=3.8602\n",
            "[step 1068] loss=3.7794\n",
            "[step 1069] loss=3.7965\n",
            "[step 1070] loss=3.6999\n",
            "[step 1071] loss=3.6998\n",
            "[step 1072] loss=3.8640\n",
            "[step 1073] loss=3.9574\n",
            "[step 1074] loss=3.7391\n",
            "[step 1075] loss=3.8373\n",
            "[step 1076] loss=3.7280\n",
            "[step 1077] loss=3.7057\n",
            "[step 1078] loss=3.6434\n",
            "[step 1079] loss=3.8939\n",
            "[step 1080] loss=3.8147\n",
            "[step 1081] loss=3.6018\n",
            "[step 1082] loss=3.4259\n",
            "[step 1083] loss=3.8793\n",
            "[step 1084] loss=3.7567\n",
            "[step 1085] loss=3.7711\n",
            "[step 1086] loss=3.7575\n",
            "[step 1087] loss=3.6461\n",
            "[step 1088] loss=3.7051\n",
            "[step 1089] loss=3.5737\n",
            "[step 1090] loss=3.7349\n",
            "[step 1091] loss=3.7408\n",
            "[step 1092] loss=3.7478\n",
            "[step 1093] loss=3.7868\n",
            "[step 1094] loss=3.7544\n",
            "[step 1095] loss=3.7838\n",
            "[step 1096] loss=3.3920\n",
            "[step 1097] loss=3.7116\n",
            "[step 1098] loss=3.6917\n",
            "[step 1099] loss=3.8287\n",
            "[step 1100] loss=3.6676\n",
            "Reached MAX_STEPS â€” stopping scan.\n",
            "Gradient scan complete.\n",
            "\n",
            "Selecting global Top-250 by |grad|...\n",
            "\n",
            "=== Global Top-250 most sensitive parameters ===\n",
            "  global #  1: transformer.h.10.ln_2.weight(373)  |grad|=2.425e+01\n",
            "  global #  2: transformer.h.10.ln_2.weight(481)  |grad|=1.883e+01\n",
            "  global #  3: transformer.h.11.ln_2.weight(447)  |grad|=8.055e+00\n",
            "  global #  4: transformer.h.11.ln_2.weight(481)  |grad|=7.594e+00\n",
            "  global #  5: transformer.h.10.ln_2.weight(447)  |grad|=7.457e+00\n",
            "  global #  6: transformer.h.9.ln_2.weight(447)  |grad|=6.090e+00\n",
            "  global #  7: transformer.h.4.ln_2.weight(64)  |grad|=5.500e+00\n",
            "  global #  8: transformer.h.11.ln_2.weight(496)  |grad|=5.441e+00\n",
            "  global #  9: transformer.h.11.ln_1.weight(496)  |grad|=5.289e+00\n",
            "  global # 10: transformer.wte.weight(2488, 496)  |grad|=5.098e+00\n",
            "  global # 11: transformer.h.9.ln_2.weight(481)  |grad|=4.582e+00\n",
            "  global # 12: transformer.wte.weight(837, 496)  |grad|=4.465e+00\n",
            "  global # 13: transformer.h.11.ln_1.weight(447)  |grad|=4.137e+00\n",
            "  global # 14: transformer.h.9.ln_2.weight(373)  |grad|=3.842e+00\n",
            "  global # 15: transformer.h.11.ln_2.weight(373)  |grad|=3.807e+00\n",
            "  global # 16: transformer.h.7.ln_1.weight(447)  |grad|=3.686e+00\n",
            "  global # 17: transformer.h.6.ln_2.weight(447)  |grad|=3.674e+00\n",
            "  global # 18: transformer.h.4.ln_2.weight(373)  |grad|=3.510e+00\n",
            "  global # 19: transformer.h.5.ln_2.weight(447)  |grad|=3.242e+00\n",
            "  global # 20: transformer.h.5.ln_1.weight(373)  |grad|=3.074e+00\n",
            "  global # 21: transformer.wte.weight(198, 496)  |grad|=3.021e+00\n",
            "  global # 22: transformer.h.5.ln_1.weight(447)  |grad|=2.990e+00\n",
            "  global # 23: transformer.wte.weight(11, 496)  |grad|=2.877e+00\n",
            "  global # 24: transformer.wte.weight(34315, 496)  |grad|=2.828e+00\n",
            "  global # 25: transformer.h.9.ln_1.weight(447)  |grad|=2.670e+00\n",
            "  global # 26: transformer.h.10.ln_1.weight(447)  |grad|=2.637e+00\n",
            "  global # 27: transformer.h.4.ln_2.weight(326)  |grad|=2.633e+00\n",
            "  global # 28: transformer.h.3.ln_2.weight(447)  |grad|=2.590e+00\n",
            "  global # 29: transformer.wte.weight(796, 496)  |grad|=2.584e+00\n",
            "  global # 30: transformer.wte.weight(220, 496)  |grad|=2.445e+00\n",
            "  global # 31: transformer.h.4.ln_2.weight(447)  |grad|=2.316e+00\n",
            "  global # 32: transformer.wte.weight(13, 496)  |grad|=2.314e+00\n",
            "  global # 33: transformer.wte.weight(764, 496)  |grad|=2.307e+00\n",
            "  global # 34: transformer.h.0.ln_1.weight(64)  |grad|=2.254e+00\n",
            "  global # 35: transformer.h.1.ln_2.weight(373)  |grad|=2.178e+00\n",
            "  global # 36: transformer.wte.weight(49063, 496)  |grad|=2.176e+00\n",
            "  global # 37: transformer.h.11.ln_2.weight(266)  |grad|=2.162e+00\n",
            "  global # 38: transformer.h.8.ln_2.weight(64)  |grad|=2.145e+00\n",
            "  global # 39: transformer.wte.weight(2488, 430)  |grad|=2.129e+00\n",
            "  global # 40: transformer.wte.weight(31, 496)  |grad|=2.107e+00\n",
            "  global # 41: transformer.h.0.ln_1.weight(87)  |grad|=2.100e+00\n",
            "  global # 42: transformer.wte.weight(198, 430)  |grad|=2.068e+00\n",
            "  global # 43: transformer.h.1.ln_2.weight(266)  |grad|=2.055e+00\n",
            "  global # 44: transformer.h.0.ln_2.weight(373)  |grad|=2.037e+00\n",
            "  global # 45: transformer.h.5.ln_1.weight(326)  |grad|=2.029e+00\n",
            "  global # 46: transformer.wte.weight(5187, 496)  |grad|=2.027e+00\n",
            "  global # 47: transformer.h.11.ln_2.weight(64)  |grad|=2.023e+00\n",
            "  global # 48: transformer.wte.weight(27583, 496)  |grad|=1.995e+00\n",
            "  global # 49: transformer.wte.weight(837, 430)  |grad|=1.986e+00\n",
            "  global # 50: transformer.wte.weight(6645, 496)  |grad|=1.980e+00\n",
            "  global # 51: transformer.h.11.ln_1.weight(36)  |grad|=1.973e+00\n",
            "  global # 52: transformer.h.1.ln_2.weight(480)  |grad|=1.952e+00\n",
            "  global # 53: transformer.h.0.ln_2.weight(288)  |grad|=1.949e+00\n",
            "  global # 54: transformer.h.0.ln_1.weight(481)  |grad|=1.939e+00\n",
            "  global # 55: transformer.h.1.ln_2.weight(64)  |grad|=1.929e+00\n",
            "  global # 56: transformer.h.10.ln_2.bias(373)  |grad|=1.889e+00\n",
            "  global # 57: transformer.h.7.ln_2.weight(447)  |grad|=1.859e+00\n",
            "  global # 58: transformer.h.11.ln_1.weight(430)  |grad|=1.837e+00\n",
            "  global # 59: transformer.h.11.ln_2.weight(480)  |grad|=1.836e+00\n",
            "  global # 60: transformer.h.0.ln_1.weight(373)  |grad|=1.833e+00\n",
            "  global # 61: transformer.h.11.ln_1.weight(481)  |grad|=1.826e+00\n",
            "  global # 62: transformer.h.0.ln_1.weight(266)  |grad|=1.815e+00\n",
            "  global # 63: transformer.h.6.ln_2.weight(64)  |grad|=1.810e+00\n",
            "  global # 64: transformer.wte.weight(21165, 496)  |grad|=1.791e+00\n",
            "  global # 65: transformer.h.2.ln_1.weight(447)  |grad|=1.776e+00\n",
            "  global # 66: transformer.h.1.ln_2.weight(87)  |grad|=1.770e+00\n",
            "  global # 67: transformer.h.1.ln_1.weight(326)  |grad|=1.728e+00\n",
            "  global # 68: transformer.wte.weight(21371, 496)  |grad|=1.720e+00\n",
            "  global # 69: transformer.h.11.ln_1.bias(138)  |grad|=1.712e+00\n",
            "  global # 70: transformer.h.8.ln_2.weight(447)  |grad|=1.711e+00\n",
            "  global # 71: transformer.h.6.ln_1.weight(447)  |grad|=1.707e+00\n",
            "  global # 72: transformer.wte.weight(3056, 496)  |grad|=1.673e+00\n",
            "  global # 73: transformer.wte.weight(15281, 496)  |grad|=1.669e+00\n",
            "  global # 74: transformer.wte.weight(12164, 496)  |grad|=1.638e+00\n",
            "  global # 75: transformer.wte.weight(497, 496)  |grad|=1.636e+00\n",
            "  global # 76: transformer.h.2.ln_2.weight(64)  |grad|=1.625e+00\n",
            "  global # 77: transformer.wte.weight(198, 36)  |grad|=1.616e+00\n",
            "  global # 78: transformer.wte.weight(8670, 496)  |grad|=1.612e+00\n",
            "  global # 79: transformer.h.4.ln_2.weight(266)  |grad|=1.604e+00\n",
            "  global # 80: transformer.wte.weight(5178, 496)  |grad|=1.589e+00\n",
            "  global # 81: transformer.wte.weight(262, 496)  |grad|=1.589e+00\n",
            "  global # 82: transformer.h.1.ln_2.weight(447)  |grad|=1.586e+00\n",
            "  global # 83: transformer.wte.weight(40902, 496)  |grad|=1.578e+00\n",
            "  global # 84: transformer.wte.weight(2488, 36)  |grad|=1.563e+00\n",
            "  global # 85: transformer.h.6.ln_2.weight(373)  |grad|=1.559e+00\n",
            "  global # 86: transformer.wte.weight(8942, 496)  |grad|=1.557e+00\n",
            "  global # 87: transformer.wte.weight(837, 36)  |grad|=1.544e+00\n",
            "  global # 88: transformer.h.0.ln_1.weight(480)  |grad|=1.541e+00\n",
            "  global # 89: transformer.h.0.ln_1.weight(447)  |grad|=1.528e+00\n",
            "  global # 90: transformer.wte.weight(449, 496)  |grad|=1.522e+00\n",
            "  global # 91: transformer.wte.weight(3999, 496)  |grad|=1.519e+00\n",
            "  global # 92: transformer.h.11.ln_1.bias(160)  |grad|=1.514e+00\n",
            "  global # 93: transformer.h.8.ln_2.weight(373)  |grad|=1.511e+00\n",
            "  global # 94: transformer.h.0.ln_2.weight(756)  |grad|=1.506e+00\n",
            "  global # 95: transformer.h.5.ln_1.weight(393)  |grad|=1.501e+00\n",
            "  global # 96: transformer.h.10.ln_2.bias(481)  |grad|=1.477e+00\n",
            "  global # 97: transformer.h.4.ln_2.weight(393)  |grad|=1.475e+00\n",
            "  global # 98: transformer.h.1.ln_1.weight(87)  |grad|=1.462e+00\n",
            "  global # 99: transformer.h.7.ln_2.weight(481)  |grad|=1.446e+00\n",
            "  global #100: transformer.wte.weight(15767, 496)  |grad|=1.442e+00\n",
            "  global #101: transformer.h.11.ln_2.weight(87)  |grad|=1.442e+00\n",
            "  global #102: transformer.wte.weight(4544, 496)  |grad|=1.429e+00\n",
            "  global #103: transformer.h.0.ln_2.weight(64)  |grad|=1.427e+00\n",
            "  global #104: transformer.h.11.ln_1.bias(142)  |grad|=1.411e+00\n",
            "  global #105: transformer.h.5.ln_1.weight(64)  |grad|=1.398e+00\n",
            "  global #106: transformer.wte.weight(564, 496)  |grad|=1.396e+00\n",
            "  global #107: transformer.wte.weight(8118, 496)  |grad|=1.389e+00\n",
            "  global #108: transformer.h.2.ln_2.weight(266)  |grad|=1.388e+00\n",
            "  global #109: transformer.h.1.ln_1.weight(447)  |grad|=1.379e+00\n",
            "  global #110: transformer.wte.weight(27512, 496)  |grad|=1.371e+00\n",
            "  global #111: transformer.wte.weight(796, 430)  |grad|=1.358e+00\n",
            "  global #112: transformer.wte.weight(29285, 496)  |grad|=1.354e+00\n",
            "  global #113: transformer.wte.weight(18884, 496)  |grad|=1.353e+00\n",
            "  global #114: transformer.wte.weight(286, 496)  |grad|=1.350e+00\n",
            "  global #115: transformer.wte.weight(23214, 496)  |grad|=1.348e+00\n",
            "  global #116: transformer.wte.weight(26451, 496)  |grad|=1.344e+00\n",
            "  global #117: transformer.h.5.mlp.c_fc.weight(393, 1866)  |grad|=1.341e+00\n",
            "  global #118: transformer.wte.weight(44665, 496)  |grad|=1.328e+00\n",
            "  global #119: transformer.h.10.ln_2.weight(64)  |grad|=1.324e+00\n",
            "  global #120: transformer.wte.weight(45674, 496)  |grad|=1.323e+00\n",
            "  global #121: transformer.wte.weight(34315, 430)  |grad|=1.323e+00\n",
            "  global #122: transformer.wte.weight(37470, 496)  |grad|=1.322e+00\n",
            "  global #123: transformer.h.5.mlp.c_fc.weight(373, 1866)  |grad|=1.319e+00\n",
            "  global #124: transformer.h.11.ln_1.weight(373)  |grad|=1.317e+00\n",
            "  global #125: transformer.wte.weight(10278, 496)  |grad|=1.313e+00\n",
            "  global #126: transformer.wte.weight(366, 496)  |grad|=1.312e+00\n",
            "  global #127: transformer.wte.weight(28972, 496)  |grad|=1.308e+00\n",
            "  global #128: transformer.wte.weight(12108, 496)  |grad|=1.306e+00\n",
            "  global #129: transformer.h.8.ln_1.weight(447)  |grad|=1.303e+00\n",
            "  global #130: transformer.wte.weight(10849, 496)  |grad|=1.302e+00\n",
            "  global #131: transformer.wte.weight(39452, 496)  |grad|=1.301e+00\n",
            "  global #132: transformer.wte.weight(19498, 496)  |grad|=1.300e+00\n",
            "  global #133: transformer.wte.weight(11565, 496)  |grad|=1.298e+00\n",
            "  global #134: transformer.h.5.mlp.c_fc.weight(64, 1866)  |grad|=1.298e+00\n",
            "  global #135: transformer.wte.weight(39261, 496)  |grad|=1.295e+00\n",
            "  global #136: transformer.wte.weight(4960, 496)  |grad|=1.282e+00\n",
            "  global #137: transformer.h.11.ln_1.weight(326)  |grad|=1.281e+00\n",
            "  global #138: transformer.wte.weight(11, 430)  |grad|=1.276e+00\n",
            "  global #139: transformer.h.8.ln_2.weight(481)  |grad|=1.260e+00\n",
            "  global #140: transformer.wte.weight(15518, 496)  |grad|=1.256e+00\n",
            "  global #141: transformer.h.11.ln_1.weight(314)  |grad|=1.235e+00\n",
            "  global #142: transformer.wte.weight(3248, 496)  |grad|=1.225e+00\n",
            "  global #143: transformer.h.0.ln_2.weight(326)  |grad|=1.224e+00\n",
            "  global #144: transformer.wte.weight(6445, 496)  |grad|=1.212e+00\n",
            "  global #145: transformer.wte.weight(11849, 496)  |grad|=1.212e+00\n",
            "  global #146: transformer.h.0.ln_2.weight(393)  |grad|=1.212e+00\n",
            "  global #147: transformer.wte.weight(9398, 496)  |grad|=1.210e+00\n",
            "  global #148: transformer.wte.weight(40929, 496)  |grad|=1.209e+00\n",
            "  global #149: transformer.wte.weight(14818, 496)  |grad|=1.200e+00\n",
            "  global #150: transformer.wte.weight(37885, 496)  |grad|=1.199e+00\n",
            "  global #151: transformer.h.2.ln_2.weight(373)  |grad|=1.195e+00\n",
            "  global #152: transformer.wte.weight(35259, 496)  |grad|=1.194e+00\n",
            "  global #153: transformer.wte.weight(16825, 496)  |grad|=1.187e+00\n",
            "  global #154: transformer.wte.weight(2039, 496)  |grad|=1.186e+00\n",
            "  global #155: transformer.wte.weight(47891, 496)  |grad|=1.183e+00\n",
            "  global #156: transformer.wte.weight(1279, 496)  |grad|=1.181e+00\n",
            "  global #157: transformer.h.10.ln_1.weight(373)  |grad|=1.180e+00\n",
            "  global #158: transformer.h.5.mlp.c_fc.weight(447, 1866)  |grad|=1.177e+00\n",
            "  global #159: transformer.h.11.ln_1.bias(703)  |grad|=1.157e+00\n",
            "  global #160: transformer.wte.weight(18551, 496)  |grad|=1.156e+00\n",
            "  global #161: transformer.h.11.ln_1.bias(77)  |grad|=1.152e+00\n",
            "  global #162: transformer.h.0.ln_2.weight(447)  |grad|=1.146e+00\n",
            "  global #163: transformer.wte.weight(12592, 496)  |grad|=1.137e+00\n",
            "  global #164: transformer.wte.weight(28437, 496)  |grad|=1.134e+00\n",
            "  global #165: transformer.h.6.ln_2.weight(326)  |grad|=1.132e+00\n",
            "  global #166: transformer.wte.weight(338, 496)  |grad|=1.124e+00\n",
            "  global #167: transformer.wte.weight(11852, 496)  |grad|=1.124e+00\n",
            "  global #168: transformer.h.11.ln_1.bias(102)  |grad|=1.122e+00\n",
            "  global #169: transformer.wte.weight(257, 496)  |grad|=1.120e+00\n",
            "  global #170: transformer.wte.weight(30498, 496)  |grad|=1.119e+00\n",
            "  global #171: transformer.wte.weight(220, 430)  |grad|=1.118e+00\n",
            "  global #172: transformer.wte.weight(10880, 496)  |grad|=1.117e+00\n",
            "  global #173: transformer.wte.weight(2448, 496)  |grad|=1.117e+00\n",
            "  global #174: transformer.h.11.ln_1.bias(107)  |grad|=1.117e+00\n",
            "  global #175: transformer.wte.weight(2635, 496)  |grad|=1.116e+00\n",
            "  global #176: transformer.wte.weight(1665, 496)  |grad|=1.114e+00\n",
            "  global #177: transformer.h.5.ln_2.weight(64)  |grad|=1.113e+00\n",
            "  global #178: transformer.wte.weight(764, 430)  |grad|=1.112e+00\n",
            "  global #179: transformer.wte.weight(4302, 496)  |grad|=1.112e+00\n",
            "  global #180: transformer.wte.weight(796, 36)  |grad|=1.108e+00\n",
            "  global #181: transformer.wte.weight(19439, 496)  |grad|=1.103e+00\n",
            "  global #182: transformer.wte.weight(25656, 496)  |grad|=1.102e+00\n",
            "  global #183: transformer.wte.weight(3240, 496)  |grad|=1.094e+00\n",
            "  global #184: transformer.h.0.ln_2.weight(497)  |grad|=1.092e+00\n",
            "  global #185: transformer.wte.weight(33911, 496)  |grad|=1.080e+00\n",
            "  global #186: transformer.h.7.ln_1.weight(393)  |grad|=1.080e+00\n",
            "  global #187: transformer.h.6.ln_1.weight(373)  |grad|=1.079e+00\n",
            "  global #188: transformer.wte.weight(17099, 496)  |grad|=1.075e+00\n",
            "  global #189: transformer.wte.weight(13, 430)  |grad|=1.062e+00\n",
            "  global #190: transformer.h.11.ln_2.bias(640)  |grad|=1.061e+00\n",
            "  global #191: transformer.wte.weight(46811, 496)  |grad|=1.060e+00\n",
            "  global #192: transformer.wte.weight(3592, 496)  |grad|=1.055e+00\n",
            "  global #193: transformer.wte.weight(38237, 496)  |grad|=1.049e+00\n",
            "  global #194: transformer.wte.weight(20404, 496)  |grad|=1.046e+00\n",
            "  global #195: transformer.h.11.ln_1.weight(393)  |grad|=1.046e+00\n",
            "  global #196: transformer.wte.weight(49063, 430)  |grad|=1.045e+00\n",
            "  global #197: transformer.h.9.ln_1.weight(373)  |grad|=1.042e+00\n",
            "  global #198: transformer.wte.weight(705, 496)  |grad|=1.040e+00\n",
            "  global #199: transformer.h.7.ln_1.bias(173)  |grad|=1.038e+00\n",
            "  global #200: transformer.wte.weight(27583, 430)  |grad|=1.034e+00\n",
            "  global #201: transformer.wte.weight(49931, 496)  |grad|=1.033e+00\n",
            "  global #202: transformer.wte.weight(39601, 496)  |grad|=1.033e+00\n",
            "  global #203: transformer.wte.weight(25979, 496)  |grad|=1.032e+00\n",
            "  global #204: transformer.wte.weight(3687, 496)  |grad|=1.030e+00\n",
            "  global #205: transformer.wte.weight(44302, 496)  |grad|=1.025e+00\n",
            "  global #206: transformer.wte.weight(8225, 496)  |grad|=1.024e+00\n",
            "  global #207: transformer.wte.weight(978, 496)  |grad|=1.022e+00\n",
            "  global #208: transformer.wte.weight(1375, 496)  |grad|=1.020e+00\n",
            "  global #209: transformer.wte.weight(32164, 496)  |grad|=1.020e+00\n",
            "  global #210: transformer.wte.weight(27599, 496)  |grad|=1.020e+00\n",
            "  global #211: transformer.wte.weight(5838, 496)  |grad|=1.018e+00\n",
            "  global #212: transformer.wte.weight(12469, 496)  |grad|=1.018e+00\n",
            "  global #213: transformer.wte.weight(38580, 496)  |grad|=1.016e+00\n",
            "  global #214: transformer.h.11.ln_2.bias(447)  |grad|=1.016e+00\n",
            "  global #215: transformer.wte.weight(9719, 496)  |grad|=1.012e+00\n",
            "  global #216: transformer.wte.weight(287, 496)  |grad|=1.012e+00\n",
            "  global #217: transformer.wte.weight(27132, 496)  |grad|=1.010e+00\n",
            "  global #218: transformer.wte.weight(3905, 496)  |grad|=1.007e+00\n",
            "  global #219: transformer.h.5.mlp.c_fc.weight(326, 1866)  |grad|=1.007e+00\n",
            "  global #220: transformer.h.1.ln_1.weight(266)  |grad|=1.006e+00\n",
            "  global #221: transformer.wte.weight(10343, 496)  |grad|=1.004e+00\n",
            "  global #222: transformer.wte.weight(13709, 496)  |grad|=1.004e+00\n",
            "  global #223: transformer.wte.weight(11, 36)  |grad|=1.003e+00\n",
            "  global #224: transformer.wte.weight(34315, 36)  |grad|=1.003e+00\n",
            "  global #225: transformer.wte.weight(14074, 496)  |grad|=9.995e-01\n",
            "  global #226: transformer.wte.weight(46446, 496)  |grad|=9.990e-01\n",
            "  global #227: transformer.wte.weight(26249, 496)  |grad|=9.980e-01\n",
            "  global #228: transformer.wte.weight(11523, 496)  |grad|=9.961e-01\n",
            "  global #229: transformer.h.10.ln_2.bias(87)  |grad|=9.946e-01\n",
            "  global #230: transformer.wte.weight(12686, 496)  |grad|=9.897e-01\n",
            "  global #231: transformer.wte.weight(46906, 496)  |grad|=9.893e-01\n",
            "  global #232: transformer.wte.weight(10769, 496)  |grad|=9.893e-01\n",
            "  global #233: transformer.wte.weight(39644, 496)  |grad|=9.888e-01\n",
            "  global #234: transformer.wte.weight(1379, 496)  |grad|=9.868e-01\n",
            "  global #235: transformer.wte.weight(3261, 496)  |grad|=9.863e-01\n",
            "  global #236: transformer.wte.weight(40580, 496)  |grad|=9.854e-01\n",
            "  global #237: transformer.wte.weight(6252, 496)  |grad|=9.854e-01\n",
            "  global #238: transformer.wte.weight(24023, 496)  |grad|=9.829e-01\n",
            "  global #239: transformer.h.1.ln_1.weight(64)  |grad|=9.829e-01\n",
            "  global #240: transformer.wte.weight(5108, 496)  |grad|=9.819e-01\n",
            "  global #241: transformer.wte.weight(1757, 496)  |grad|=9.810e-01\n",
            "  global #242: transformer.h.5.mlp.c_fc.bias(1866)  |grad|=9.751e-01\n",
            "  global #243: transformer.h.5.mlp.c_fc.weight(138, 1866)  |grad|=9.741e-01\n",
            "  global #244: transformer.wte.weight(20842, 496)  |grad|=9.707e-01\n",
            "  global #245: transformer.wte.weight(2046, 496)  |grad|=9.707e-01\n",
            "  global #246: transformer.h.6.attn.c_attn.weight(64, 2197)  |grad|=9.673e-01\n",
            "  global #247: transformer.wte.weight(290, 496)  |grad|=9.595e-01\n",
            "  global #248: transformer.wte.weight(9502, 496)  |grad|=9.575e-01\n",
            "  global #249: transformer.h.0.ln_2.weight(459)  |grad|=9.546e-01\n",
            "  global #250: transformer.wte.weight(29490, 496)  |grad|=9.536e-01\n",
            "\n",
            "=== Filtered Top-250 (local + global ranks) ===\n",
            "Filter: ['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.1.ln_1.weight', 'transformer.h.2.ln_1.weight', 'transformer.h.3.ln_1.weight', 'transformer.h.4.ln_1.weight', 'transformer.h.5.ln_1.weight', 'transformer.h.6.ln_1.weight', 'transformer.h.7.ln_1.weight', 'transformer.h.8.ln_1.weight', 'transformer.h.9.ln_1.weight', 'transformer.h.10.ln_1.weight', 'transformer.h.11.ln_1.weight', 'transformer.h.0.ln_2.weight', 'transformer.h.1.ln_2.weight', 'transformer.h.2.ln_2.weight', 'transformer.h.3.ln_2.weight', 'transformer.h.4.ln_2.weight', 'transformer.h.5.ln_2.weight', 'transformer.h.6.ln_2.weight', 'transformer.h.7.ln_2.weight', 'transformer.h.8.ln_2.weight', 'transformer.h.9.ln_2.weight', 'transformer.h.10.ln_2.weight', 'transformer.h.11.ln_2.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.1.ln_1.bias', 'transformer.h.2.ln_1.bias', 'transformer.h.3.ln_1.bias', 'transformer.h.4.ln_1.bias', 'transformer.h.5.ln_1.bias', 'transformer.h.6.ln_1.bias', 'transformer.h.7.ln_1.bias', 'transformer.h.8.ln_1.bias', 'transformer.h.9.ln_1.bias', 'transformer.h.10.ln_1.bias', 'transformer.h.11.ln_1.bias', 'transformer.h.0.ln_2.bias', 'transformer.h.1.ln_2.bias', 'transformer.h.2.ln_2.bias', 'transformer.h.3.ln_2.bias', 'transformer.h.4.ln_2.bias', 'transformer.h.5.ln_2.bias', 'transformer.h.6.ln_2.bias', 'transformer.h.7.ln_2.bias', 'transformer.h.8.ln_2.bias', 'transformer.h.9.ln_2.bias', 'transformer.h.10.ln_2.bias', 'transformer.h.11.ln_2.bias', 'transformer.ln_f.weight', 'transformer.ln_f.bias']\n",
            "  local #  1 | global #117:  transformer.h.5.mlp.c_fc.weight(393, 1866)  |grad|=1.341e+00\n",
            "  local #  2 | global #123:  transformer.h.5.mlp.c_fc.weight(373, 1866)  |grad|=1.319e+00\n",
            "  local #  3 | global #134:  transformer.h.5.mlp.c_fc.weight(64, 1866)  |grad|=1.298e+00\n",
            "  local #  4 | global #158:  transformer.h.5.mlp.c_fc.weight(447, 1866)  |grad|=1.177e+00\n",
            "  local #  5 | global #219:  transformer.h.5.mlp.c_fc.weight(326, 1866)  |grad|=1.007e+00\n",
            "  local #  6 | global #242:  transformer.h.5.mlp.c_fc.bias(1866)  |grad|=9.751e-01\n",
            "  local #  7 | global #243:  transformer.h.5.mlp.c_fc.weight(138, 1866)  |grad|=9.741e-01\n",
            "  local #  8 | global #246:  transformer.h.6.attn.c_attn.weight(64, 2197)  |grad|=9.673e-01\n",
            "  local #  9 | global #262:  transformer.h.7.attn.c_attn.weight(64, 2296)  |grad|=9.316e-01\n",
            "  local # 10 | global #300:  transformer.h.3.mlp.c_fc.weight(393, 1612)  |grad|=8.853e-01\n",
            "  local # 11 | global #319:  transformer.h.6.attn.c_attn.weight(64, 2217)  |grad|=8.691e-01\n",
            "  local # 12 | global #325:  transformer.h.6.attn.c_attn.weight(64, 2238)  |grad|=8.628e-01\n",
            "  local # 13 | global #340:  transformer.h.8.attn.c_attn.weight(64, 1712)  |grad|=8.516e-01\n",
            "  local # 14 | global #346:  transformer.h.5.mlp.c_fc.weight(288, 1866)  |grad|=8.457e-01\n",
            "  local # 15 | global #371:  transformer.h.9.attn.c_attn.weight(64, 2281)  |grad|=8.203e-01\n",
            "  local # 16 | global #376:  transformer.h.9.attn.c_attn.weight(64, 2302)  |grad|=8.154e-01\n",
            "  local # 17 | global #383:  transformer.h.6.attn.c_attn.weight(64, 2207)  |grad|=8.125e-01\n",
            "  local # 18 | global #391:  transformer.h.7.attn.c_attn.weight(64, 2271)  |grad|=8.071e-01\n",
            "  local # 19 | global #397:  transformer.h.5.attn.c_attn.weight(64, 888)  |grad|=8.022e-01\n",
            "  local # 20 | global #401:  transformer.h.5.mlp.c_fc.weight(640, 1866)  |grad|=7.988e-01\n",
            "  local # 21 | global #406:  transformer.h.5.attn.c_attn.weight(64, 2156)  |grad|=7.935e-01\n",
            "  local # 22 | global #420:  transformer.h.5.attn.c_attn.weight(64, 2176)  |grad|=7.778e-01\n",
            "  local # 23 | global #421:  transformer.h.5.attn.c_attn.weight(64, 834)  |grad|=7.769e-01\n",
            "  local # 24 | global #423:  transformer.h.6.attn.c_attn.weight(64, 2199)  |grad|=7.690e-01\n",
            "  local # 25 | global #426:  transformer.h.0.attn.c_attn.bias(2091)  |grad|=7.676e-01\n",
            "  local # 26 | global #427:  transformer.h.6.attn.c_attn.weight(64, 2211)  |grad|=7.666e-01\n",
            "  local # 27 | global #437:  transformer.h.3.mlp.c_fc.weight(138, 1612)  |grad|=7.529e-01\n",
            "  local # 28 | global #438:  transformer.h.6.attn.c_attn.weight(64, 2225)  |grad|=7.520e-01\n",
            "  local # 29 | global #442:  transformer.h.2.attn.c_attn.weight(64, 1637)  |grad|=7.476e-01\n",
            "  local # 30 | global #443:  transformer.h.5.attn.c_attn.weight(64, 2158)  |grad|=7.407e-01\n",
            "  local # 31 | global #444:  transformer.h.6.attn.c_attn.weight(64, 2181)  |grad|=7.354e-01\n",
            "  local # 32 | global #448:  transformer.h.5.attn.c_attn.weight(64, 2123)  |grad|=7.275e-01\n",
            "  local # 33 | global #453:  transformer.h.9.attn.c_attn.weight(64, 2287)  |grad|=7.231e-01\n",
            "  local # 34 | global #464:  transformer.h.11.attn.c_attn.weight(138, 1528)  |grad|=7.104e-01\n",
            "  local # 35 | global #474:  transformer.h.6.attn.c_attn.weight(64, 2235)  |grad|=6.997e-01\n",
            "  local # 36 | global #477:  transformer.h.7.attn.c_attn.weight(64, 2255)  |grad|=6.992e-01\n",
            "  local # 37 | global #478:  transformer.h.7.attn.c_attn.weight(64, 2289)  |grad|=6.987e-01\n",
            "  local # 38 | global #479:  transformer.h.5.attn.c_attn.weight(64, 590)  |grad|=6.982e-01\n",
            "  local # 39 | global #480:  transformer.h.5.attn.c_attn.weight(64, 592)  |grad|=6.973e-01\n",
            "  local # 40 | global #481:  transformer.h.6.attn.c_attn.weight(64, 2227)  |grad|=6.973e-01\n",
            "  local # 41 | global #483:  transformer.h.8.attn.c_attn.weight(64, 1671)  |grad|=6.919e-01\n",
            "  local # 42 | global #485:  transformer.h.8.attn.c_attn.weight(64, 1693)  |grad|=6.909e-01\n",
            "  local # 43 | global #494:  transformer.h.0.attn.c_attn.bias(2095)  |grad|=6.797e-01\n",
            "  local # 44 | global #499:  transformer.h.6.attn.c_attn.weight(64, 2120)  |grad|=6.758e-01\n",
            "  local # 45 | global #501:  transformer.h.6.attn.c_attn.weight(64, 2188)  |grad|=6.724e-01\n",
            "  local # 46 | global #504:  transformer.h.9.attn.c_attn.weight(64, 2277)  |grad|=6.719e-01\n",
            "  local # 47 | global #510:  transformer.h.2.attn.c_attn.weight(64, 1613)  |grad|=6.650e-01\n",
            "  local # 48 | global #515:  transformer.h.2.mlp.c_fc.weight(138, 1597)  |grad|=6.611e-01\n",
            "  local # 49 | global #520:  transformer.h.7.attn.c_attn.weight(64, 1982)  |grad|=6.582e-01\n",
            "  local # 50 | global #523:  transformer.h.6.attn.c_attn.weight(64, 2200)  |grad|=6.567e-01\n",
            "  local # 51 | global #527:  transformer.h.3.attn.c_attn.weight(64, 1819)  |grad|=6.553e-01\n",
            "  local # 52 | global #528:  transformer.h.8.attn.c_attn.weight(64, 1674)  |grad|=6.553e-01\n",
            "  local # 53 | global #531:  transformer.h.5.attn.c_attn.weight(64, 833)  |grad|=6.538e-01\n",
            "  local # 54 | global #532:  transformer.h.5.attn.c_attn.weight(64, 2143)  |grad|=6.538e-01\n",
            "  local # 55 | global #533:  transformer.h.6.attn.c_attn.weight(64, 1935)  |grad|=6.538e-01\n",
            "  local # 56 | global #538:  transformer.h.7.attn.c_attn.weight(64, 2247)  |grad|=6.519e-01\n",
            "  local # 57 | global #543:  transformer.h.6.attn.c_attn.weight(64, 2223)  |grad|=6.475e-01\n",
            "  local # 58 | global #551:  transformer.h.3.mlp.c_fc.weight(64, 1612)  |grad|=6.406e-01\n",
            "  local # 59 | global #555:  transformer.h.5.attn.c_attn.weight(64, 876)  |grad|=6.392e-01\n",
            "  local # 60 | global #561:  transformer.h.5.attn.c_attn.weight(64, 873)  |grad|=6.372e-01\n",
            "  local # 61 | global #562:  transformer.h.6.attn.c_attn.weight(64, 2203)  |grad|=6.372e-01\n",
            "  local # 62 | global #564:  transformer.h.5.attn.c_attn.weight(64, 2186)  |grad|=6.362e-01\n",
            "  local # 63 | global #566:  transformer.h.4.attn.c_attn.weight(64, 2111)  |grad|=6.357e-01\n",
            "  local # 64 | global #567:  transformer.h.5.attn.c_attn.weight(64, 2192)  |grad|=6.348e-01\n",
            "  local # 65 | global #569:  transformer.h.5.attn.c_attn.weight(64, 835)  |grad|=6.343e-01\n",
            "  local # 66 | global #571:  transformer.h.5.attn.c_attn.weight(64, 2179)  |grad|=6.338e-01\n",
            "  local # 67 | global #572:  transformer.h.6.attn.c_attn.weight(64, 2186)  |grad|=6.338e-01\n",
            "  local # 68 | global #573:  transformer.h.5.attn.c_attn.weight(64, 887)  |grad|=6.328e-01\n",
            "  local # 69 | global #588:  transformer.h.5.attn.c_attn.weight(64, 585)  |grad|=6.226e-01\n",
            "  local # 70 | global #589:  transformer.h.9.attn.c_attn.weight(64, 2245)  |grad|=6.226e-01\n",
            "  local # 71 | global #598:  transformer.h.5.attn.c_attn.weight(64, 2117)  |grad|=6.196e-01\n",
            "  local # 72 | global #601:  transformer.h.5.attn.c_attn.weight(447, 2241)  |grad|=6.187e-01\n",
            "  local # 73 | global #604:  transformer.h.5.attn.c_attn.weight(64, 1402)  |grad|=6.172e-01\n",
            "  local # 74 | global #610:  transformer.h.4.attn.c_attn.weight(64, 2062)  |grad|=6.147e-01\n",
            "  local # 75 | global #615:  transformer.h.5.attn.c_attn.weight(64, 761)  |grad|=6.123e-01\n",
            "  local # 76 | global #616:  transformer.h.5.attn.c_attn.weight(64, 2221)  |grad|=6.108e-01\n",
            "  local # 77 | global #620:  transformer.h.3.mlp.c_fc.weight(447, 1612)  |grad|=6.099e-01\n",
            "  local # 78 | global #621:  transformer.h.5.attn.c_attn.weight(64, 591)  |grad|=6.089e-01\n",
            "  local # 79 | global #622:  transformer.h.5.attn.c_attn.weight(64, 2171)  |grad|=6.089e-01\n",
            "  local # 80 | global #624:  transformer.h.8.attn.c_attn.weight(64, 1725)  |grad|=6.084e-01\n",
            "  local # 81 | global #626:  transformer.h.0.attn.c_attn.bias(2185)  |grad|=6.079e-01\n",
            "  local # 82 | global #633:  transformer.h.5.attn.c_attn.weight(64, 2241)  |grad|=6.035e-01\n",
            "  local # 83 | global #636:  transformer.h.6.attn.c_attn.weight(64, 2237)  |grad|=6.025e-01\n",
            "  local # 84 | global #641:  transformer.h.9.attn.c_attn.weight(64, 2244)  |grad|=6.021e-01\n",
            "  local # 85 | global #644:  transformer.h.8.attn.c_attn.weight(64, 1667)  |grad|=5.991e-01\n",
            "  local # 86 | global #646:  transformer.h.5.attn.c_attn.weight(64, 1783)  |grad|=5.972e-01\n",
            "  local # 87 | global #648:  transformer.h.5.attn.c_attn.weight(64, 2132)  |grad|=5.962e-01\n",
            "  local # 88 | global #649:  transformer.h.6.attn.c_attn.weight(64, 1830)  |grad|=5.957e-01\n",
            "  local # 89 | global #650:  transformer.h.5.attn.c_attn.weight(64, 2223)  |grad|=5.947e-01\n",
            "  local # 90 | global #653:  transformer.h.0.attn.c_attn.bias(2071)  |grad|=5.928e-01\n",
            "  local # 91 | global #659:  transformer.h.5.attn.c_attn.weight(64, 2238)  |grad|=5.898e-01\n",
            "  local # 92 | global #666:  transformer.h.6.attn.c_attn.weight(64, 2226)  |grad|=5.874e-01\n",
            "  local # 93 | global #669:  transformer.h.9.attn.c_attn.weight(64, 2290)  |grad|=5.859e-01\n",
            "  local # 94 | global #670:  transformer.h.5.attn.c_attn.weight(64, 2202)  |grad|=5.854e-01\n",
            "  local # 95 | global #671:  transformer.h.5.attn.c_attn.weight(447, 2271)  |grad|=5.854e-01\n",
            "  local # 96 | global #676:  transformer.h.5.attn.c_attn.weight(64, 2271)  |grad|=5.825e-01\n",
            "  local # 97 | global #677:  transformer.h.6.attn.c_attn.weight(64, 2213)  |grad|=5.815e-01\n",
            "  local # 98 | global #681:  transformer.h.6.attn.c_attn.weight(64, 1842)  |grad|=5.791e-01\n",
            "  local # 99 | global #688:  transformer.h.5.attn.c_attn.weight(64, 2178)  |grad|=5.762e-01\n",
            "  local #100 | global #689:  transformer.h.5.attn.c_attn.weight(64, 119)  |grad|=5.757e-01\n",
            "  local #101 | global #692:  transformer.h.6.attn.c_attn.weight(64, 2189)  |grad|=5.752e-01\n",
            "  local #102 | global #698:  transformer.h.9.attn.c_attn.weight(64, 2276)  |grad|=5.723e-01\n",
            "  local #103 | global #700:  transformer.h.8.attn.c_attn.weight(64, 1679)  |grad|=5.718e-01\n",
            "  local #104 | global #701:  transformer.h.6.attn.c_attn.weight(64, 2179)  |grad|=5.713e-01\n",
            "  local #105 | global #703:  transformer.h.3.attn.c_attn.weight(64, 1599)  |grad|=5.703e-01\n",
            "  local #106 | global #705:  transformer.h.7.attn.c_attn.weight(64, 2240)  |grad|=5.703e-01\n",
            "  local #107 | global #709:  transformer.h.5.attn.c_attn.weight(64, 2233)  |grad|=5.688e-01\n",
            "  local #108 | global #712:  transformer.h.5.attn.c_attn.weight(64, 2189)  |grad|=5.679e-01\n",
            "  local #109 | global #720:  transformer.h.7.attn.c_attn.weight(64, 1949)  |grad|=5.664e-01\n",
            "  local #110 | global #722:  transformer.h.4.mlp.c_fc.weight(138, 2253)  |grad|=5.654e-01\n",
            "  local #111 | global #753:  transformer.h.0.attn.c_attn.bias(1621)  |grad|=5.552e-01\n",
            "  local #112 | global #754:  transformer.h.5.attn.c_attn.weight(64, 1734)  |grad|=5.552e-01\n",
            "  local #113 | global #767:  transformer.h.9.attn.c_attn.weight(64, 2258)  |grad|=5.518e-01\n",
            "  local #114 | global #771:  transformer.h.7.attn.c_attn.weight(64, 2261)  |grad|=5.503e-01\n",
            "  local #115 | global #774:  transformer.h.5.attn.c_attn.weight(64, 1599)  |grad|=5.493e-01\n",
            "  local #116 | global #775:  transformer.h.4.attn.c_attn.weight(64, 2289)  |grad|=5.483e-01\n",
            "  local #117 | global #780:  transformer.h.3.attn.c_attn.weight(64, 1581)  |grad|=5.469e-01\n",
            "  local #118 | global #781:  transformer.h.0.attn.c_attn.bias(2284)  |grad|=5.464e-01\n",
            "  local #119 | global #782:  transformer.h.2.attn.c_attn.weight(64, 1652)  |grad|=5.464e-01\n",
            "  local #120 | global #785:  transformer.h.8.attn.c_attn.weight(64, 1682)  |grad|=5.459e-01\n",
            "  local #121 | global #788:  transformer.h.9.attn.c_attn.weight(64, 2254)  |grad|=5.459e-01\n",
            "  local #122 | global #791:  transformer.h.5.attn.c_attn.weight(64, 1359)  |grad|=5.449e-01\n",
            "  local #123 | global #793:  transformer.h.7.attn.c_attn.weight(64, 2270)  |grad|=5.449e-01\n",
            "  local #124 | global #794:  transformer.h.5.attn.c_attn.weight(64, 120)  |grad|=5.444e-01\n",
            "  local #125 | global #796:  transformer.h.5.attn.c_attn.weight(64, 2113)  |grad|=5.435e-01\n",
            "  local #126 | global #799:  transformer.h.5.attn.c_attn.weight(64, 2150)  |grad|=5.425e-01\n",
            "  local #127 | global #813:  transformer.h.5.attn.c_attn.weight(447, 2156)  |grad|=5.371e-01\n",
            "  local #128 | global #815:  transformer.h.6.attn.c_attn.weight(64, 1812)  |grad|=5.366e-01\n",
            "  local #129 | global #821:  transformer.h.6.attn.c_attn.weight(64, 2205)  |grad|=5.356e-01\n",
            "  local #130 | global #822:  transformer.h.5.attn.c_attn.weight(64, 1358)  |grad|=5.347e-01\n",
            "  local #131 | global #830:  transformer.h.3.attn.c_attn.weight(64, 1575)  |grad|=5.327e-01\n",
            "  local #132 | global #831:  transformer.h.5.attn.c_attn.weight(64, 2205)  |grad|=5.327e-01\n",
            "  local #133 | global #832:  transformer.h.7.attn.c_attn.weight(64, 2246)  |grad|=5.327e-01\n",
            "  local #134 | global #833:  transformer.h.3.attn.c_attn.weight(64, 1562)  |grad|=5.322e-01\n",
            "  local #135 | global #834:  transformer.h.5.attn.c_attn.weight(64, 2137)  |grad|=5.322e-01\n",
            "  local #136 | global #837:  transformer.h.3.attn.c_attn.weight(64, 1798)  |grad|=5.312e-01\n",
            "  local #137 | global #838:  transformer.h.5.attn.c_attn.weight(64, 2116)  |grad|=5.312e-01\n",
            "  local #138 | global #841:  transformer.h.5.attn.c_attn.weight(64, 67)  |grad|=5.303e-01\n",
            "  local #139 | global #847:  transformer.h.7.attn.c_attn.weight(64, 2248)  |grad|=5.293e-01\n",
            "  local #140 | global #848:  transformer.h.7.attn.c_attn.weight(64, 2256)  |grad|=5.293e-01\n",
            "  local #141 | global #853:  transformer.h.7.attn.c_attn.weight(64, 2269)  |grad|=5.254e-01\n",
            "  local #142 | global #867:  transformer.h.6.attn.c_attn.weight(64, 2193)  |grad|=5.220e-01\n",
            "  local #143 | global #873:  transformer.h.5.mlp.c_fc.weight(526, 1866)  |grad|=5.215e-01\n",
            "  local #144 | global #876:  transformer.h.8.attn.c_attn.weight(64, 1676)  |grad|=5.210e-01\n",
            "  local #145 | global #887:  transformer.h.2.mlp.c_fc.weight(138, 605)  |grad|=5.176e-01\n",
            "  local #146 | global #894:  transformer.h.5.attn.c_attn.weight(64, 2175)  |grad|=5.166e-01\n",
            "  local #147 | global #908:  transformer.h.5.attn.c_attn.weight(64, 1914)  |grad|=5.127e-01\n",
            "  local #148 | global #912:  transformer.h.8.attn.c_attn.weight(64, 1692)  |grad|=5.117e-01\n",
            "  local #149 | global #913:  transformer.h.8.attn.c_attn.weight(64, 1721)  |grad|=5.117e-01\n",
            "  local #150 | global #920:  transformer.h.7.attn.c_attn.weight(64, 2273)  |grad|=5.107e-01\n",
            "  local #151 | global #924:  transformer.h.8.attn.c_attn.weight(64, 1550)  |grad|=5.103e-01\n",
            "  local #152 | global #925:  transformer.h.5.attn.c_attn.weight(447, 2246)  |grad|=5.098e-01\n",
            "  local #153 | global #929:  transformer.h.6.attn.c_attn.weight(64, 2222)  |grad|=5.088e-01\n",
            "  local #154 | global #930:  transformer.h.6.attn.c_attn.weight(64, 1953)  |grad|=5.088e-01\n",
            "  local #155 | global #932:  transformer.h.5.attn.c_attn.weight(64, 1583)  |grad|=5.078e-01\n",
            "  local #156 | global #935:  transformer.h.5.attn.c_attn.weight(447, 2285)  |grad|=5.073e-01\n",
            "  local #157 | global #941:  transformer.h.3.attn.c_attn.weight(64, 1851)  |grad|=5.063e-01\n",
            "  local #158 | global #942:  transformer.h.5.attn.c_attn.weight(64, 2187)  |grad|=5.063e-01\n",
            "  local #159 | global #946:  transformer.h.5.attn.c_attn.weight(64, 76)  |grad|=5.059e-01\n",
            "  local #160 | global #948:  transformer.h.5.attn.c_attn.weight(64, 2127)  |grad|=5.054e-01\n",
            "  local #161 | global #958:  transformer.h.3.mlp.c_fc.bias(1612)  |grad|=5.044e-01\n",
            "  local #162 | global #961:  transformer.h.6.attn.c_attn.weight(64, 1813)  |grad|=5.039e-01\n",
            "  local #163 | global #969:  transformer.h.3.attn.c_attn.weight(64, 1828)  |grad|=5.029e-01\n",
            "  local #164 | global #970:  transformer.h.5.attn.c_attn.weight(64, 844)  |grad|=5.029e-01\n",
            "  local #165 | global #974:  transformer.h.6.attn.c_attn.weight(64, 1706)  |grad|=5.020e-01\n",
            "  local #166 | global #976:  transformer.h.3.attn.c_attn.weight(64, 1724)  |grad|=5.010e-01\n",
            "  local #167 | global #983:  transformer.h.5.attn.c_attn.weight(64, 1912)  |grad|=4.990e-01\n",
            "  local #168 | global #984:  transformer.h.5.attn.c_attn.weight(64, 2285)  |grad|=4.988e-01\n",
            "  local #169 | global #991:  transformer.h.5.attn.c_attn.weight(64, 2181)  |grad|=4.978e-01\n",
            "  local #170 | global #993:  transformer.h.7.attn.c_attn.weight(64, 1952)  |grad|=4.971e-01\n",
            "  local #171 | global #994:  transformer.h.5.attn.c_attn.weight(447, 2123)  |grad|=4.968e-01\n",
            "  local #172 | global #1002:  transformer.h.6.attn.c_attn.weight(64, 1969)  |grad|=4.944e-01\n",
            "  local #173 | global #1008:  transformer.h.0.attn.c_attn.bias(1924)  |grad|=4.932e-01\n",
            "  local #174 | global #1012:  transformer.h.8.attn.c_attn.weight(64, 1713)  |grad|=4.929e-01\n",
            "  local #175 | global #1015:  transformer.h.5.attn.c_attn.weight(64, 2153)  |grad|=4.924e-01\n",
            "  local #176 | global #1026:  transformer.h.5.attn.c_attn.weight(64, 2160)  |grad|=4.905e-01\n",
            "  local #177 | global #1029:  transformer.h.5.attn.c_attn.weight(64, 2200)  |grad|=4.902e-01\n",
            "  local #178 | global #1031:  transformer.h.4.attn.c_attn.weight(64, 2065)  |grad|=4.900e-01\n",
            "  local #179 | global #1036:  transformer.h.5.attn.c_attn.weight(64, 2198)  |grad|=4.893e-01\n",
            "  local #180 | global #1037:  transformer.h.3.attn.c_attn.weight(64, 1829)  |grad|=4.888e-01\n",
            "  local #181 | global #1044:  transformer.h.6.attn.c_attn.weight(64, 1977)  |grad|=4.868e-01\n",
            "  local #182 | global #1047:  transformer.h.5.attn.c_attn.weight(64, 2172)  |grad|=4.863e-01\n",
            "  local #183 | global #1048:  transformer.h.7.attn.c_attn.weight(64, 1954)  |grad|=4.863e-01\n",
            "  local #184 | global #1058:  transformer.h.7.attn.c_attn.weight(64, 1932)  |grad|=4.841e-01\n",
            "  local #185 | global #1069:  transformer.h.6.attn.c_attn.weight(64, 1854)  |grad|=4.824e-01\n",
            "  local #186 | global #1070:  transformer.h.4.attn.c_attn.weight(64, 2300)  |grad|=4.822e-01\n",
            "  local #187 | global #1071:  transformer.h.5.attn.c_attn.weight(64, 1775)  |grad|=4.819e-01\n",
            "  local #188 | global #1074:  transformer.h.6.attn.c_attn.weight(64, 2191)  |grad|=4.819e-01\n",
            "  local #189 | global #1076:  transformer.h.8.attn.c_attn.weight(64, 1673)  |grad|=4.819e-01\n",
            "  local #190 | global #1079:  transformer.h.5.attn.c_attn.weight(64, 2115)  |grad|=4.812e-01\n",
            "  local #191 | global #1081:  transformer.h.7.attn.c_attn.weight(64, 1631)  |grad|=4.810e-01\n",
            "  local #192 | global #1083:  transformer.h.2.mlp.c_fc.weight(138, 854)  |grad|=4.800e-01\n",
            "  local #193 | global #1085:  transformer.h.6.attn.c_attn.weight(64, 2198)  |grad|=4.797e-01\n",
            "  local #194 | global #1088:  transformer.h.8.attn.c_attn.weight(64, 643)  |grad|=4.792e-01\n",
            "  local #195 | global #1095:  transformer.h.4.attn.c_attn.weight(64, 2092)  |grad|=4.783e-01\n",
            "  local #196 | global #1100:  transformer.h.5.attn.c_attn.weight(64, 1557)  |grad|=4.773e-01\n",
            "  local #197 | global #1103:  transformer.h.5.attn.c_attn.weight(447, 1603)  |grad|=4.771e-01\n",
            "  local #198 | global #1105:  transformer.h.5.attn.c_attn.weight(64, 2133)  |grad|=4.768e-01\n",
            "  local #199 | global #1113:  transformer.h.6.attn.c_attn.weight(64, 1690)  |grad|=4.753e-01\n",
            "  local #200 | global #1115:  transformer.h.6.attn.c_attn.weight(64, 1919)  |grad|=4.751e-01\n",
            "  local #201 | global #1127:  transformer.h.8.attn.c_attn.weight(64, 1681)  |grad|=4.744e-01\n",
            "  local #202 | global #1129:  transformer.h.7.attn.c_attn.weight(64, 1928)  |grad|=4.741e-01\n",
            "  local #203 | global #1133:  transformer.h.3.mlp.c_fc.weight(288, 1612)  |grad|=4.739e-01\n",
            "  local #204 | global #1136:  transformer.h.5.attn.c_attn.weight(64, 2217)  |grad|=4.736e-01\n",
            "  local #205 | global #1137:  transformer.h.5.attn.c_attn.weight(64, 1549)  |grad|=4.731e-01\n",
            "  local #206 | global #1140:  transformer.h.0.attn.c_attn.bias(1759)  |grad|=4.722e-01\n",
            "  local #207 | global #1152:  transformer.h.5.attn.c_attn.weight(64, 2246)  |grad|=4.695e-01\n",
            "  local #208 | global #1155:  transformer.h.5.attn.c_attn.weight(64, 2226)  |grad|=4.688e-01\n",
            "  local #209 | global #1157:  transformer.h.5.attn.c_attn.weight(64, 2112)  |grad|=4.685e-01\n",
            "  local #210 | global #1159:  transformer.h.6.attn.c_attn.weight(64, 1942)  |grad|=4.680e-01\n",
            "  local #211 | global #1164:  transformer.h.7.attn.c_attn.weight(64, 2258)  |grad|=4.670e-01\n",
            "  local #212 | global #1165:  transformer.h.3.mlp.c_fc.weight(373, 1612)  |grad|=4.668e-01\n",
            "  local #213 | global #1171:  transformer.h.6.attn.c_attn.weight(64, 1962)  |grad|=4.661e-01\n",
            "  local #214 | global #1179:  transformer.h.5.attn.c_attn.weight(447, 2280)  |grad|=4.646e-01\n",
            "  local #215 | global #1198:  transformer.h.1.attn.c_attn.weight(745, 2278)  |grad|=4.624e-01\n",
            "  local #216 | global #1202:  transformer.h.3.attn.c_attn.weight(64, 1832)  |grad|=4.614e-01\n",
            "  local #217 | global #1203:  transformer.h.6.attn.c_attn.weight(64, 2215)  |grad|=4.612e-01\n",
            "  local #218 | global #1207:  transformer.h.6.attn.c_attn.weight(64, 2206)  |grad|=4.607e-01\n",
            "  local #219 | global #1209:  transformer.h.7.attn.c_attn.weight(64, 2284)  |grad|=4.604e-01\n",
            "  local #220 | global #1211:  transformer.h.0.attn.c_attn.bias(2268)  |grad|=4.600e-01\n",
            "  local #221 | global #1212:  transformer.h.8.attn.c_attn.weight(64, 1545)  |grad|=4.597e-01\n",
            "  local #222 | global #1221:  transformer.h.3.attn.c_attn.weight(64, 1799)  |grad|=4.575e-01\n",
            "  local #223 | global #1222:  transformer.h.6.attn.c_attn.weight(64, 1697)  |grad|=4.573e-01\n",
            "  local #224 | global #1226:  transformer.h.9.attn.c_attn.weight(64, 2292)  |grad|=4.568e-01\n",
            "  local #225 | global #1234:  transformer.h.0.attn.c_attn.bias(2067)  |grad|=4.556e-01\n",
            "  local #226 | global #1247:  transformer.h.7.attn.c_attn.weight(64, 2274)  |grad|=4.541e-01\n",
            "  local #227 | global #1248:  transformer.h.8.attn.c_attn.weight(64, 1586)  |grad|=4.541e-01\n",
            "  local #228 | global #1258:  transformer.h.5.attn.c_attn.weight(64, 1740)  |grad|=4.519e-01\n",
            "  local #229 | global #1261:  transformer.h.6.attn.c_attn.weight(64, 2073)  |grad|=4.517e-01\n",
            "  local #230 | global #1265:  transformer.h.5.attn.c_attn.weight(64, 1779)  |grad|=4.504e-01\n",
            "  local #231 | global #1277:  transformer.h.10.attn.c_attn.weight(447, 1906)  |grad|=4.482e-01\n",
            "  local #232 | global #1279:  transformer.h.2.attn.c_attn.weight(64, 1659)  |grad|=4.480e-01\n",
            "  local #233 | global #1280:  transformer.h.7.attn.c_attn.weight(64, 1983)  |grad|=4.480e-01\n",
            "  local #234 | global #1286:  transformer.h.5.attn.c_attn.weight(64, 2280)  |grad|=4.473e-01\n",
            "  local #235 | global #1292:  transformer.h.7.attn.c_attn.weight(64, 2272)  |grad|=4.470e-01\n",
            "  local #236 | global #1303:  transformer.h.3.attn.c_attn.weight(64, 1538)  |grad|=4.458e-01\n",
            "  local #237 | global #1305:  transformer.h.5.attn.c_attn.weight(64, 2183)  |grad|=4.456e-01\n",
            "  local #238 | global #1311:  transformer.h.5.attn.c_attn.weight(64, 2125)  |grad|=4.448e-01\n",
            "  local #239 | global #1312:  transformer.h.3.attn.c_attn.weight(64, 2042)  |grad|=4.446e-01\n",
            "  local #240 | global #1313:  transformer.h.5.attn.c_attn.weight(64, 895)  |grad|=4.446e-01\n",
            "  local #241 | global #1315:  transformer.h.8.attn.c_attn.weight(64, 1710)  |grad|=4.446e-01\n",
            "  local #242 | global #1319:  transformer.h.6.attn.c_attn.weight(64, 1575)  |grad|=4.443e-01\n",
            "  local #243 | global #1321:  transformer.h.6.attn.c_attn.weight(64, 1686)  |grad|=4.441e-01\n",
            "  local #244 | global #1324:  transformer.h.4.attn.c_attn.weight(64, 2293)  |grad|=4.438e-01\n",
            "  local #245 | global #1325:  transformer.h.7.attn.c_attn.weight(64, 2290)  |grad|=4.438e-01\n",
            "  local #246 | global #1327:  transformer.h.0.attn.c_proj.bias(138)  |grad|=4.434e-01\n",
            "  local #247 | global #1338:  transformer.h.0.attn.c_attn.bias(2073)  |grad|=4.421e-01\n",
            "  local #248 | global #1339:  transformer.h.5.attn.c_attn.weight(64, 2130)  |grad|=4.421e-01\n",
            "  local #249 | global #1342:  transformer.h.6.attn.c_attn.weight(64, 1852)  |grad|=4.417e-01\n",
            "  local #250 | global #1343:  transformer.h.10.attn.c_attn.weight(447, 1860)  |grad|=4.417e-01\n"
          ]
        }
      ]
    }
  ]
}