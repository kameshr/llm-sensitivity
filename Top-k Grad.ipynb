{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNrsvsjYmRkg5fuf08vfhI8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzboobhcXnuD",
        "outputId": "1407ba0f-97a8-4125-e3a7-94ada1807642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top-250 most sensitive tensor elements:\n",
            "  #1: transformer.wte.weight(2488, 496)  |grad|=5.235e+00\n",
            "  #2: transformer.wte.weight(837, 496)  |grad|=4.481e+00\n",
            "  #3: transformer.wte.weight(198, 496)  |grad|=3.247e+00\n",
            "  #4: transformer.wte.weight(11, 496)  |grad|=2.898e+00\n",
            "  #5: transformer.wte.weight(34315, 496)  |grad|=2.669e+00\n",
            "  #6: transformer.wte.weight(796, 496)  |grad|=2.517e+00\n",
            "  #7: transformer.wte.weight(764, 496)  |grad|=2.499e+00\n",
            "  #8: transformer.wte.weight(13, 496)  |grad|=2.427e+00\n",
            "  #9: transformer.wte.weight(220, 496)  |grad|=2.242e+00\n",
            "  #10: transformer.wte.weight(2488, 430)  |grad|=2.207e+00\n",
            "  #11: transformer.wte.weight(198, 430)  |grad|=2.154e+00\n",
            "  #12: transformer.wte.weight(31, 496)  |grad|=2.059e+00\n",
            "  #13: transformer.wte.weight(5187, 496)  |grad|=2.046e+00\n",
            "  #14: transformer.wte.weight(49063, 496)  |grad|=2.029e+00\n",
            "  #15: transformer.wte.weight(27583, 496)  |grad|=2.001e+00\n",
            "  #16: transformer.wte.weight(837, 430)  |grad|=1.993e+00\n",
            "  #17: transformer.wte.weight(6645, 496)  |grad|=1.939e+00\n",
            "  #18: transformer.wte.weight(21165, 496)  |grad|=1.789e+00\n",
            "  #19: transformer.wte.weight(15281, 496)  |grad|=1.745e+00\n",
            "  #20: transformer.wte.weight(3056, 496)  |grad|=1.702e+00\n",
            "  #21: transformer.wte.weight(12164, 496)  |grad|=1.672e+00\n",
            "  #22: transformer.wte.weight(198, 36)  |grad|=1.671e+00\n",
            "  #23: transformer.wte.weight(497, 496)  |grad|=1.666e+00\n",
            "  #24: transformer.wte.weight(2488, 36)  |grad|=1.663e+00\n",
            "  #25: transformer.wte.weight(21371, 496)  |grad|=1.626e+00\n",
            "  #26: transformer.wte.weight(449, 496)  |grad|=1.624e+00\n",
            "  #27: transformer.wte.weight(40902, 496)  |grad|=1.597e+00\n",
            "  #28: transformer.wte.weight(8670, 496)  |grad|=1.562e+00\n",
            "  #29: transformer.wte.weight(3999, 496)  |grad|=1.561e+00\n",
            "  #30: transformer.wte.weight(5178, 496)  |grad|=1.556e+00\n",
            "  #31: transformer.wte.weight(8942, 496)  |grad|=1.540e+00\n",
            "  #32: transformer.wte.weight(837, 36)  |grad|=1.508e+00\n",
            "  #33: transformer.wte.weight(4544, 496)  |grad|=1.505e+00\n",
            "  #34: transformer.wte.weight(262, 496)  |grad|=1.470e+00\n",
            "  #35: transformer.wte.weight(8118, 496)  |grad|=1.467e+00\n",
            "  #36: transformer.wte.weight(18884, 496)  |grad|=1.432e+00\n",
            "  #37: transformer.wte.weight(12108, 496)  |grad|=1.377e+00\n",
            "  #38: transformer.wte.weight(27512, 496)  |grad|=1.364e+00\n",
            "  #39: transformer.wte.weight(44665, 496)  |grad|=1.353e+00\n",
            "  #40: transformer.wte.weight(23214, 496)  |grad|=1.348e+00\n",
            "  #41: transformer.wte.weight(366, 496)  |grad|=1.336e+00\n",
            "  #42: transformer.wte.weight(796, 430)  |grad|=1.317e+00\n",
            "  #43: transformer.wte.weight(45674, 496)  |grad|=1.313e+00\n",
            "  #44: transformer.wte.weight(16825, 496)  |grad|=1.310e+00\n",
            "  #45: transformer.wte.weight(19498, 496)  |grad|=1.309e+00\n",
            "  #46: transformer.wte.weight(15767, 496)  |grad|=1.303e+00\n",
            "  #47: transformer.wte.weight(28972, 496)  |grad|=1.301e+00\n",
            "  #48: transformer.wte.weight(11, 430)  |grad|=1.279e+00\n",
            "  #49: transformer.wte.weight(34315, 430)  |grad|=1.277e+00\n",
            "  #50: transformer.wte.weight(3248, 496)  |grad|=1.268e+00\n",
            "  #51: transformer.wte.weight(37470, 496)  |grad|=1.264e+00\n",
            "  #52: transformer.wte.weight(10849, 496)  |grad|=1.261e+00\n",
            "  #53: transformer.wte.weight(11849, 496)  |grad|=1.254e+00\n",
            "  #54: transformer.wte.weight(37885, 496)  |grad|=1.243e+00\n",
            "  #55: transformer.wte.weight(39452, 496)  |grad|=1.242e+00\n",
            "  #56: transformer.wte.weight(10278, 496)  |grad|=1.232e+00\n",
            "  #57: transformer.wte.weight(29285, 496)  |grad|=1.228e+00\n",
            "  #58: transformer.wte.weight(2448, 496)  |grad|=1.222e+00\n",
            "  #59: transformer.wte.weight(9398, 496)  |grad|=1.221e+00\n",
            "  #60: transformer.wte.weight(4960, 496)  |grad|=1.216e+00\n",
            "  #61: transformer.wte.weight(6445, 496)  |grad|=1.216e+00\n",
            "  #62: transformer.wte.weight(12592, 496)  |grad|=1.210e+00\n",
            "  #63: transformer.wte.weight(11565, 496)  |grad|=1.206e+00\n",
            "  #64: transformer.wte.weight(2039, 496)  |grad|=1.196e+00\n",
            "  #65: transformer.wte.weight(286, 496)  |grad|=1.191e+00\n",
            "  #66: transformer.wte.weight(19439, 496)  |grad|=1.183e+00\n",
            "  #67: transformer.wte.weight(564, 496)  |grad|=1.180e+00\n",
            "  #68: transformer.wte.weight(26451, 496)  |grad|=1.178e+00\n",
            "  #69: transformer.wte.weight(14818, 496)  |grad|=1.177e+00\n",
            "  #70: transformer.wte.weight(39261, 496)  |grad|=1.170e+00\n",
            "  #71: transformer.wte.weight(40929, 496)  |grad|=1.163e+00\n",
            "  #72: transformer.wte.weight(11852, 496)  |grad|=1.161e+00\n",
            "  #73: transformer.wte.weight(4302, 496)  |grad|=1.153e+00\n",
            "  #74: transformer.wte.weight(39601, 496)  |grad|=1.144e+00\n",
            "  #75: transformer.wte.weight(220, 430)  |grad|=1.144e+00\n",
            "  #76: transformer.wte.weight(15518, 496)  |grad|=1.142e+00\n",
            "  #77: transformer.wte.weight(1279, 496)  |grad|=1.133e+00\n",
            "  #78: transformer.wte.weight(8225, 496)  |grad|=1.132e+00\n",
            "  #79: transformer.wte.weight(13, 430)  |grad|=1.120e+00\n",
            "  #80: transformer.wte.weight(18551, 496)  |grad|=1.119e+00\n",
            "  #81: transformer.wte.weight(38237, 496)  |grad|=1.114e+00\n",
            "  #82: transformer.wte.weight(2635, 496)  |grad|=1.112e+00\n",
            "  #83: transformer.wte.weight(25656, 496)  |grad|=1.112e+00\n",
            "  #84: transformer.wte.weight(338, 496)  |grad|=1.103e+00\n",
            "  #85: transformer.wte.weight(764, 430)  |grad|=1.102e+00\n",
            "  #86: transformer.wte.weight(3240, 496)  |grad|=1.102e+00\n",
            "  #87: transformer.wte.weight(30498, 496)  |grad|=1.095e+00\n",
            "  #88: transformer.wte.weight(35259, 496)  |grad|=1.094e+00\n",
            "  #89: transformer.wte.weight(24023, 496)  |grad|=1.087e+00\n",
            "  #90: transformer.wte.weight(44302, 496)  |grad|=1.085e+00\n",
            "  #91: transformer.wte.weight(27583, 430)  |grad|=1.084e+00\n",
            "  #92: transformer.wte.weight(28437, 496)  |grad|=1.083e+00\n",
            "  #93: transformer.wte.weight(3592, 496)  |grad|=1.083e+00\n",
            "  #94: transformer.wte.weight(39644, 496)  |grad|=1.082e+00\n",
            "  #95: transformer.wte.weight(33911, 496)  |grad|=1.077e+00\n",
            "  #96: transformer.wte.weight(10880, 496)  |grad|=1.076e+00\n",
            "  #97: transformer.wte.weight(20404, 496)  |grad|=1.074e+00\n",
            "  #98: transformer.wte.weight(705, 496)  |grad|=1.072e+00\n",
            "  #99: transformer.wte.weight(796, 36)  |grad|=1.065e+00\n",
            "  #100: transformer.wte.weight(29490, 496)  |grad|=1.061e+00\n",
            "  #101: transformer.wte.weight(46906, 496)  |grad|=1.060e+00\n",
            "  #102: transformer.wte.weight(24872, 496)  |grad|=1.057e+00\n",
            "  #103: transformer.wte.weight(287, 496)  |grad|=1.055e+00\n",
            "  #104: transformer.wte.weight(47891, 496)  |grad|=1.049e+00\n",
            "  #105: transformer.wte.weight(257, 496)  |grad|=1.048e+00\n",
            "  #106: transformer.wte.weight(1375, 496)  |grad|=1.042e+00\n",
            "  #107: transformer.wte.weight(978, 496)  |grad|=1.041e+00\n",
            "  #108: transformer.wte.weight(17099, 496)  |grad|=1.037e+00\n",
            "  #109: transformer.wte.weight(10769, 496)  |grad|=1.036e+00\n",
            "  #110: transformer.wte.weight(1665, 496)  |grad|=1.030e+00\n",
            "  #111: transformer.wte.weight(9719, 496)  |grad|=1.022e+00\n",
            "  #112: transformer.wte.weight(8599, 496)  |grad|=1.017e+00\n",
            "  #113: transformer.wte.weight(27132, 496)  |grad|=1.015e+00\n",
            "  #114: transformer.wte.weight(11, 36)  |grad|=1.015e+00\n",
            "  #115: transformer.wte.weight(25979, 496)  |grad|=1.012e+00\n",
            "  #116: transformer.wte.weight(3261, 496)  |grad|=1.007e+00\n",
            "  #117: transformer.wte.weight(38580, 496)  |grad|=9.963e-01\n",
            "  #118: transformer.wte.weight(11523, 496)  |grad|=9.962e-01\n",
            "  #119: transformer.wte.weight(3687, 496)  |grad|=9.959e-01\n",
            "  #120: transformer.wte.weight(32684, 496)  |grad|=9.945e-01\n",
            "  #121: transformer.wte.weight(14780, 496)  |grad|=9.882e-01\n",
            "  #122: transformer.wte.weight(32831, 496)  |grad|=9.881e-01\n",
            "  #123: transformer.wte.weight(20531, 496)  |grad|=9.869e-01\n",
            "  #124: transformer.wte.weight(9718, 496)  |grad|=9.805e-01\n",
            "  #125: transformer.wte.weight(1379, 496)  |grad|=9.805e-01\n",
            "  #126: transformer.wte.weight(20842, 496)  |grad|=9.728e-01\n",
            "  #127: transformer.wte.weight(7096, 496)  |grad|=9.698e-01\n",
            "  #128: transformer.wte.weight(13709, 496)  |grad|=9.697e-01\n",
            "  #129: transformer.wte.weight(14074, 496)  |grad|=9.689e-01\n",
            "  #130: transformer.wte.weight(46547, 496)  |grad|=9.649e-01\n",
            "  #131: transformer.wte.weight(304, 496)  |grad|=9.643e-01\n",
            "  #132: transformer.wte.weight(49063, 430)  |grad|=9.596e-01\n",
            "  #133: transformer.wte.weight(220, 36)  |grad|=9.595e-01\n",
            "  #134: transformer.wte.weight(34315, 36)  |grad|=9.577e-01\n",
            "  #135: transformer.wte.weight(32164, 496)  |grad|=9.575e-01\n",
            "  #136: transformer.wte.weight(21679, 496)  |grad|=9.564e-01\n",
            "  #137: transformer.wte.weight(46619, 496)  |grad|=9.518e-01\n",
            "  #138: transformer.wte.weight(46446, 496)  |grad|=9.491e-01\n",
            "  #139: transformer.wte.weight(2046, 496)  |grad|=9.488e-01\n",
            "  #140: transformer.wte.weight(6252, 496)  |grad|=9.469e-01\n",
            "  #141: transformer.wte.weight(4991, 496)  |grad|=9.458e-01\n",
            "  #142: transformer.wte.weight(9502, 496)  |grad|=9.456e-01\n",
            "  #143: transformer.wte.weight(5686, 496)  |grad|=9.453e-01\n",
            "  #144: transformer.wte.weight(40580, 496)  |grad|=9.438e-01\n",
            "  #145: transformer.wte.weight(3905, 496)  |grad|=9.426e-01\n",
            "  #146: transformer.wte.weight(46811, 496)  |grad|=9.419e-01\n",
            "  #147: transformer.wte.weight(5838, 496)  |grad|=9.387e-01\n",
            "  #148: transformer.wte.weight(12686, 496)  |grad|=9.378e-01\n",
            "  #149: transformer.wte.weight(15608, 496)  |grad|=9.364e-01\n",
            "  #150: transformer.wte.weight(6285, 496)  |grad|=9.331e-01\n",
            "  #151: transformer.wte.weight(314, 496)  |grad|=9.325e-01\n",
            "  #152: transformer.wte.weight(10343, 496)  |grad|=9.322e-01\n",
            "  #153: transformer.wte.weight(37449, 496)  |grad|=9.305e-01\n",
            "  #154: transformer.wte.weight(15281, 430)  |grad|=9.274e-01\n",
            "  #155: transformer.wte.weight(27599, 496)  |grad|=9.240e-01\n",
            "  #156: transformer.wte.weight(43579, 496)  |grad|=9.174e-01\n",
            "  #157: transformer.wte.weight(47922, 496)  |grad|=9.140e-01\n",
            "  #158: transformer.wte.weight(5108, 496)  |grad|=9.011e-01\n",
            "  #159: transformer.wte.weight(13098, 496)  |grad|=8.999e-01\n",
            "  #160: transformer.wte.weight(1757, 496)  |grad|=8.985e-01\n",
            "  #161: transformer.wte.weight(14328, 496)  |grad|=8.973e-01\n",
            "  #162: transformer.wte.weight(13, 36)  |grad|=8.964e-01\n",
            "  #163: transformer.wte.weight(5187, 430)  |grad|=8.959e-01\n",
            "  #164: transformer.wte.weight(520, 496)  |grad|=8.954e-01\n",
            "  #165: transformer.wte.weight(290, 496)  |grad|=8.946e-01\n",
            "  #166: transformer.wte.weight(2646, 496)  |grad|=8.944e-01\n",
            "  #167: transformer.wte.weight(9552, 496)  |grad|=8.927e-01\n",
            "  #168: transformer.wte.weight(764, 36)  |grad|=8.926e-01\n",
            "  #169: transformer.wte.weight(31, 430)  |grad|=8.916e-01\n",
            "  #170: transformer.wte.weight(20603, 496)  |grad|=8.907e-01\n",
            "  #171: transformer.wte.weight(5628, 496)  |grad|=8.890e-01\n",
            "  #172: transformer.wte.weight(5706, 496)  |grad|=8.883e-01\n",
            "  #173: transformer.wte.weight(47489, 496)  |grad|=8.847e-01\n",
            "  #174: transformer.wte.weight(289, 496)  |grad|=8.846e-01\n",
            "  #175: transformer.wte.weight(7158, 496)  |grad|=8.829e-01\n",
            "  #176: transformer.wte.weight(645, 496)  |grad|=8.827e-01\n",
            "  #177: transformer.wte.weight(26249, 496)  |grad|=8.803e-01\n",
            "  #178: transformer.wte.weight(8765, 496)  |grad|=8.784e-01\n",
            "  #179: transformer.wte.weight(46626, 496)  |grad|=8.773e-01\n",
            "  #180: transformer.wte.weight(7653, 496)  |grad|=8.723e-01\n",
            "  #181: transformer.wte.weight(49931, 496)  |grad|=8.721e-01\n",
            "  #182: transformer.wte.weight(45630, 496)  |grad|=8.713e-01\n",
            "  #183: transformer.wte.weight(9450, 496)  |grad|=8.711e-01\n",
            "  #184: transformer.wte.weight(10977, 496)  |grad|=8.705e-01\n",
            "  #185: transformer.wte.weight(16140, 496)  |grad|=8.688e-01\n",
            "  #186: transformer.wte.weight(1210, 496)  |grad|=8.686e-01\n",
            "  #187: transformer.h.5.ln_1.weight(447,)  |grad|=8.676e-01\n",
            "  #188: transformer.wte.weight(34927, 496)  |grad|=8.674e-01\n",
            "  #189: transformer.wte.weight(33298, 496)  |grad|=8.673e-01\n",
            "  #190: transformer.wte.weight(449, 430)  |grad|=8.654e-01\n",
            "  #191: transformer.wte.weight(5915, 496)  |grad|=8.639e-01\n",
            "  #192: transformer.wte.weight(9666, 496)  |grad|=8.585e-01\n",
            "  #193: transformer.wte.weight(6645, 430)  |grad|=8.585e-01\n",
            "  #194: transformer.wte.weight(15945, 496)  |grad|=8.571e-01\n",
            "  #195: transformer.wte.weight(10169, 496)  |grad|=8.543e-01\n",
            "  #196: transformer.wte.weight(8050, 496)  |grad|=8.541e-01\n",
            "  #197: transformer.wte.weight(42192, 496)  |grad|=8.536e-01\n",
            "  #198: transformer.wte.weight(13922, 496)  |grad|=8.536e-01\n",
            "  #199: transformer.wte.weight(784, 496)  |grad|=8.513e-01\n",
            "  #200: transformer.wte.weight(284, 496)  |grad|=8.504e-01\n",
            "  #201: transformer.wte.weight(4847, 496)  |grad|=8.500e-01\n",
            "  #202: transformer.wte.weight(2089, 496)  |grad|=8.476e-01\n",
            "  #203: transformer.wte.weight(23975, 496)  |grad|=8.460e-01\n",
            "  #204: transformer.wte.weight(554, 496)  |grad|=8.457e-01\n",
            "  #205: transformer.wte.weight(31210, 496)  |grad|=8.455e-01\n",
            "  #206: transformer.wte.weight(12469, 496)  |grad|=8.453e-01\n",
            "  #207: transformer.wte.weight(5780, 496)  |grad|=8.446e-01\n",
            "  #208: transformer.wte.weight(7459, 496)  |grad|=8.426e-01\n",
            "  #209: transformer.wte.weight(19305, 496)  |grad|=8.424e-01\n",
            "  #210: transformer.wte.weight(4319, 496)  |grad|=8.416e-01\n",
            "  #211: transformer.wte.weight(12164, 430)  |grad|=8.411e-01\n",
            "  #212: transformer.wte.weight(1971, 496)  |grad|=8.409e-01\n",
            "  #213: transformer.wte.weight(20400, 496)  |grad|=8.388e-01\n",
            "  #214: transformer.wte.weight(983, 496)  |grad|=8.379e-01\n",
            "  #215: transformer.wte.weight(47268, 496)  |grad|=8.378e-01\n",
            "  #216: transformer.wte.weight(35293, 496)  |grad|=8.359e-01\n",
            "  #217: transformer.wte.weight(3831, 496)  |grad|=8.320e-01\n",
            "  #218: transformer.wte.weight(5030, 496)  |grad|=8.316e-01\n",
            "  #219: transformer.wte.weight(791, 496)  |grad|=8.315e-01\n",
            "  #220: transformer.wte.weight(5825, 496)  |grad|=8.294e-01\n",
            "  #221: transformer.wte.weight(32355, 496)  |grad|=8.285e-01\n",
            "  #222: transformer.wte.weight(25176, 496)  |grad|=8.264e-01\n",
            "  #223: transformer.wte.weight(8685, 496)  |grad|=8.260e-01\n",
            "  #224: transformer.wte.weight(6046, 496)  |grad|=8.231e-01\n",
            "  #225: transformer.wte.weight(49203, 496)  |grad|=8.198e-01\n",
            "  #226: transformer.wte.weight(13257, 496)  |grad|=8.195e-01\n",
            "  #227: transformer.wte.weight(3208, 496)  |grad|=8.150e-01\n",
            "  #228: transformer.wte.weight(6881, 496)  |grad|=8.148e-01\n",
            "  #229: transformer.wte.weight(9634, 496)  |grad|=8.129e-01\n",
            "  #230: transformer.wte.weight(2644, 496)  |grad|=8.085e-01\n",
            "  #231: transformer.wte.weight(7164, 496)  |grad|=8.080e-01\n",
            "  #232: transformer.wte.weight(20716, 496)  |grad|=8.070e-01\n",
            "  #233: transformer.wte.weight(2284, 496)  |grad|=8.067e-01\n",
            "  #234: transformer.wte.weight(11287, 496)  |grad|=8.064e-01\n",
            "  #235: transformer.wte.weight(37884, 496)  |grad|=8.057e-01\n",
            "  #236: transformer.wte.weight(12551, 496)  |grad|=8.045e-01\n",
            "  #237: transformer.wte.weight(6107, 496)  |grad|=8.045e-01\n",
            "  #238: transformer.wte.weight(7979, 496)  |grad|=8.044e-01\n",
            "  #239: transformer.wte.weight(12099, 496)  |grad|=8.011e-01\n",
            "  #240: transformer.wte.weight(2688, 496)  |grad|=7.983e-01\n",
            "  #241: transformer.wte.weight(7897, 496)  |grad|=7.979e-01\n",
            "  #242: transformer.wte.weight(18501, 496)  |grad|=7.958e-01\n",
            "  #243: transformer.wte.weight(9281, 496)  |grad|=7.921e-01\n",
            "  #244: transformer.wte.weight(21165, 430)  |grad|=7.920e-01\n",
            "  #245: transformer.wte.weight(1126, 496)  |grad|=7.908e-01\n",
            "  #246: transformer.wte.weight(27583, 36)  |grad|=7.901e-01\n",
            "  #247: transformer.wte.weight(39775, 496)  |grad|=7.892e-01\n",
            "  #248: transformer.wte.weight(7920, 496)  |grad|=7.873e-01\n",
            "  #249: transformer.wte.weight(37053, 496)  |grad|=7.863e-01\n",
            "  #250: transformer.wte.weight(12862, 496)  |grad|=7.852e-01\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 0) Setup\n",
        "# ============================================================\n",
        "import torch, random\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
        "\n",
        "# ---- Config ----\n",
        "SEED       = 123\n",
        "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_ID   = \"gpt2\"\n",
        "SPLIT      = \"train\"\n",
        "SEQ_LEN    = 1024\n",
        "BATCH_SIZE = 16\n",
        "MAX_STEPS  = 1100\n",
        "TOP_K      = 250  # report top-k most sensitive elements\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# ============================================================\n",
        "# 1) Model & Tokenizer\n",
        "# ============================================================\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "tok.pad_token = tok.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
        "model.train()  # need gradients\n",
        "\n",
        "# ============================================================\n",
        "# 2) Dataset & Chunking\n",
        "# ============================================================\n",
        "wiki = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=SPLIT)\n",
        "\n",
        "def chunk_generator():\n",
        "    cache = []\n",
        "    for doc in wiki:\n",
        "        cache.extend(tok(doc[\"text\"]).input_ids)\n",
        "        while len(cache) >= SEQ_LEN + 1:\n",
        "            win, cache = cache[:SEQ_LEN+1], cache[SEQ_LEN+1:]  # non-overlap\n",
        "            yield win[:-1], win[1:]\n",
        "\n",
        "def get_batch(gen, bs=BATCH_SIZE):\n",
        "    buf = []\n",
        "    for x, _ in gen:\n",
        "        buf.append(x)\n",
        "        if len(buf) == bs:\n",
        "            yield torch.tensor(buf, device=DEVICE)\n",
        "            buf = []\n",
        "\n",
        "# ============================================================\n",
        "# 3) Gradient Scan\n",
        "# ============================================================\n",
        "param_dict  = {n: p for n, p in model.named_parameters() if p.requires_grad}\n",
        "running_max = {n: torch.zeros_like(p, device=\"cpu\") for n, p in param_dict.items()}\n",
        "\n",
        "for step, inp in enumerate(get_batch(chunk_generator()), 1):\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    loss = model(inp, labels=inp).loss\n",
        "    loss.backward()\n",
        "    for name, p in param_dict.items():\n",
        "        running_max[name] = torch.maximum(\n",
        "            running_max[name],\n",
        "            p.grad.detach().abs().to(\"cpu\")\n",
        "        )\n",
        "    if step >= MAX_STEPS:\n",
        "        break\n",
        "\n",
        "# ============================================================\n",
        "# 4) Find Top-K Sensitive Coordinates\n",
        "# ============================================================\n",
        "candidates = []\n",
        "for name, rm in running_max.items():\n",
        "    k_local = min(TOP_K, rm.numel())\n",
        "    if k_local > 0:\n",
        "        vals, idxs = torch.topk(rm.view(-1), k_local)\n",
        "        for v, flat in zip(vals, idxs):\n",
        "            coord = torch.unravel_index(flat, rm.shape)\n",
        "            candidates.append((v.item(), name, coord))\n",
        "\n",
        "candidates.sort(key=lambda t: t[0], reverse=True)\n",
        "topk_entries = candidates[:TOP_K]\n",
        "\n",
        "print(f\"\\nTop-{TOP_K} most sensitive tensor elements:\")\n",
        "for rank, (val, name, coord) in enumerate(topk_entries, 1):\n",
        "    print(f\"  #{rank}: {name}{tuple(map(int,coord))}  |grad|={val:.3e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 0) Setup & Config\n",
        "# ============================================================\n",
        "import random\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# ---- Experiment knobs ----\n",
        "SEED       = 123\n",
        "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "MODEL_ID   = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "SPLIT      = \"train\"\n",
        "SEQ_LEN    = 750      # keep modest to avoid OOM\n",
        "BATCH_SIZE = 8        # start small; you can try 2 later\n",
        "MAX_STEPS  = 100        # you said even 1 step is enough for now\n",
        "TOP_K      = 250      # number of elements to report\n",
        "\n",
        "# ---- Optional exclusion for param_dict (usually keep OFF) ----\n",
        "EXCLUDE_ENABLED = False\n",
        "EXCLUDE_PARAM_NAMES = [\n",
        "    # e.g. \"model.embed_tokens.weight\", \"lm_head.weight\"\n",
        "]\n",
        "\n",
        "# ---- Filter for the SECOND Top-K (local vs global) ----\n",
        "# These names will be EXCLUDED from the \"local\" (filtered) 250.\n",
        "FILTER_ENABLED = True\n",
        "FILTER_PARAM_NAMES = [\n",
        "    \"model.embed_tokens.weight\",  # DeepSeek token embeddings\n",
        "    \"lm_head.weight\",             # optional: output head\n",
        "]\n",
        "\n",
        "# Reproducibility\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# ============================================================\n",
        "# 1) Model & Tokenizer\n",
        "# ============================================================\n",
        "print(\"Loading tokenizer...\")\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "if tok.pad_token is None:\n",
        "    if tok.eos_token is not None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    else:\n",
        "        tok.pad_token = tok.convert_ids_to_tokens(0)\n",
        "\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    dtype=torch.float16,           # half precision to reduce memory\n",
        "    device_map={\"\": DEVICE},\n",
        ")\n",
        "\n",
        "# Optional: gradient checkpointing to save memory\n",
        "model.gradient_checkpointing_enable()\n",
        "model.train()  # we need gradients\n",
        "\n",
        "first_param = next(model.parameters())\n",
        "print(f\"Model loaded on {DEVICE} with dtype {first_param.dtype}\")\n",
        "\n",
        "# ============================================================\n",
        "# 2) Dataset & Chunking\n",
        "# ============================================================\n",
        "print(\"Loading dataset...\")\n",
        "wiki = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=SPLIT)\n",
        "\n",
        "def chunk_generator():\n",
        "    \"\"\"\n",
        "    Stream Wikitext into a rolling buffer of token ids, and yield\n",
        "    non-overlapping windows of length SEQ_LEN+1, split into (inp, labels).\n",
        "    \"\"\"\n",
        "    cache = []\n",
        "    for doc in wiki:\n",
        "        text = doc[\"text\"]\n",
        "        if not text:\n",
        "            continue\n",
        "        ids = tok(text).input_ids\n",
        "        cache.extend(ids)\n",
        "        while len(cache) >= SEQ_LEN + 1:\n",
        "            win, cache = cache[:SEQ_LEN+1], cache[SEQ_LEN+1:]\n",
        "            inp, labels = win[:-1], win[1:]\n",
        "            yield inp, labels\n",
        "\n",
        "def get_batch(gen, bs=BATCH_SIZE):\n",
        "    \"\"\"\n",
        "    Stack windows from chunk_generator into batches of size bs.\n",
        "    \"\"\"\n",
        "    buf = []\n",
        "    for x, _ in gen:\n",
        "        buf.append(x)\n",
        "        if len(buf) == bs:\n",
        "            batch = torch.tensor(buf, dtype=torch.long, device=DEVICE)\n",
        "            yield batch\n",
        "            buf = []\n",
        "\n",
        "# ============================================================\n",
        "# 3) Parameter Dictionary & Exclusion (for scan)\n",
        "# ============================================================\n",
        "def should_exclude_for_scan(name: str) -> bool:\n",
        "    \"\"\"Exact-name exclusion for building param_dict.\"\"\"\n",
        "    if not EXCLUDE_ENABLED:\n",
        "        return False\n",
        "    return name in EXCLUDE_PARAM_NAMES\n",
        "\n",
        "param_dict = {\n",
        "    n: p for n, p in model.named_parameters()\n",
        "    if p.requires_grad and not should_exclude_for_scan(n)\n",
        "}\n",
        "\n",
        "print(f\"Total tracked parameter tensors: {len(param_dict)}\")\n",
        "total_elems = sum(p.numel() for p in param_dict.values())\n",
        "print(f\"Total tracked elements: {total_elems:,}\")\n",
        "\n",
        "# Keep running max on CPU in float32\n",
        "running_max = {\n",
        "    n: torch.zeros_like(p, device=\"cpu\", dtype=torch.float32)\n",
        "    for n, p in param_dict.items()\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# 4) Gradient Scan\n",
        "# ============================================================\n",
        "print(\"Starting gradient scan...\")\n",
        "stream = get_batch(chunk_generator(), bs=BATCH_SIZE)\n",
        "\n",
        "for step, inp in enumerate(stream, 1):\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    out = model(inp, labels=inp)   # causal LM cross-entropy next-token loss\n",
        "    loss = out.loss\n",
        "    loss.backward()\n",
        "\n",
        "    for name, p in param_dict.items():\n",
        "        if p.grad is None:\n",
        "            continue\n",
        "        grad_abs = p.grad.detach().abs().to(\"cpu\", dtype=torch.float32)\n",
        "        running_max[name] = torch.maximum(running_max[name], grad_abs)\n",
        "\n",
        "    print(f\"  [step {step}] loss = {loss.item():.4f}\")\n",
        "    if step >= MAX_STEPS:\n",
        "        print(f\"Reached MAX_STEPS = {MAX_STEPS}. Stopping scan.\")\n",
        "        break\n",
        "\n",
        "print(\"Gradient scan complete.\")\n",
        "\n",
        "# ============================================================\n",
        "# 5) Build global ranked list (with global ranks)\n",
        "# ============================================================\n",
        "print(f\"Selecting global Top-{TOP_K} elements by |grad|...\")\n",
        "\n",
        "candidates = []  # will hold (value, name, coord_tuple)\n",
        "\n",
        "for name, rm in running_max.items():\n",
        "    numel = rm.numel()\n",
        "    if numel == 0:\n",
        "        continue\n",
        "    k_local = min(TOP_K, numel)  # top-k within this tensor\n",
        "    vals, idxs = torch.topk(rm.view(-1), k_local)\n",
        "    for v, flat_idx in zip(vals, idxs):\n",
        "        coord = torch.unravel_index(flat_idx, rm.shape)\n",
        "        candidates.append((v.item(), name, coord))\n",
        "\n",
        "# Global sort by gradient magnitude\n",
        "candidates.sort(key=lambda t: t[0], reverse=True)\n",
        "\n",
        "# Attach global rank and store in a structured list\n",
        "ranked_global = []\n",
        "for global_rank, (val, name, coord) in enumerate(candidates, 1):\n",
        "    ranked_global.append({\n",
        "        \"global_rank\": global_rank,\n",
        "        \"value\": val,\n",
        "        \"name\": name,\n",
        "        \"coord\": coord,\n",
        "    })\n",
        "\n",
        "# Global Top-K\n",
        "global_topk = ranked_global[:TOP_K]\n",
        "\n",
        "# ============================================================\n",
        "# 6) Build filtered Top-K with local + global ranks\n",
        "# ============================================================\n",
        "def passes_filter(name: str) -> bool:\n",
        "    \"\"\"Return True if this parameter is allowed in the filtered list.\"\"\"\n",
        "    if not FILTER_ENABLED:\n",
        "        return True\n",
        "    return name not in FILTER_PARAM_NAMES\n",
        "\n",
        "filtered = [item for item in ranked_global if passes_filter(item[\"name\"])]\n",
        "filtered_topk = filtered[:TOP_K]\n",
        "\n",
        "# Attach local ranks\n",
        "for local_rank, item in enumerate(filtered_topk, 1):\n",
        "    item[\"local_rank\"] = local_rank\n",
        "\n",
        "# ============================================================\n",
        "# 7) Print Results\n",
        "# ============================================================\n",
        "print(f\"\\n=== Global Top-{TOP_K} most sensitive tensor elements ===\")\n",
        "for item in global_topk:\n",
        "    gr = item[\"global_rank\"]\n",
        "    val = item[\"value\"]\n",
        "    name = item[\"name\"]\n",
        "    coord = item[\"coord\"]\n",
        "    coord_str = \"(\" + \", \".join(str(int(c)) for c in coord) + \")\"\n",
        "    print(f\"  global # {gr:4d}: {name}{coord_str}  |grad|={val:.3e}\")\n",
        "\n",
        "print(f\"\\n=== Filtered Top-{TOP_K} (w/ local & global ranks) ===\")\n",
        "print(\"Filter excludes:\", FILTER_PARAM_NAMES if FILTER_ENABLED else \"None\")\n",
        "for item in filtered_topk:\n",
        "    lr = item[\"local_rank\"]\n",
        "    gr = item[\"global_rank\"]\n",
        "    val = item[\"value\"]\n",
        "    name = item[\"name\"]\n",
        "    coord = item[\"coord\"]\n",
        "    coord_str = \"(\" + \", \".join(str(int(c)) for c in coord) + \")\"\n",
        "    print(\n",
        "        f\"  local # {lr:4d} | global # {gr:4d} : \"\n",
        "        f\"{name}{coord_str}  |grad|={val:.3e}\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "SgRGEo_e2N94"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}